# This list can be obtained by invoking a TensorFlow API, `LOG(WARNING) << OpRegistry::Global()->DebugString(false);`.

Op<name=Abort; signature= -> ; attr=error_msg:string,default=""; attr=exit_without_error:bool,default=false>
Op<name=Abs; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64]>
Op<name=AccumulateNV2; signature=inputs:N*T -> sum:T; attr=N:int,min=1; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; is_commutative=true; is_aggregate=true>
Op<name=AccumulatorApplyGradient; signature=handle:Ref(string), local_step:int64, gradient:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=AccumulatorNumAccumulated; signature=handle:Ref(string) -> num_accumulated:int32>
Op<name=AccumulatorSetGlobalStep; signature=handle:Ref(string), new_global_step:int64 -> >
Op<name=AccumulatorTakeGradient; signature=handle:Ref(string), num_required:int32 -> average:dtype; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=Acos; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Acosh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Add; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_STRING]>
Op<name=AddManySparseToTensorsMap; signature=sparse_indices:int64, sparse_values:T, sparse_shape:int64 -> sparse_handles:int64; attr=T:type; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=AddN; signature=inputs:N*T -> sum:T; attr=N:int,min=1; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_VARIANT]; is_commutative=true; is_aggregate=true>
Op<name=AddSparseToTensorsMap; signature=sparse_indices:int64, sparse_values:T, sparse_shape:int64 -> sparse_handle:int64; attr=T:type; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=AddV2; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true; is_aggregate=true>
Op<name=AdjustContrast; signature=images:T, contrast_factor:float, min_value:float, max_value:float -> output:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]>
Op<name=AdjustContrastv2; signature=images:T, contrast_factor:float -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]>
Op<name=AdjustHue; signature=images:T, delta:float -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]>
Op<name=AdjustSaturation; signature=images:T, scale:float -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]>
Op<name=All; signature=input:bool, reduction_indices:Tidx -> output:bool; attr=keep_dims:bool,default=false; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=AllCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=AllToAll; signature=input:T, group_assignment:int32 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_BOOL]; attr=concat_dimension:int; attr=split_dimension:int; attr=split_count:int; is_stateful=true>
Op<name=Angle; signature=input:T -> output:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=AnonymousHashTable; signature= -> table_handle:resource; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=AnonymousIterator; signature= -> handle:resource; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=AnonymousIteratorV2; signature= -> handle:resource, deleter:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=AnonymousIteratorV3; signature= -> handle:resource; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=AnonymousMemoryCache; signature= -> handle:resource, deleter:variant; is_stateful=true>
Op<name=AnonymousMultiDeviceIterator; signature= -> handle:resource, deleter:variant; attr=devices:list(string),min=1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=AnonymousMultiDeviceIteratorV3; signature= -> handle:resource; attr=devices:list(string),min=1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=AnonymousMutableDenseHashTable; signature=empty_key:key_dtype, deleted_key:key_dtype -> table_handle:resource; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; attr=initial_num_buckets:int,default=131072; attr=max_load_factor:float,default=0.8; is_stateful=true>
Op<name=AnonymousMutableHashTable; signature= -> table_handle:resource; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=AnonymousMutableHashTableOfTensors; signature= -> table_handle:resource; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; is_stateful=true>
Op<name=AnonymousRandomSeedGenerator; signature=seed:int64, seed2:int64 -> handle:resource, deleter:variant; is_stateful=true>
Op<name=AnonymousSeedGenerator; signature=seed:int64, seed2:int64, reshuffle:bool -> handle:resource, deleter:variant; is_stateful=true>
Op<name=Any; signature=input:bool, reduction_indices:Tidx -> output:bool; attr=keep_dims:bool,default=false; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ApplyAdaMax; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyAdadelta; signature=var:Ref(T), accum:Ref(T), accum_update:Ref(T), lr:T, rho:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyAdagrad; signature=var:Ref(T), accum:Ref(T), lr:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true>
Op<name=ApplyAdagradDA; signature=var:Ref(T), gradient_accumulator:Ref(T), gradient_squared_accumulator:Ref(T), grad:T, lr:T, l1:T, l2:T, global_step:int64 -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyAdagradV2; signature=var:Ref(T), accum:Ref(T), lr:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true>
Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false>
Op<name=ApplyAddSign; signature=var:Ref(T), m:Ref(T), lr:T, alpha:T, sign_decay:T, beta:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyCenteredRMSProp; signature=var:Ref(T), mg:Ref(T), ms:Ref(T), mom:Ref(T), lr:T, rho:T, momentum:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyFtrl; signature=var:Ref(T), accum:Ref(T), linear:Ref(T), grad:T, lr:T, l1:T, l2:T, lr_power:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false>
Op<name=ApplyFtrlV2; signature=var:Ref(T), accum:Ref(T), linear:Ref(T), grad:T, lr:T, l1:T, l2:T, l2_shrinkage:T, lr_power:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false>
Op<name=ApplyGradientDescent; signature=var:Ref(T), alpha:T, delta:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyMomentum; signature=var:Ref(T), accum:Ref(T), lr:T, grad:T, momentum:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false>
Op<name=ApplyPowerSign; signature=var:Ref(T), m:Ref(T), lr:T, logbase:T, sign_decay:T, beta:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyProximalAdagrad; signature=var:Ref(T), accum:Ref(T), lr:T, l1:T, l2:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyProximalGradientDescent; signature=var:Ref(T), alpha:T, l1:T, l2:T, delta:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApplyRMSProp; signature=var:Ref(T), ms:Ref(T), mom:Ref(T), lr:T, rho:T, momentum:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=ApproxTopK; signature=input:T -> values:T, indices:int32; attr=k:int,min=0; attr=reduction_dimension:int,default=-1; attr=recall_target:float,default=0.95; attr=is_max_k:bool,default=true; attr=reduction_input_size_override:int,default=-1; attr=aggregate_to_topk:bool,default=true; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]>
Op<name=ApproximateEqual; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=tolerance:float,default=1e-05; is_commutative=true>
Op<name=ArgMax; signature=input:T, dimension:Tidx -> output:output_type; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16, DT_BOOL]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=output_type:type,default=DT_INT64,allowed=[DT_INT16, DT_UINT16, DT_INT32, DT_INT64]>
Op<name=ArgMin; signature=input:T, dimension:Tidx -> output:output_type; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16, DT_BOOL]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=output_type:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=AsString; signature=input:T -> output:string; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_VARIANT, DT_STRING]; attr=precision:int,default=-1; attr=scientific:bool,default=false; attr=shortest:bool,default=false; attr=width:int,default=-1; attr=fill:string,default="">
Op<name=Asin; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Asinh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Assert; signature=condition:bool, data: -> ; attr=T:list(type),min=1; attr=summarize:int,default=3; is_stateful=true>
Op<name=AssertCardinalityDataset; signature=input_dataset:variant, cardinality:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=AssertNextDataset; signature=input_dataset:variant, transformations:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=AssertPrevDataset; signature=input_dataset:variant, transformations:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Assign; signature=ref:Ref(T), value:T -> output_ref:Ref(T); attr=T:type; attr=validate_shape:bool,default=true; attr=use_locking:bool,default=true; allows_uninitialized_input=true>
Op<name=AssignAdd; signature=ref:Ref(T), value:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=AssignAddVariableOp; signature=resource:resource, value:dtype -> ; attr=dtype:type; is_stateful=true>
Op<name=AssignSub; signature=ref:Ref(T), value:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false>
Op<name=AssignSubVariableOp; signature=resource:resource, value:dtype -> ; attr=dtype:type; is_stateful=true>
Op<name=AssignVariableOp; signature=resource:resource, value:dtype -> ; attr=dtype:type; attr=validate_shape:bool,default=false; is_stateful=true>
Op<name=AssignVariableXlaConcatND; signature=resource:resource, inputs:N*T -> ; attr=T:type; attr=N:int,min=1; attr=num_concats:list(int); attr=paddings:list(int),default=[]; is_stateful=true>
Op<name=Atan; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Atan2; signature=y:T, x:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Atanh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=AudioSpectrogram; signature=input:float -> spectrogram:float; attr=window_size:int; attr=stride:int; attr=magnitude_squared:bool,default=false>
Op<name=AudioSummary; signature=tag:string, tensor:float -> summary:string; attr=sample_rate:float; attr=max_outputs:int,default=3,min=1>
Op<name=AudioSummaryV2; signature=tag:string, tensor:float, sample_rate:float -> summary:string; attr=max_outputs:int,default=3,min=1>
Op<name=AutoShardDataset; signature=input_dataset:variant, num_workers:int64, index:int64 -> handle:variant; attr=auto_shard_policy:int,default=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=num_replicas:int,default=0>
Op<name=AvgPool; signature=value:T -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=AvgPool3D; signature=input:T -> output:T; attr=ksize:list(int),min=5; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=AvgPool3DGrad; signature=orig_input_shape:int32, grad:T -> output:T; attr=ksize:list(int),min=5; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=AvgPoolGrad; signature=orig_input_shape:int32, grad:T -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=BandedTriangularSolve; signature=matrix:T, rhs:T -> output:T; attr=lower:bool,default=true; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Barrier; signature= -> handle:Ref(string); attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=BarrierClose; signature=handle:Ref(string) -> ; attr=cancel_pending_enqueues:bool,default=false>
Op<name=BarrierIncompleteSize; signature=handle:Ref(string) -> size:int32>
Op<name=BarrierInsertMany; signature=handle:Ref(string), keys:string, values:T -> ; attr=T:type; attr=component_index:int>
Op<name=BarrierReadySize; signature=handle:Ref(string) -> size:int32>
Op<name=BarrierTakeMany; signature=handle:Ref(string), num_elements:int32 -> indices:int64, keys:string, values:; attr=component_types:list(type),min=1; attr=allow_small_batch:bool,default=false; attr=wait_for_incomplete:bool,default=false; attr=timeout_ms:int,default=-1>
Op<name=Batch; signature=in_tensors: -> batched_tensors:, batch_index:int64, id:int64; attr=num_batch_threads:int; attr=max_batch_size:int; attr=max_enqueued_batches:int,default=10; attr=batch_timeout_micros:int; attr=allowed_batch_sizes:list(int),default=[]; attr=grad_timeout_micros:int; attr=container:string,default=""; attr=shared_name:string,default=""; attr=batching_queue:string,default=""; attr=T:list(type),min=1; is_distributed_communication=true>
Op<name=BatchCholesky; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchCholeskyGrad; signature=l:T, grad:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=BatchDataset; signature=input_dataset:variant, batch_size:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=BatchDatasetV2; signature=input_dataset:variant, batch_size:int64, drop_remainder:bool -> handle:variant; attr=parallel_copy:bool,default=false; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=BatchFFT; signature=input:complex64 -> output:complex64>
Op<name=BatchFFT2D; signature=input:complex64 -> output:complex64>
Op<name=BatchFFT3D; signature=input:complex64 -> output:complex64>
Op<name=BatchFunction; signature=in_tensors:, captured_tensors: -> out_tensors:; attr=f:func; attr=num_batch_threads:int; attr=max_batch_size:int; attr=batch_timeout_micros:int; attr=max_enqueued_batches:int,default=10; attr=allowed_batch_sizes:list(int),default=[]; attr=container:string,default=""; attr=shared_name:string,default=""; attr=batching_queue:string,default=""; attr=low_priority_max_batch_size:int,default=0; attr=low_priority_batch_timeout_micros:int,default=0; attr=low_priority_allowed_batch_sizes:list(int),default=[]; attr=low_priority_max_enqueued_batches:int,default=0; attr=Tin:list(type),min=1; attr=Tcaptured:list(type),min=0; attr=Tout:list(type),min=1; attr=enable_large_batch_splitting:bool,default=false; is_distributed_communication=true>
Op<name=BatchIFFT; signature=input:complex64 -> output:complex64>
Op<name=BatchIFFT2D; signature=input:complex64 -> output:complex64>
Op<name=BatchIFFT3D; signature=input:complex64 -> output:complex64>
Op<name=BatchMatMul; signature=x:T, y:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=grad_x:bool,default=false; attr=grad_y:bool,default=false>
Op<name=BatchMatMulV2; signature=x:T, y:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=grad_x:bool,default=false; attr=grad_y:bool,default=false>
Op<name=BatchMatMulV3; signature=x:Ta, y:Tb -> output:Tout; attr=Ta:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=Tb:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; attr=adj_x:bool,default=false; attr=adj_y:bool,default=false; attr=grad_x:bool,default=false; attr=grad_y:bool,default=false>
Op<name=BatchMatrixBandPart; signature=input:T, num_lower:int64, num_upper:int64 -> band:T; attr=T:type>
Op<name=BatchMatrixDeterminant; signature=input:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=BatchMatrixDiag; signature=diagonal:T -> output:T; attr=T:type>
Op<name=BatchMatrixDiagPart; signature=input:T -> diagonal:T; attr=T:type>
Op<name=BatchMatrixInverse; signature=input:T -> output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchMatrixSetDiag; signature=input:T, diagonal:T -> output:T; attr=T:type>
Op<name=BatchMatrixSolve; signature=matrix:T, rhs:T -> output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchMatrixSolveLs; signature=matrix:T, rhs:T, l2_regularizer:double -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]; attr=fast:bool,default=true>
Op<name=BatchMatrixTriangularSolve; signature=matrix:T, rhs:T -> output:T; attr=lower:bool,default=true; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchNormWithGlobalNormalization; signature=t:T, m:T, v:T, beta:T, gamma:T -> result:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=variance_epsilon:float; attr=scale_after_normalization:bool>
Op<name=BatchNormWithGlobalNormalizationGrad; signature=t:T, m:T, v:T, gamma:T, backprop:T -> dx:T, dm:T, dv:T, db:T, dg:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=variance_epsilon:float; attr=scale_after_normalization:bool>
Op<name=BatchSelfAdjointEig; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchSelfAdjointEigV2; signature=input:T -> e:T, v:T; attr=compute_v:bool,default=true; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=BatchSvd; signature=input:T -> s:T, u:T, v:T; attr=compute_uv:bool,default=true; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=BatchToSpace; signature=input:T, crops:Tidx -> output:T; attr=T:type; attr=block_size:int,min=2; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=BatchToSpaceND; signature=input:T, block_shape:Tblock_shape, crops:Tcrops -> output:T; attr=T:type; attr=Tblock_shape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tcrops:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=BesselI0; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselI0e; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselI1; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselI1e; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselJ0; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselJ1; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselK0; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselK0e; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselK1; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselK1e; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselY0; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=BesselY1; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Betainc; signature=a:T, b:T, x:T -> z:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=BiasAdd; signature=value:T, bias:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]>
Op<name=BiasAddGrad; signature=out_backprop:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]>
Op<name=BiasAddV1; signature=value:T, bias:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=Bincount; signature=arr:int32, size:int32, weights:T -> bins:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]>
Op<name=Bitcast; signature=input:T -> output:type; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32]; attr=type:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32]>
Op<name=BitwiseAnd; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]; is_commutative=true>
Op<name=BitwiseOr; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]; is_commutative=true>
Op<name=BitwiseXor; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]; is_commutative=true>
Op<name=BlockLSTM; signature=seq_len_max:int64, x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T -> i:T, cs:T, f:T, o:T, ci:T, co:T, h:T; attr=forget_bias:float,default=1; attr=cell_clip:float,default=3; attr=use_peephole:bool,default=false; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=BlockLSTMGrad; signature=seq_len_max:int64, x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T, i:T, cs:T, f:T, o:T, ci:T, co:T, h:T, cs_grad:T, h_grad:T -> x_grad:T, cs_prev_grad:T, h_prev_grad:T, w_grad:T, wci_grad:T, wcf_grad:T, wco_grad:T, b_grad:T; attr=use_peephole:bool; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=BlockLSTMGradV2; signature=seq_len_max:int64, x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T, i:T, cs:T, f:T, o:T, ci:T, co:T, h:T, cs_grad:T, h_grad:T -> x_grad:T, cs_prev_grad:T, h_prev_grad:T, w_grad:T, wci_grad:T, wcf_grad:T, wco_grad:T, b_grad:T; attr=use_peephole:bool; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=BlockLSTMV2; signature=seq_len_max:int64, x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T -> i:T, cs:T, f:T, o:T, ci:T, co:T, h:T; attr=cell_clip:float,default=0; attr=use_peephole:bool,default=false; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=BoostedTreesAggregateStats; signature=node_ids:int32, gradients:float, hessians:float, feature:int32 -> stats_summary:float; attr=max_splits:int,min=1; attr=num_buckets:int,min=1>
Op<name=BoostedTreesBucketize; signature=float_values:num_features*float, bucket_boundaries:num_features*float -> buckets:num_features*int32; attr=num_features:int,min=0>
Op<name=BoostedTreesCalculateBestFeatureSplit; signature=node_id_range:int32, stats_summary:float, l1:float, l2:float, tree_complexity:float, min_node_weight:float -> node_ids:int32, gains:float, feature_dimensions:int32, thresholds:int32, left_node_contribs:float, right_node_contribs:float, split_with_default_directions:string; attr=logits_dimension:int,min=1; attr=split_type:string,default="inequality",allowed=["inequality", "equality"]>
Op<name=BoostedTreesCalculateBestFeatureSplitV2; signature=node_id_range:int32, stats_summaries_list:num_features*float, split_types:string, candidate_feature_ids:int32, l1:float, l2:float, tree_complexity:float, min_node_weight:float -> node_ids:int32, gains:float, feature_ids:int32, feature_dimensions:int32, thresholds:int32, left_node_contribs:float, right_node_contribs:float, split_with_default_directions:string; attr=num_features:int,min=1; attr=logits_dimension:int,min=1>
Op<name=BoostedTreesCalculateBestGainsPerFeature; signature=node_id_range:int32, stats_summary_list:num_features*float, l1:float, l2:float, tree_complexity:float, min_node_weight:float -> node_ids_list:num_features*int32, gains_list:num_features*float, thresholds_list:num_features*int32, left_node_contribs_list:num_features*float, right_node_contribs_list:num_features*float; attr=max_splits:int,min=1; attr=num_features:int,min=1>
Op<name=BoostedTreesCenterBias; signature=tree_ensemble_handle:resource, mean_gradients:float, mean_hessians:float, l1:float, l2:float -> continue_centering:bool; is_stateful=true>
Op<name=BoostedTreesCreateEnsemble; signature=tree_ensemble_handle:resource, stamp_token:int64, tree_ensemble_serialized:string -> ; is_stateful=true>
Op<name=BoostedTreesCreateQuantileStreamResource; signature=quantile_stream_resource_handle:resource, epsilon:float, num_streams:int64 -> ; attr=max_elements:int,default=1099511627776; is_stateful=true>
Op<name=BoostedTreesDeserializeEnsemble; signature=tree_ensemble_handle:resource, stamp_token:int64, tree_ensemble_serialized:string -> ; is_stateful=true>
Op<name=BoostedTreesEnsembleResourceHandleOp; signature= -> resource:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=BoostedTreesExampleDebugOutputs; signature=tree_ensemble_handle:resource, bucketized_features:num_bucketized_features*int32 -> examples_debug_outputs_serialized:string; attr=num_bucketized_features:int,min=1; attr=logits_dimension:int; is_stateful=true>
Op<name=BoostedTreesFlushQuantileSummaries; signature=quantile_stream_resource_handle:resource -> summaries:num_features*float; attr=num_features:int,min=0; is_stateful=true>
Op<name=BoostedTreesGetEnsembleStates; signature=tree_ensemble_handle:resource -> stamp_token:int64, num_trees:int32, num_finalized_trees:int32, num_attempted_layers:int32, last_layer_nodes_range:int32; is_stateful=true>
Op<name=BoostedTreesMakeQuantileSummaries; signature=float_values:num_features*float, example_weights:float, epsilon:float -> summaries:num_features*float; attr=num_features:int,min=0>
Op<name=BoostedTreesMakeStatsSummary; signature=node_ids:int32, gradients:float, hessians:float, bucketized_features_list:num_features*int32 -> stats_summary:float; attr=max_splits:int,min=1; attr=num_buckets:int,min=1; attr=num_features:int,min=1>
Op<name=BoostedTreesPredict; signature=tree_ensemble_handle:resource, bucketized_features:num_bucketized_features*int32 -> logits:float; attr=num_bucketized_features:int,min=1; attr=logits_dimension:int; is_stateful=true>
Op<name=BoostedTreesQuantileStreamResourceAddSummaries; signature=quantile_stream_resource_handle:resource, summaries:num_features*float -> ; attr=num_features:int,min=0; is_stateful=true>
Op<name=BoostedTreesQuantileStreamResourceDeserialize; signature=quantile_stream_resource_handle:resource, bucket_boundaries:num_streams*float -> ; attr=num_streams:int,min=1; is_stateful=true>
Op<name=BoostedTreesQuantileStreamResourceFlush; signature=quantile_stream_resource_handle:resource, num_buckets:int64 -> ; attr=generate_quantiles:bool,default=false; is_stateful=true>
Op<name=BoostedTreesQuantileStreamResourceGetBucketBoundaries; signature=quantile_stream_resource_handle:resource -> bucket_boundaries:num_features*float; attr=num_features:int,min=0; is_stateful=true>
Op<name=BoostedTreesQuantileStreamResourceHandleOp; signature= -> resource:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=BoostedTreesSerializeEnsemble; signature=tree_ensemble_handle:resource -> stamp_token:int64, tree_ensemble_serialized:string; is_stateful=true>
Op<name=BoostedTreesSparseAggregateStats; signature=node_ids:int32, gradients:float, hessians:float, feature_indices:int32, feature_values:int32, feature_shape:int32 -> stats_summary_indices:int32, stats_summary_values:float, stats_summary_shape:int32; attr=max_splits:int,min=1; attr=num_buckets:int,min=1>
Op<name=BoostedTreesSparseCalculateBestFeatureSplit; signature=node_id_range:int32, stats_summary_indices:int32, stats_summary_values:float, stats_summary_shape:int32, l1:float, l2:float, tree_complexity:float, min_node_weight:float -> node_ids:int32, gains:float, feature_dimensions:int32, thresholds:int32, left_node_contribs:float, right_node_contribs:float, split_with_default_directions:string; attr=logits_dimension:int,min=1; attr=split_type:string,default="inequality",allowed=["inequality"]>
Op<name=BoostedTreesTrainingPredict; signature=tree_ensemble_handle:resource, cached_tree_ids:int32, cached_node_ids:int32, bucketized_features:num_bucketized_features*int32 -> partial_logits:float, tree_ids:int32, node_ids:int32; attr=num_bucketized_features:int,min=1; attr=logits_dimension:int; is_stateful=true>
Op<name=BoostedTreesUpdateEnsemble; signature=tree_ensemble_handle:resource, feature_ids:int32, node_ids:num_features*int32, gains:num_features*float, thresholds:num_features*int32, left_node_contribs:num_features*float, right_node_contribs:num_features*float, max_depth:int32, learning_rate:float -> ; attr=pruning_mode:int,min=0; attr=num_features:int,min=0; is_stateful=true>
Op<name=BoostedTreesUpdateEnsembleV2; signature=tree_ensemble_handle:resource, feature_ids:num_groups*int32, dimension_ids:num_features*int32, node_ids:num_features*int32, gains:num_features*float, thresholds:num_features*int32, left_node_contribs:num_features*float, right_node_contribs:num_features*float, split_types:num_features*string, max_depth:int32, learning_rate:float, pruning_mode:int32 -> ; attr=num_features:int,min=0; attr=logits_dimension:int,default=1; attr=num_groups:int,default=1,min=1; is_stateful=true>
Op<name=BroadcastArgs; signature=s0:T, s1:T -> r0:T; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=BroadcastGradientArgs; signature=s0:T, s1:T -> r0:T, r1:T; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=BroadcastTo; signature=input:T, shape:Tidx -> output:T; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Bucketize; signature=input:T -> output:int32; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=boundaries:list(float)>
Op<name=BytesProducedStatsDataset; signature=input_dataset:variant, tag:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=CSRSparseMatrixComponents; signature=csr_sparse_matrix:variant, index:int32 -> row_ptrs:int32, col_inds:int32, values:type; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=CSRSparseMatrixToDense; signature=sparse_input:variant -> dense_output:type; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=CSRSparseMatrixToSparseTensor; signature=sparse_matrix:variant -> indices:int64, values:type, dense_shape:int64; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=CSVDataset; signature=filenames:string, compression_type:string, buffer_size:int64, header:bool, field_delim:string, use_quote_delim:bool, na_value:string, select_cols:int64, record_defaults: -> handle:variant; attr=output_types:list(type),min=1,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING]; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=CSVDatasetV2; signature=filenames:string, compression_type:string, buffer_size:int64, header:bool, field_delim:string, use_quote_delim:bool, na_value:string, select_cols:int64, record_defaults:, exclude_cols:int64 -> handle:variant; attr=output_types:list(type),min=1,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING]; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=CTCBeamSearchDecoder; signature=inputs:T, sequence_length:int32 -> decoded_indices:top_paths*int64, decoded_values:top_paths*int64, decoded_shape:top_paths*int64, log_probability:T; attr=beam_width:int,min=1; attr=top_paths:int,min=1; attr=merge_repeated:bool,default=true; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=CTCGreedyDecoder; signature=inputs:T, sequence_length:int32 -> decoded_indices:int64, decoded_values:int64, decoded_shape:int64, log_probability:T; attr=merge_repeated:bool,default=false; attr=blank_index:int,default=-1; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=CTCLoss; signature=inputs:T, labels_indices:int64, labels_values:int32, sequence_length:int32 -> loss:T, gradient:T; attr=preprocess_collapse_repeated:bool,default=false; attr=ctc_merge_repeated:bool,default=true; attr=ignore_longer_outputs_than_inputs:bool,default=false; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=CTCLossV2; signature=inputs:float, labels_indices:int64, labels_values:int32, sequence_length:int32 -> loss:float, gradient:float; attr=preprocess_collapse_repeated:bool,default=false; attr=ctc_merge_repeated:bool,default=true; attr=ignore_longer_outputs_than_inputs:bool,default=false>
Op<name=CacheDataset; signature=input_dataset:variant, filename:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=CacheDatasetV2; signature=input_dataset:variant, filename:string, cache:resource -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=Case; signature=branch_index:int32, input: -> output:; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=branches:list(func),min=1; attr=output_shapes:list(shape),default=[]; is_stateful=true>
Op<name=Cast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type; attr=Truncate:bool,default=false>
Op<name=Ceil; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=CheckNumerics; signature=tensor:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=message:string; is_stateful=true>
Op<name=CheckNumericsV2; signature=tensor:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=message:string; is_stateful=true>
Op<name=Cholesky; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=CholeskyGrad; signature=l:T, grad:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=ChooseFastestBranchDataset; signature=input_dataset:variant, ratio_numerator:int64, ratio_denominator:int64, other_arguments: -> handle:variant; attr=Targuments:list(type),min=0; attr=num_elements_per_branch:int,min=1; attr=branches:list(func),min=1; attr=other_arguments_lengths:list(int),min=1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ChooseFastestDataset; signature=input_datasets:N*variant -> handle:variant; attr=N:int,min=2; attr=num_experiments:int; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ClipByValue; signature=t:T, clip_value_min:T, clip_value_max:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=CloseSummaryWriter; signature=writer:resource -> ; is_stateful=true>
Op<name=CollateTPUEmbeddingMemory; signature=memory_configs:N*string -> merged_memory_config:string; attr=N:int,min=1; is_stateful=true>
Op<name=CollectiveAllToAllV2; signature=input:T, group_size:int32, group_key:int32, instance_key:int32, ordering_token:Nordering_token*resource -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; attr=is_stateless:bool,default=false; attr=Nordering_token:int,default=0,min=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveAllToAllV3; signature=input:T, communicator:resource, group_assignment:int32 -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveAssignGroupV2; signature=group_assignment:int32, device_index:int32, base_key:int32 -> group_size:int32, group_key:int32; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveBcastRecv; signature= -> data:T; attr=T:type,allowed=[DT_BOOL, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=group_size:int; attr=group_key:int; attr=instance_key:int; attr=shape:shape; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveBcastRecvV2; signature=group_size:int32, group_key:int32, instance_key:int32, shape:Tshape -> data:T; attr=T:type,allowed=[DT_BOOL, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveBcastSend; signature=input:T -> data:T; attr=T:type,allowed=[DT_BOOL, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=group_size:int; attr=group_key:int; attr=instance_key:int; attr=shape:shape; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveBcastSendV2; signature=input:T, group_size:int32, group_key:int32, instance_key:int32 -> data:T; attr=T:type,allowed=[DT_BOOL, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveGather; signature=input:T -> data:T; attr=T:type,allowed=[DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=group_size:int; attr=group_key:int; attr=instance_key:int; attr=shape:shape; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveGatherV2; signature=input:T, group_size:int32, group_key:int32, instance_key:int32, ordering_token:Nordering_token*resource -> data:T; attr=T:type,allowed=[DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; attr=is_stateless:bool,default=false; attr=Nordering_token:int,default=0,min=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveInitializeCommunicator; signature=group_key:int32, rank:int32, group_size:int32 -> communicator:resource; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectivePermute; signature=input:T, source_target_pairs:int32 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; is_stateful=true>
Op<name=CollectiveReduce; signature=input:T -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=group_size:int; attr=group_key:int; attr=instance_key:int; attr=merge_op:string,allowed=["Min", "Max", "Mul", "Add"]; attr=final_op:string,allowed=["Id", "Div"]; attr=subdiv_offsets:list(int); attr=wait_for:list(int),default=[]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveReduceScatterV2; signature=input:T, group_size:int32, group_key:int32, instance_key:int32, ordering_token:Nordering_token*resource -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=merge_op:string,allowed=["Min", "Max", "Mul", "Add"]; attr=final_op:string,allowed=["Id", "Div"]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; attr=is_stateless:bool,default=false; attr=Nordering_token:int,default=0,min=0; attr=max_subdivs_per_device:int,default=-1; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveReduceV2; signature=input:T, group_size:int32, group_key:int32, instance_key:int32, ordering_token:Nordering_token*resource -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=merge_op:string,allowed=["Min", "Max", "Mul", "Add"]; attr=final_op:string,allowed=["Id", "Div"]; attr=communication_hint:string,default="auto"; attr=timeout_seconds:float,default=0; attr=is_stateless:bool,default=false; attr=Nordering_token:int,default=0,min=0; attr=max_subdivs_per_device:int,default=-1; is_stateful=true; is_distributed_communication=true>
Op<name=CollectiveReduceV3; signature=input:T, communicator:resource, group_assignment:int32 -> data:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT32, DT_INT64]; attr=reduction:string,allowed=["Min", "Max", "Mul", "Add"]; attr=timeout_seconds:float,default=0; is_stateful=true; is_distributed_communication=true>
Op<name=CombinedNonMaxSuppression; signature=boxes:float, scores:float, max_output_size_per_class:int32, max_total_size:int32, iou_threshold:float, score_threshold:float -> nmsed_boxes:float, nmsed_scores:float, nmsed_classes:float, valid_detections:int32; attr=pad_per_class:bool,default=false; attr=clip_boxes:bool,default=true>
Op<name=Complex; signature=real:T, imag:T -> out:Tout; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tout:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=ComplexAbs; signature=x:T -> y:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=CompositeTensorVariantFromComponents; signature=components: -> encoded:variant; attr=metadata:string; attr=Tcomponents:list(type),min=0>
Op<name=CompositeTensorVariantToComponents; signature=encoded:variant -> components:; attr=metadata:string; attr=Tcomponents:list(type),min=0>
Op<name=CompressElement; signature=components: -> compressed:variant; attr=input_types:list(type),min=1>
Op<name=ComputeAccidentalHits; signature=true_classes:int64, sampled_candidates:int64 -> indices:int32, ids:int64, weights:float; attr=num_true:int; attr=seed:int,default=0; attr=seed2:int,default=0>
Op<name=ComputeBatchSize; signature=input_dataset:variant -> batch_size:int64>
Op<name=ComputeDedupDataSize; signature= -> num_elements:int32; attr=config:string; is_stateful=true>
Op<name=ComputeDedupDataTupleMask; signature= -> output_shape:int32; attr=config:string; is_stateful=true>
Op<name=Concat; signature=concat_dim:int32, values:N*T -> output:T; attr=N:int,min=2; attr=T:type>
Op<name=ConcatOffset; signature=concat_dim:int32, shape:N*shape_type -> offset:N*shape_type; attr=N:int,min=2; attr=shape_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ConcatV2; signature=values:N*T, axis:Tidx -> output:T; attr=N:int,min=2; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ConcatenateDataset; signature=input_dataset:variant, another_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=""; attr=shared_name:string,default=""; attr=reduction_type:string,default="MEAN",allowed=["MEAN", "SUM"]; is_stateful=true>
Op<name=ConfigureAndInitializeGlobalTPU; signature= -> output:int32; attr=use_tfrt_host_runtime:bool,default=true; is_stateful=true>
Op<name=ConfigureDistributedTPU; signature= -> topology:string; attr=embedding_config:string,default=""; attr=tpu_embedding_config:string,default=""; attr=is_global_init:bool,default=false; attr=enable_whole_mesh_compilations:bool,default=false; attr=compilation_failure_closes_chips:bool,default=true; attr=tpu_cancellation_closes_chips:int,default=0; is_stateful=true>
Op<name=ConfigureTPUEmbedding; signature= -> ; attr=config:string; is_stateful=true>
Op<name=ConfigureTPUEmbeddingHost; signature=common_config:string, memory_config:string -> network_config:string; attr=config:string; is_stateful=true>
Op<name=ConfigureTPUEmbeddingMemory; signature=common_config:string -> memory_config:string; is_stateful=true>
Op<name=Conj; signature=input:T -> output:T; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128, DT_VARIANT]>
Op<name=ConjugateTranspose; signature=x:T, perm:Tperm -> y:T; attr=T:type; attr=Tperm:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ConnectTPUEmbeddingHosts; signature=network_configs:N*string -> ; attr=N:int,min=1; is_stateful=true>
Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>
Op<name=ConsumeMutexLock; signature=mutex_lock:variant -> ; is_stateful=true>
Op<name=ControlTrigger; signature= -> >
Op<name=Conv; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="CHANNELS_LAST",allowed=["CHANNELS_FIRST", "CHANNELS_LAST"]; attr=dilations:list(int),default=[]; attr=batch_dims:int,default=1; attr=groups:int,default=1>
Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Conv2DBackpropFilter; signature=input:T, filter_sizes:int32, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Conv2DBackpropFilterV2; signature=input:T, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Conv2DBackpropInput; signature=input_sizes:int32, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Conv2DBackpropInputV2; signature=input:T, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Conv3D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=dilations:list(int),default=[1, 1, 1, 1, 1]>
Op<name=Conv3DBackpropFilter; signature=input:T, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1, 1]>
Op<name=Conv3DBackpropFilterV2; signature=input:T, filter_sizes:int32, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=dilations:list(int),default=[1, 1, 1, 1, 1]>
Op<name=Conv3DBackpropInput; signature=input:T, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1, 1]>
Op<name=Conv3DBackpropInputV2; signature=input_sizes:Tshape, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=dilations:list(int),default=[1, 1, 1, 1, 1]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ConvertToCooTensor; signature=indices_or_row_splits:int32, values:int32, weights:float -> row_ids:int32, col_ids:int32, gains:float; attr=sample_count:int,min=1; attr=combiner:string; is_stateful=true>
Op<name=Copy; signature=input:T -> output:T; attr=T:type; attr=tensor_name:string,default=""; attr=debug_ops_spec:list(string),default=[]; allows_uninitialized_input=true>
Op<name=CopyHost; signature=input:T -> output:T; attr=T:type; attr=tensor_name:string,default=""; attr=debug_ops_spec:list(string),default=[]; allows_uninitialized_input=true>
Op<name=CopyToMesh; signature=input:T -> output:T; attr=mesh:string; attr=T:type>
Op<name=CopyToMeshGrad; signature=input:T, forward_input:T -> output:T; attr=T:type>
Op<name=Cos; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Cosh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=CountUpTo; signature=ref:Ref(T) -> output:T; attr=limit:int; attr=T:type,allowed=[DT_INT32, DT_INT64]>
Op<name=CreateSummaryDbWriter; signature=writer:resource, db_uri:string, experiment_name:string, run_name:string, user_name:string -> ; is_stateful=true>
Op<name=CreateSummaryFileWriter; signature=writer:resource, logdir:string, max_queue:int32, flush_millis:int32, filename_suffix:string -> ; is_stateful=true>
Op<name=CropAndResize; signature=image:T, boxes:float, box_ind:int32, crop_size:int32 -> crops:float; attr=T:type,allowed=[DT_UINT8, DT_UINT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=method:string,default="bilinear",allowed=["bilinear", "nearest"]; attr=extrapolation_value:float,default=0>
Op<name=CropAndResizeGradBoxes; signature=grads:float, image:T, boxes:float, box_ind:int32 -> output:float; attr=T:type,allowed=[DT_UINT8, DT_UINT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=method:string,default="bilinear",allowed=["bilinear"]>
Op<name=CropAndResizeGradImage; signature=grads:float, boxes:float, box_ind:int32, image_size:int32 -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_HALF, DT_DOUBLE]; attr=method:string,default="bilinear",allowed=["bilinear", "nearest"]>
Op<name=Cross; signature=a:T, b:T -> product:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=CrossReplicaSum; signature=input:T, group_assignment:int32 -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT32]; is_stateful=true>
Op<name=CudnnRNN; signature=input:T, input_h:T, input_c:T, params:T -> output:T, output_h:T, output_c:T, reserve_space:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=is_training:bool,default=true; is_stateful=true>
Op<name=CudnnRNNBackprop; signature=input:T, input_h:T, input_c:T, params:T, output:T, output_h:T, output_c:T, output_backprop:T, output_h_backprop:T, output_c_backprop:T, reserve_space:T -> input_backprop:T, input_h_backprop:T, input_c_backprop:T, params_backprop:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=CudnnRNNBackpropV2; signature=input:T, input_h:T, input_c:T, params:T, output:T, output_h:T, output_c:T, output_backprop:T, output_h_backprop:T, output_c_backprop:T, reserve_space:T, host_reserved:int8 -> input_backprop:T, input_h_backprop:T, input_c_backprop:T, params_backprop:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=CudnnRNNBackpropV3; signature=input:T, input_h:T, input_c:T, params:T, sequence_lengths:int32, output:T, output_h:T, output_c:T, output_backprop:T, output_h_backprop:T, output_c_backprop:T, reserve_space:T, host_reserved:int8 -> input_backprop:T, input_h_backprop:T, input_c_backprop:T, params_backprop:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=num_proj:int,default=0; attr=time_major:bool,default=true; is_stateful=true>
Op<name=CudnnRNNCanonicalToParams; signature=num_layers:int32, num_units:int32, input_size:int32, weights:num_params*T, biases:num_params*T -> params:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=num_params:int,min=1; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0>
Op<name=CudnnRNNCanonicalToParamsV2; signature=num_layers:int32, num_units:int32, input_size:int32, weights:num_params_weights*T, biases:num_params_biases*T -> params:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=num_params_weights:int,min=1; attr=num_params_biases:int,min=1; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=num_proj:int,default=0>
Op<name=CudnnRNNParamsSize; signature=num_layers:int32, num_units:int32, input_size:int32 -> params_size:S; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=num_proj:int,default=0>
Op<name=CudnnRNNParamsToCanonical; signature=num_layers:int32, num_units:int32, input_size:int32, params:T -> weights:num_params*T, biases:num_params*T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=num_params:int,min=1; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0>
Op<name=CudnnRNNParamsToCanonicalV2; signature=num_layers:int32, num_units:int32, input_size:int32, params:T -> weights:num_params_weights*T, biases:num_params_biases*T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=num_params_weights:int,min=1; attr=num_params_biases:int,min=1; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=num_proj:int,default=0>
Op<name=CudnnRNNV2; signature=input:T, input_h:T, input_c:T, params:T -> output:T, output_h:T, output_c:T, reserve_space:T, host_reserved:int8; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=is_training:bool,default=true; is_stateful=true>
Op<name=CudnnRNNV3; signature=input:T, input_h:T, input_c:T, params:T, sequence_lengths:int32 -> output:T, output_h:T, output_c:T, reserve_space:T, host_reserved:int8; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=rnn_mode:string,default="lstm",allowed=["rnn_relu", "rnn_tanh", "lstm", "gru"]; attr=input_mode:string,default="linear_input",allowed=["linear_input", "skip_input", "auto_select"]; attr=direction:string,default="unidirectional",allowed=["unidirectional", "bidirectional"]; attr=dropout:float,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=num_proj:int,default=0; attr=is_training:bool,default=true; attr=time_major:bool,default=true; is_stateful=true>
Op<name=Cumprod; signature=x:T, axis:Tidx -> out:T; attr=exclusive:bool,default=false; attr=reverse:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Cumsum; signature=x:T, axis:Tidx -> out:T; attr=exclusive:bool,default=false; attr=reverse:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=CumulativeLogsumexp; signature=x:T, axis:Tidx -> out:T; attr=exclusive:bool,default=false; attr=reverse:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=DTensorRestoreV2; signature=prefix:string, tensor_names:string, shape_and_slices:string -> tensors:; attr=input_shapes:list(shape); attr=input_layouts:list(string); attr=dtypes:list(type),min=1; is_stateful=true>
Op<name=DTensorSetGlobalTPUArray; signature=topology:string -> ; is_stateful=true>
Op<name=DataFormatDimMap; signature=x:T -> y:T; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=src_format:string,default="NHWC"; attr=dst_format:string,default="NCHW">
Op<name=DataFormatVecPermute; signature=x:T -> y:T; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=src_format:string,default="NHWC"; attr=dst_format:string,default="NCHW">
Op<name=DataServiceDataset; signature=dataset_id:int64, processing_mode:string, address:string, protocol:string, job_name:string, max_outstanding_requests:int64, iteration_counter:resource -> handle:variant; attr=task_refresh_interval_hint_ms:int,default=-1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=data_transfer_protocol:string,default=""; attr=target_workers:string,default="AUTO"; attr=cross_trainer_cache_options:string,default=""; is_stateful=true>
Op<name=DataServiceDatasetV2; signature=dataset_id:int64, processing_mode:string, address:string, protocol:string, job_name:string, consumer_index:int64, num_consumers:int64, max_outstanding_requests:int64, iteration_counter:resource -> handle:variant; attr=task_refresh_interval_hint_ms:int,default=-1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=data_transfer_protocol:string,default=""; attr=target_workers:string,default="AUTO"; attr=cross_trainer_cache_options:string,default=""; is_stateful=true>
Op<name=DataServiceDatasetV3; signature=dataset_id:int64, processing_mode:string, address:string, protocol:string, job_name:string, consumer_index:int64, num_consumers:int64, max_outstanding_requests:int64, iteration_counter:resource -> handle:variant; attr=task_refresh_interval_hint_ms:int,default=-1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=data_transfer_protocol:string,default=""; attr=target_workers:string,default="AUTO"; attr=uncompress:bool,default=false; attr=uncompress_fn:func; attr=cross_trainer_cache_options:string,default=""; is_stateful=true>
Op<name=DataServiceDatasetV4; signature=dataset_id:string, processing_mode:string, address:string, protocol:string, job_name:string, consumer_index:int64, num_consumers:int64, max_outstanding_requests:int64, iteration_counter:resource -> handle:variant; attr=task_refresh_interval_hint_ms:int,default=-1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=data_transfer_protocol:string,default=""; attr=target_workers:string,default="AUTO"; attr=uncompress:bool,default=false; attr=uncompress_fn:func; attr=cross_trainer_cache_options:string,default=""; is_stateful=true>
Op<name=DatasetCardinality; signature=input_dataset:variant -> cardinality:int64; attr=cardinality_options:string,default="">
Op<name=DatasetFromGraph; signature=graph_def:string -> handle:variant>
Op<name=DatasetToGraph; signature=input_dataset:variant -> graph:string; attr=stateful_whitelist:list(string),default=[],min=0; attr=allow_stateful:bool,default=false; attr=strip_device_assignment:bool,default=false>
Op<name=DatasetToGraphV2; signature=input_dataset:variant -> graph:string; attr=external_state_policy:int,default=0; attr=strip_device_assignment:bool,default=false>
Op<name=DatasetToSingleElement; signature=dataset:variant -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=DatasetToTFRecord; signature=input_dataset:variant, filename:string, compression_type:string -> ; is_stateful=true>
Op<name=Dawsn; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=DebugGradientIdentity; signature=input:T -> output:T; attr=T:type; allows_uninitialized_input=true>
Op<name=DebugGradientRefIdentity; signature=input:Ref(T) -> output:Ref(T); attr=T:type; allows_uninitialized_input=true>
Op<name=DebugIdentity; signature=input:T -> output:T; attr=T:type; attr=device_name:string,default=""; attr=tensor_name:string,default=""; attr=debug_urls:list(string),default=[]; attr=gated_grpc:bool,default=false; allows_uninitialized_input=true>
Op<name=DebugIdentityV2; signature=input:T -> output:T; attr=T:type; attr=tfdbg_context_id:string,default=""; attr=op_name:string,default=""; attr=output_slot:int,default=-1; attr=tensor_debug_mode:int,default=-1; attr=debug_urls:list(string),default=[]; attr=circular_buffer_size:int,default=1000; attr=tfdbg_run_id:string,default=""; is_stateful=true>
Op<name=DebugIdentityV3; signature=input:T -> output:T; attr=T:type; attr=device_name:string,default=""; attr=tensor_name:string,default=""; attr=io_of_node:string,default=""; attr=is_input:bool,default=false; attr=io_index:int,default=-1; attr=debug_urls:list(string),default=[]; attr=gated_grpc:bool,default=false; is_stateful=true; allows_uninitialized_input=true>
Op<name=DebugNanCount; signature=input:T -> output:int64; attr=T:type; attr=device_name:string,default=""; attr=tensor_name:string,default=""; attr=debug_urls:list(string),default=[]; attr=gated_grpc:bool,default=false; allows_uninitialized_input=true>
Op<name=DebugNumericSummary; signature=input:T -> output:double; attr=T:type; attr=device_name:string,default=""; attr=tensor_name:string,default=""; attr=debug_urls:list(string),default=[]; attr=lower_bound:float,default=-inf; attr=upper_bound:float,default=inf; attr=mute_if_healthy:bool,default=false; attr=gated_grpc:bool,default=false; allows_uninitialized_input=true>
Op<name=DebugNumericSummaryV2; signature=input:T -> output:output_dtype; attr=output_dtype:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=T:type; attr=tensor_debug_mode:int,default=-1; attr=tensor_id:int,default=-1>
Op<name=DecodeAndCropJpeg; signature=contents:string, crop_window:int32 -> image:uint8; attr=channels:int,default=0; attr=ratio:int,default=1; attr=fancy_upscaling:bool,default=true; attr=try_recover_truncated:bool,default=false; attr=acceptable_fraction:float,default=1; attr=dct_method:string,default="">
Op<name=DecodeBase64; signature=input:string -> output:string>
Op<name=DecodeBmp; signature=contents:string -> image:uint8; attr=channels:int,default=0>
Op<name=DecodeCSV; signature=records:string, record_defaults: -> output:; attr=OUT_TYPE:list(type),min=1,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING]; attr=field_delim:string,default=","; attr=use_quote_delim:bool,default=true; attr=na_value:string,default=""; attr=select_cols:list(int),default=[]>
Op<name=DecodeCompressed; signature=bytes:string -> output:string; attr=compression_type:string,default="">
Op<name=DecodeGif; signature=contents:string -> image:uint8>
Op<name=DecodeImage; signature=contents:string -> image:dtype; attr=channels:int,default=0; attr=dtype:type,default=DT_UINT8,allowed=[DT_UINT8, DT_UINT16, DT_FLOAT]; attr=expand_animations:bool,default=true>
Op<name=DecodeJSONExample; signature=json_examples:string -> binary_examples:string>
Op<name=DecodeJpeg; signature=contents:string -> image:uint8; attr=channels:int,default=0; attr=ratio:int,default=1; attr=fancy_upscaling:bool,default=true; attr=try_recover_truncated:bool,default=false; attr=acceptable_fraction:float,default=1; attr=dct_method:string,default="">
Op<name=DecodePaddedRaw; signature=input_bytes:string, fixed_length:int32 -> output:out_type; attr=out_type:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT16, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16]; attr=little_endian:bool,default=true>
Op<name=DecodePng; signature=contents:string -> image:dtype; attr=channels:int,default=0; attr=dtype:type,default=DT_UINT8,allowed=[DT_UINT8, DT_UINT16]>
Op<name=DecodeProtoV2; signature=bytes:string -> sizes:int32, values:; attr=message_type:string; attr=field_names:list(string); attr=output_types:list(type),min=0; attr=descriptor_source:string,default="local://"; attr=message_format:string,default="binary"; attr=sanitize:bool,default=false>
Op<name=DecodeRaw; signature=bytes:string -> output:out_type; attr=out_type:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT16, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]; attr=little_endian:bool,default=true>
Op<name=DecodeWav; signature=contents:string -> audio:float, sample_rate:int32; attr=desired_channels:int,default=-1; attr=desired_samples:int,default=-1>
Op<name=DeepCopy; signature=x:T -> y:T; attr=T:type; is_stateful=true>
Op<name=DeleteIterator; signature=handle:resource, deleter:variant -> ; is_stateful=true>
Op<name=DeleteMemoryCache; signature=handle:resource, deleter:variant -> ; is_stateful=true>
Op<name=DeleteMultiDeviceIterator; signature=multi_device_iterator:resource, iterators:N*resource, deleter:variant -> ; attr=N:int,min=0; is_stateful=true>
Op<name=DeleteRandomSeedGenerator; signature=handle:resource, deleter:variant -> ; is_stateful=true>
Op<name=DeleteSeedGenerator; signature=handle:resource, deleter:variant -> ; is_stateful=true>
Op<name=DeleteSessionTensor; signature=handle:string -> ; is_stateful=true>
Op<name=DenseBincount; signature=input:Tidx, size:Tidx, weights:T -> output:T; attr=Tidx:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=binary_output:bool,default=false>
Op<name=DenseCountSparseOutput; signature=values:T, weights:output_type -> output_indices:int64, output_values:output_type, output_dense_shape:int64; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=minlength:int,default=-1,min=-1; attr=maxlength:int,default=-1,min=-1; attr=binary_output:bool; attr=output_type:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]>
Op<name=DenseToCSRSparseMatrix; signature=dense_input:T, indices:int64 -> sparse_output:variant; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=DenseToDenseSetOperation; signature=set1:T, set2:T -> result_indices:int64, result_values:T, result_shape:int64; attr=set_operation:string; attr=validate_indices:bool,default=true; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_STRING]>
Op<name=DenseToSparseBatchDataset; signature=input_dataset:variant, batch_size:int64, row_shape:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=DenseToSparseSetOperation; signature=set1:T, set2_indices:int64, set2_values:T, set2_shape:int64 -> result_indices:int64, result_values:T, result_shape:int64; attr=set_operation:string; attr=validate_indices:bool,default=true; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_STRING]>
Op<name=DepthToSpace; signature=input:T -> output:T; attr=T:type; attr=block_size:int,min=2; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NCHW_VECT_C"]>
Op<name=DepthwiseConv2dNative; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=DepthwiseConv2dNativeBackpropFilter; signature=input:T, filter_sizes:int32, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=DepthwiseConv2dNativeBackpropInput; signature=input_sizes:int32, filter:T, out_backprop:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=Dequantize; signature=input:T, min_range:float, max_range:float -> output:dtype; attr=T:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=mode:string,default="MIN_COMBINED",allowed=["MIN_COMBINED", "MIN_FIRST", "SCALED"]; attr=narrow_range:bool,default=false; attr=axis:int,default=-1; attr=dtype:type,default=DT_FLOAT,allowed=[DT_BFLOAT16, DT_FLOAT]>
Op<name=DeserializeIterator; signature=resource_handle:resource, serialized:variant -> ; is_stateful=true>
Op<name=DeserializeManySparse; signature=serialized_sparse:string -> sparse_indices:int64, sparse_values:dtype, sparse_shape:int64; attr=dtype:type>
Op<name=DeserializeSparse; signature=serialized_sparse:Tserialized -> sparse_indices:int64, sparse_values:dtype, sparse_shape:int64; attr=dtype:type; attr=Tserialized:type,default=DT_STRING,allowed=[DT_STRING, DT_VARIANT]>
Op<name=DestroyResourceOp; signature=resource:resource -> ; attr=ignore_lookup_error:bool,default=true; is_stateful=true>
Op<name=DestroyTemporaryVariable; signature=ref:Ref(T) -> value:T; attr=T:type; attr=var_name:string>
Op<name=DeviceIndex; signature= -> index:int32; attr=device_names:list(string); is_stateful=true>
Op<name=Diag; signature=diagonal:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=DiagPart; signature=input:T -> diagonal:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Digamma; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Dilation2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=strides:list(int),min=4; attr=rates:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=Dilation2DBackpropFilter; signature=input:T, filter:T, out_backprop:T -> filter_backprop:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=strides:list(int),min=4; attr=rates:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=Dilation2DBackpropInput; signature=input:T, filter:T, out_backprop:T -> in_backprop:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=strides:list(int),min=4; attr=rates:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=DirectedInterleaveDataset; signature=selector_input_dataset:variant, data_input_datasets:N*variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1; attr=stop_on_empty_dataset:bool,default=false>
Op<name=DisableCopyOnRead; signature=resource:resource -> ; is_stateful=true>
Op<name=DistributedSave; signature=dataset:variant, directory:string, address:string -> ; attr=metadata:string,default=""; is_stateful=true>
Op<name=Div; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=DivNoNan; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_BFLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=DrawBoundingBoxes; signature=images:T, boxes:float -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_HALF]>
Op<name=DrawBoundingBoxesV2; signature=images:T, boxes:float, colors:float -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_HALF]>
Op<name=DummyIterationCounter; signature= -> handle:resource; is_stateful=true>
Op<name=DummyMemoryCache; signature= -> handle:resource; is_stateful=true>
Op<name=DummySeedGenerator; signature= -> handle:resource; is_stateful=true>
Op<name=DynamicEnqueueTPUEmbeddingArbitraryTensorBatch; signature=sample_indices_or_row_splits:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string, device_ordinal:int32 -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=combiners:list(string),default=[]; is_stateful=true>
Op<name=DynamicEnqueueTPUEmbeddingRaggedTensorBatch; signature=sample_splits:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string, device_ordinal:int32 -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=combiners:list(string),default=[]; attr=table_ids:list(int); attr=max_sequence_lengths:list(int),default=[]; attr=num_features:list(int),default=[]; is_stateful=true>
Op<name=DynamicPartition; signature=data:T, partitions:int32 -> outputs:num_partitions*T; attr=num_partitions:int,min=1; attr=T:type>
Op<name=DynamicStitch; signature=indices:N*int32, data:N*T -> merged:T; attr=N:int,min=1; attr=T:type>
Op<name=EagerPyFunc; signature=input: -> output:; attr=token:string; attr=is_async:bool,default=false; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; is_stateful=true>
Op<name=EditDistance; signature=hypothesis_indices:int64, hypothesis_values:T, hypothesis_shape:int64, truth_indices:int64, truth_values:T, truth_shape:int64 -> output:float; attr=normalize:bool,default=true; attr=T:type>
Op<name=Eig; signature=input:T -> e:Tout, v:Tout; attr=compute_v:bool,default=true; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Einsum; signature=inputs:N*T -> output:T; attr=equation:string; attr=N:int,min=1; attr=T:type>
Op<name=Elu; signature=features:T -> activations:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=EluGrad; signature=gradients:T, outputs:T -> backprops:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=Empty; signature=shape:int32 -> output:dtype; attr=dtype:type; attr=init:bool,default=false; is_stateful=true>
Op<name=EmptyTensorList; signature=element_shape:shape_type, max_num_elements:int32 -> handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=EmptyTensorMap; signature= -> handle:variant>
Op<name=EncodeBase64; signature=input:string -> output:string; attr=pad:bool,default=false>
Op<name=EncodeJpeg; signature=image:uint8 -> contents:string; attr=format:string,default="",allowed=["", "grayscale", "rgb"]; attr=quality:int,default=95; attr=progressive:bool,default=false; attr=optimize_size:bool,default=false; attr=chroma_downsampling:bool,default=true; attr=density_unit:string,default="in",allowed=["in", "cm"]; attr=x_density:int,default=300; attr=y_density:int,default=300; attr=xmp_metadata:string,default="">
Op<name=EncodeJpegVariableQuality; signature=images:uint8, quality:int32 -> contents:string>
Op<name=EncodePng; signature=image:T -> contents:string; attr=compression:int,default=-1; attr=T:type,default=DT_UINT8,allowed=[DT_UINT8, DT_UINT16]>
Op<name=EncodeProto; signature=sizes:int32, values: -> bytes:string; attr=field_names:list(string); attr=message_type:string; attr=descriptor_source:string,default="local://"; attr=Tinput_types:list(type),min=1>
Op<name=EncodeWav; signature=audio:float, sample_rate:int32 -> contents:string>
Op<name=EnqueueTPUEmbeddingArbitraryTensorBatch; signature=sample_indices_or_row_splits:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=device_ordinal:int,default=-1; attr=combiners:list(string),default=[]; is_stateful=true>
Op<name=EnqueueTPUEmbeddingBatch; signature=batch:N*string, mode_override:string -> ; attr=N:int,min=1; attr=device_ordinal:int,default=-1; attr=combiners:list(string),default=[]; is_stateful=true>
Op<name=EnqueueTPUEmbeddingIntegerBatch; signature=batch:N*int32, mode_override:string -> ; attr=N:int,min=1; attr=device_ordinal:int,default=-1; is_stateful=true>
Op<name=EnqueueTPUEmbeddingRaggedTensorBatch; signature=sample_splits:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=device_ordinal:int,default=-1; attr=combiners:list(string),default=[]; attr=table_ids:list(int); attr=max_sequence_lengths:list(int),default=[]; attr=num_features:list(int),default=[]; is_stateful=true>
Op<name=EnqueueTPUEmbeddingSparseBatch; signature=sample_indices:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=device_ordinal:int,default=-1; attr=combiners:list(string),default=[]; is_stateful=true>
Op<name=EnqueueTPUEmbeddingSparseTensorBatch; signature=sample_indices:N*T1, embedding_indices:N*T2, aggregation_weights:N*T3, mode_override:string -> ; attr=T1:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T2:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T3:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=N:int,min=1; attr=device_ordinal:int,default=-1; attr=combiners:list(string),default=[]; attr=table_ids:list(int); attr=max_sequence_lengths:list(int),default=[]; attr=num_features:list(int),default=[]; is_stateful=true>
Op<name=EnsureShape; signature=input:T -> output:T; attr=shape:shape; attr=T:type>
Op<name=Enter; signature=data:T -> output:T; attr=T:type; attr=frame_name:string; attr=is_constant:bool,default=false; attr=parallel_iterations:int,default=10>
Op<name=Equal; signature=x:T, y:T -> z:bool; attr=T:type; attr=incompatible_shape_error:bool,default=true; is_commutative=true>
Op<name=Erf; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Erfc; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Erfinv; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=EuclideanNorm; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ExecuteTPUEmbeddingPartitioner; signature= -> common_config:string; attr=config:string; is_stateful=true>
Op<name=Exit; signature=data:T -> output:T; attr=T:type>
Op<name=Exp; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=ExpandDims; signature=input:T, dim:Tdim -> output:T; attr=T:type; attr=Tdim:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ExperimentalAssertNextDataset; signature=input_dataset:variant, transformations:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalAutoShardDataset; signature=input_dataset:variant, num_workers:int64, index:int64 -> handle:variant; attr=auto_shard_policy:int,default=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalBytesProducedStatsDataset; signature=input_dataset:variant, tag:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalCSVDataset; signature=filenames:string, compression_type:string, buffer_size:int64, header:bool, field_delim:string, use_quote_delim:bool, na_value:string, select_cols:int64, record_defaults: -> handle:variant; attr=output_types:list(type),min=1,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING]; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalChooseFastestDataset; signature=input_datasets:N*variant -> handle:variant; attr=N:int,min=2; attr=num_experiments:int; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalDatasetCardinality; signature=input_dataset:variant -> cardinality:int64>
Op<name=ExperimentalDatasetToTFRecord; signature=input_dataset:variant, filename:string, compression_type:string -> ; is_stateful=true>
Op<name=ExperimentalDenseToSparseBatchDataset; signature=input_dataset:variant, batch_size:int64, row_shape:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalDirectedInterleaveDataset; signature=selector_input_dataset:variant, data_input_datasets:N*variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1>
Op<name=ExperimentalGroupByReducerDataset; signature=input_dataset:variant, key_func_other_arguments:, init_func_other_arguments:, reduce_func_other_arguments:, finalize_func_other_arguments: -> handle:variant; attr=key_func:func; attr=init_func:func; attr=reduce_func:func; attr=finalize_func:func; attr=Tkey_func_other_arguments:list(type),min=0; attr=Tinit_func_other_arguments:list(type),min=0; attr=Treduce_func_other_arguments:list(type),min=0; attr=Tfinalize_func_other_arguments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalGroupByWindowDataset; signature=input_dataset:variant, key_func_other_arguments:, reduce_func_other_arguments:, window_size_func_other_arguments: -> handle:variant; attr=key_func:func; attr=reduce_func:func; attr=window_size_func:func; attr=Tkey_func_other_arguments:list(type),min=0; attr=Treduce_func_other_arguments:list(type),min=0; attr=Twindow_size_func_other_arguments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalIgnoreErrorsDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=log_warning:bool,default=false>
Op<name=ExperimentalIteratorGetDevice; signature=resource:resource -> device:string; is_stateful=true>
Op<name=ExperimentalLMDBDataset; signature=filenames:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalLatencyStatsDataset; signature=input_dataset:variant, tag:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalMapAndBatchDataset; signature=input_dataset:variant, other_arguments:, batch_size:int64, num_parallel_calls:int64, drop_remainder:bool -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=preserve_cardinality:bool,default=false>
Op<name=ExperimentalMapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false>
Op<name=ExperimentalMatchingFilesDataset; signature=patterns:string -> handle:variant; is_stateful=true>
Op<name=ExperimentalMaxIntraOpParallelismDataset; signature=input_dataset:variant, max_intra_op_parallelism:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalNonSerializableDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalParallelInterleaveDataset; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, sloppy:bool, buffer_output_elements:int64, prefetch_input_elements:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalParseExampleDataset; signature=input_dataset:variant, num_parallel_calls:int64, dense_defaults: -> handle:variant; attr=sparse_keys:list(string),min=0; attr=dense_keys:list(string),min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=dense_shapes:list(shape),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=sloppy:bool,default=false>
Op<name=ExperimentalPrivateThreadPoolDataset; signature=input_dataset:variant, num_threads:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalRandomDataset; signature=seed:int64, seed2:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalRebatchDataset; signature=input_dataset:variant, num_replicas:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_fallback:bool,default=true>
Op<name=ExperimentalScanDataset; signature=input_dataset:variant, initial_state:, other_arguments: -> handle:variant; attr=f:func; attr=Tstate:list(type),min=1; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=preserve_cardinality:bool,default=false>
Op<name=ExperimentalSetStatsAggregatorDataset; signature=input_dataset:variant, stats_aggregator:resource, tag:string, counter_prefix:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalSleepDataset; signature=input_dataset:variant, sleep_microseconds:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalSlidingWindowDataset; signature=input_dataset:variant, window_size:int64, window_shift:int64, window_stride:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalSqlDataset; signature=driver_name:string, data_source_name:string, query:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalStatsAggregatorHandle; signature= -> handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=ExperimentalStatsAggregatorSummary; signature=iterator:resource -> summary:string; is_stateful=true>
Op<name=ExperimentalTakeWhileDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=predicate:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalThreadPoolDataset; signature=input_dataset:variant, thread_pool:resource -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ExperimentalThreadPoolHandle; signature= -> handle:resource; attr=num_threads:int; attr=max_intra_op_parallelism:int,default=1; attr=display_name:string; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=ExperimentalUnbatchDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=ExperimentalUniqueDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Expint; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Expm1; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=ExtractGlimpse; signature=input:float, size:int32, offsets:float -> glimpse:float; attr=centered:bool,default=true; attr=normalized:bool,default=true; attr=uniform_noise:bool,default=true; attr=noise:string,default="uniform">
Op<name=ExtractGlimpseV2; signature=input:float, size:int32, offsets:float -> glimpse:float; attr=centered:bool,default=true; attr=normalized:bool,default=true; attr=uniform_noise:bool,default=true; attr=noise:string,default="uniform">
Op<name=ExtractImagePatches; signature=images:T -> patches:T; attr=ksizes:list(int),min=4; attr=strides:list(int),min=4; attr=rates:list(int),min=4; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL]; attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=ExtractJpegShape; signature=contents:string -> image_shape:output_type; attr=output_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ExtractVolumePatches; signature=input:T -> patches:T; attr=ksizes:list(int),min=5; attr=strides:list(int),min=5; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=FFT; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=FFT2D; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=FFT3D; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=FFTND; signature=input:Tcomplex, fft_length:int32, axes:int32 -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=FIFOQueue; signature= -> handle:Ref(string); attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=FIFOQueueV2; signature= -> handle:resource; attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=Fact; signature= -> fact:string>
Op<name=FakeParam; signature= -> output:dtype; attr=dtype:type; attr=shape:shape>
Op<name=FakeQuantWithMinMaxArgs; signature=inputs:float -> outputs:float; attr=min:float,default=-6; attr=max:float,default=6; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQuantWithMinMaxArgsGradient; signature=gradients:float, inputs:float -> backprops:float; attr=min:float,default=-6; attr=max:float,default=6; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQuantWithMinMaxVars; signature=inputs:float, min:float, max:float -> outputs:float; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQuantWithMinMaxVarsGradient; signature=gradients:float, inputs:float, min:float, max:float -> backprops_wrt_input:float, backprop_wrt_min:float, backprop_wrt_max:float; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQuantWithMinMaxVarsPerChannel; signature=inputs:float, min:float, max:float -> outputs:float; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQuantWithMinMaxVarsPerChannelGradient; signature=gradients:float, inputs:float, min:float, max:float -> backprops_wrt_input:float, backprop_wrt_min:float, backprop_wrt_max:float; attr=num_bits:int,default=8; attr=narrow_range:bool,default=false>
Op<name=FakeQueue; signature=resource:resource -> handle:Ref(string); is_stateful=true>
Op<name=FileSystemSetConfiguration; signature=scheme:string, key:string, value:string -> ; is_stateful=true>
Op<name=Fill; signature=dims:index_type, value:T -> output:T; attr=T:type; attr=index_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=FilterByLastComponentDataset; signature=input_dataset:variant -> output:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=FilterDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=predicate:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=FinalizeDataset; signature=input_dataset:variant -> handle:variant; attr=has_captured_ref:bool,default=false; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=FinalizeTPUEmbedding; signature=common_config:string, memory_config:string -> ; is_stateful=true>
Op<name=Fingerprint; signature=data:T, method:string -> fingerprint:uint8; attr=T:type>
Op<name=FixedLengthRecordDataset; signature=filenames:string, header_bytes:int64, record_bytes:int64, footer_bytes:int64, buffer_size:int64 -> handle:variant; attr=metadata:string,default=""; is_stateful=true>
Op<name=FixedLengthRecordDatasetV2; signature=filenames:string, header_bytes:int64, record_bytes:int64, footer_bytes:int64, buffer_size:int64, compression_type:string -> handle:variant; attr=metadata:string,default=""; is_stateful=true>
Op<name=FixedLengthRecordReader; signature= -> reader_handle:Ref(string); attr=header_bytes:int,default=0; attr=record_bytes:int; attr=footer_bytes:int,default=0; attr=hop_bytes:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=FixedLengthRecordReaderV2; signature= -> reader_handle:resource; attr=header_bytes:int,default=0; attr=record_bytes:int; attr=footer_bytes:int,default=0; attr=hop_bytes:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; attr=encoding:string,default=""; is_stateful=true>
Op<name=FixedUnigramCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=range_max:int,min=1; attr=vocab_file:string,default=""; attr=distortion:float,default=1; attr=num_reserved_ids:int,default=0; attr=num_shards:int,default=1,min=1; attr=shard:int,default=0,min=0; attr=unigrams:list(float),default=[]; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=FlatMapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=Floor; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=FloorDiv; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=FloorMod; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=FlushSummaryWriter; signature=writer:resource -> ; is_stateful=true>
Op<name=For; signature=start:int32, limit:int32, delta:int32, input: -> output:; attr=T:list(type),min=0; attr=body:func>
Op<name=FractionalAvgPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=FractionalAvgPoolGrad; signature=orig_input_tensor_shape:int64, out_backprop:T, row_pooling_sequence:int64, col_pooling_sequence:int64 -> output:T; attr=overlapping:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=FractionalMaxPool; signature=value:T -> output:T, row_pooling_sequence:int64, col_pooling_sequence:int64; attr=pooling_ratio:list(float),min=4; attr=pseudo_random:bool,default=false; attr=overlapping:bool,default=false; attr=deterministic:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=FractionalMaxPoolGrad; signature=orig_input:T, orig_output:T, out_backprop:T, row_pooling_sequence:int64, col_pooling_sequence:int64 -> output:T; attr=overlapping:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=FresnelCos; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=FresnelSin; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=FusedBatchNorm; signature=x:T, scale:T, offset:T, mean:T, variance:T -> y:T, batch_mean:T, batch_variance:T, reserve_space_1:T, reserve_space_2:T; attr=T:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=exponential_avg_factor:float,default=1; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=is_training:bool,default=true>
Op<name=FusedBatchNormGrad; signature=y_backprop:T, x:T, scale:T, reserve_space_1:T, reserve_space_2:T -> x_backprop:T, scale_backprop:T, offset_backprop:T, reserve_space_3:T, reserve_space_4:T; attr=T:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=is_training:bool,default=true>
Op<name=FusedBatchNormGradV2; signature=y_backprop:T, x:T, scale:float, reserve_space_1:U, reserve_space_2:U -> x_backprop:T, scale_backprop:U, offset_backprop:U, reserve_space_3:U, reserve_space_4:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=is_training:bool,default=true>
Op<name=FusedBatchNormGradV3; signature=y_backprop:T, x:T, scale:float, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U -> x_backprop:T, scale_backprop:U, offset_backprop:U, reserve_space_4:U, reserve_space_5:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NDHWC", "NCDHW"]; attr=is_training:bool,default=true>
Op<name=FusedBatchNormV2; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=exponential_avg_factor:float,default=1; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=is_training:bool,default=true>
Op<name=FusedBatchNormV3; signature=x:T, scale:U, offset:U, mean:U, variance:U -> y:T, batch_mean:U, batch_variance:U, reserve_space_1:U, reserve_space_2:U, reserve_space_3:U; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=U:type,allowed=[DT_BFLOAT16, DT_FLOAT]; attr=epsilon:float,default=0.0001; attr=exponential_avg_factor:float,default=1; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NDHWC", "NCDHW"]; attr=is_training:bool,default=true>
Op<name=FusedPadConv2D; signature=input:T, paddings:int32, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=mode:string,allowed=["REFLECT", "SYMMETRIC"]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=FusedResizeAndPadConv2D; signature=input:T, size:int32, paddings:int32, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=resize_align_corners:bool,default=false; attr=mode:string,allowed=["REFLECT", "SYMMETRIC"]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=GRUBlockCell; signature=x:T, h_prev:T, w_ru:T, w_c:T, b_ru:T, b_c:T -> r:T, u:T, c:T, h:T; attr=T:type,allowed=[DT_FLOAT]>
Op<name=GRUBlockCellGrad; signature=x:T, h_prev:T, w_ru:T, w_c:T, b_ru:T, b_c:T, r:T, u:T, c:T, d_h:T -> d_x:T, d_h_prev:T, d_c_bar:T, d_r_bar_u_bar:T; attr=T:type,allowed=[DT_FLOAT]>
Op<name=Gather; signature=params:Tparams, indices:Tindices -> output:Tparams; attr=validate_indices:bool,default=true; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=GatherNd; signature=params:Tparams, indices:Tindices -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64]>
Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=batch_dims:int,default=0; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>
Op<name=GenerateBoundingBoxProposals; signature=scores:float, bbox_deltas:float, image_info:float, anchors:float, nms_threshold:float, pre_nms_topn:int32, min_size:float -> rois:float, roi_probabilities:float; attr=post_nms_topn:int,default=300>
Op<name=GenerateVocabRemapping; signature=new_vocab_file:string, old_vocab_file:string -> remapping:int64, num_present:int32; attr=new_vocab_offset:int,min=0; attr=num_new_vocab:int,min=0; attr=old_vocab_size:int,default=-1,min=-1>
Op<name=GeneratorDataset; signature=init_func_other_args:, next_func_other_args:, finalize_func_other_args: -> handle:variant; attr=init_func:func; attr=next_func:func; attr=finalize_func:func; attr=Tinit_func_args:list(type),min=0; attr=Tnext_func_args:list(type),min=0; attr=Tfinalize_func_args:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=GetElementAtIndex; signature=dataset:variant, index:int64 -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=GetMinibatchSplitsWithPhysicalReplica; signature=program_key:string, row_ids:int32, col_ids:int32, gains:float -> sorted_row_ids:int32, sorted_col_ids:int32, sorted_gains:float, splits:int64, id_counts:int32, max_ids:int32, max_uniques:int32; attr=sample_count:int,min=1; attr=num_replica:int,min=1; attr=table_vocab_size:int,min=1; attr=feature_width:int,min=1; attr=num_sc_per_chip:int,min=1; attr=table_name:string; attr=mini_batch_splits:string; is_stateful=true>
Op<name=GetMinibatchesInCsrWithPhysicalReplica; signature=program_key:string, row_ids:int32, col_ids:int32, gains:float, splits:int64, id_counts:int32 -> row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, row_pointers_unpadded_size:int32, ids_unpadded_size:int32, num_minibatches_per_physical_sparse_core:int32; attr=sample_count:int,min=1; attr=num_replica:int,min=1; attr=max_minibatches_per_sc:int,min=1; attr=max_ids_per_chip_per_sample:int,min=1; attr=table_vocab_size:int,min=1; attr=feature_width:int,min=1; attr=num_sc_per_chip:int,min=1; attr=table_name:string; attr=mini_batch_in_csr:string; is_stateful=true>
Op<name=GetOptions; signature=input_dataset:variant -> serialized_options:string>
Op<name=GetSessionHandle; signature=value:T -> handle:string; attr=T:type; is_stateful=true>
Op<name=GetSessionHandleV2; signature=value:T -> handle:resource; attr=T:type; is_stateful=true>
Op<name=GetSessionTensor; signature=handle:string -> value:dtype; attr=dtype:type; is_stateful=true>
Op<name=GlobalIterId; signature= -> iter_id:int64; is_stateful=true>
Op<name=Greater; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=GreaterEqual; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=GroupByReducerDataset; signature=input_dataset:variant, key_func_other_arguments:, init_func_other_arguments:, reduce_func_other_arguments:, finalize_func_other_arguments: -> handle:variant; attr=key_func:func; attr=init_func:func; attr=reduce_func:func; attr=finalize_func:func; attr=Tkey_func_other_arguments:list(type),min=0; attr=Tinit_func_other_arguments:list(type),min=0; attr=Treduce_func_other_arguments:list(type),min=0; attr=Tfinalize_func_other_arguments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=GroupByWindowDataset; signature=input_dataset:variant, key_func_other_arguments:, reduce_func_other_arguments:, window_size_func_other_arguments: -> handle:variant; attr=key_func:func; attr=reduce_func:func; attr=window_size_func:func; attr=Tkey_func_other_arguments:list(type),min=0; attr=Treduce_func_other_arguments:list(type),min=0; attr=Twindow_size_func_other_arguments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=GuaranteeConst; signature=input:T -> output:T; attr=T:type; is_stateful=true>
Op<name=HSVToRGB; signature=images:T -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=HashTable; signature= -> table_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=HashTableV2; signature= -> table_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=HistogramFixedWidth; signature=values:T, value_range:T, nbins:int32 -> out:dtype; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=dtype:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=HistogramSummary; signature=tag:string, values:T -> summary:string; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=HostConst; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>
Op<name=IFFT; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IFFT2D; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IFFT3D; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IFFTND; signature=input:Tcomplex, fft_length:int32, axes:int32 -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IRFFT; signature=input:Tcomplex, fft_length:int32 -> output:Treal; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IRFFT2D; signature=input:Tcomplex, fft_length:int32 -> output:Treal; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IRFFT3D; signature=input:Tcomplex, fft_length:int32 -> output:Treal; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=IRFFTND; signature=input:Tcomplex, fft_length:int32, axes:int32 -> output:Treal; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Identity; signature=input:T -> output:T; attr=T:type>
Op<name=IdentityN; signature=input: -> output:; attr=T:list(type),min=1>
Op<name=IdentityReader; signature= -> reader_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=IdentityReaderV2; signature= -> reader_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=If; signature=cond:Tcond, input: -> output:; attr=Tcond:type; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=then_branch:func; attr=else_branch:func; attr=output_shapes:list(shape),default=[]; is_stateful=true>
Op<name=Igamma; signature=a:T, x:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=IgammaGradA; signature=a:T, x:T -> z:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=Igammac; signature=a:T, x:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=IgnoreErrorsDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=log_warning:bool,default=false>
Op<name=Imag; signature=input:T -> output:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=ImageProjectiveTransformV2; signature=images:dtype, transforms:float, output_shape:int32 -> transformed_images:dtype; attr=dtype:type,allowed=[DT_UINT8, DT_INT32, DT_INT64, DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=interpolation:string; attr=fill_mode:string,default="CONSTANT">
Op<name=ImageProjectiveTransformV3; signature=images:dtype, transforms:float, output_shape:int32, fill_value:float -> transformed_images:dtype; attr=dtype:type,allowed=[DT_UINT8, DT_INT32, DT_INT64, DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=interpolation:string; attr=fill_mode:string,default="CONSTANT">
Op<name=ImageSummary; signature=tag:string, tensor:T -> summary:string; attr=max_images:int,default=3,min=1; attr=T:type,default=DT_FLOAT,allowed=[DT_UINT8, DT_FLOAT, DT_HALF, DT_DOUBLE]; attr=bad_color:tensor,default=Tensor<type: uint8 shape: [4] values: 255 0 0...>>
Op<name=ImmutableConst; signature= -> tensor:dtype; attr=dtype:type; attr=shape:shape; attr=memory_region_name:string>
Op<name=ImportEvent; signature=writer:resource, event:string -> ; is_stateful=true>
Op<name=InTopK; signature=predictions:float, targets:T -> precision:bool; attr=k:int; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=InTopKV2; signature=predictions:float, targets:T, k:T -> precision:bool; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=InfeedDequeue; signature= -> output:dtype; attr=dtype:type; attr=shape:shape; is_stateful=true>
Op<name=InfeedDequeueTuple; signature= -> outputs:; attr=dtypes:list(type),min=1; attr=shapes:list(shape); is_stateful=true>
Op<name=InfeedEnqueue; signature=input:dtype -> ; attr=dtype:type; attr=shape:shape,default=[]; attr=layout:list(int),default=[]; attr=device_ordinal:int,default=-1; is_stateful=true>
Op<name=InfeedEnqueuePrelinearizedBuffer; signature=input:variant -> ; attr=device_ordinal:int,default=-1>
Op<name=InfeedEnqueueTuple; signature=inputs: -> ; attr=dtypes:list(type),min=1; attr=shapes:list(shape); attr=layouts:list(int),default=[]; attr=device_ordinal:int,default=-1; is_stateful=true>
Op<name=InitializeTable; signature=table_handle:Ref(string), keys:Tkey, values:Tval -> ; attr=Tkey:type; attr=Tval:type>
Op<name=InitializeTableFromDataset; signature=table_handle:resource, dataset:variant -> ; is_stateful=true>
Op<name=InitializeTableFromTextFile; signature=table_handle:Ref(string), filename:string -> ; attr=key_index:int,min=-2; attr=value_index:int,min=-2; attr=vocab_size:int,default=-1,min=-1; attr=delimiter:string,default="\t"; attr=offset:int,default=0>
Op<name=InitializeTableFromTextFileV2; signature=table_handle:resource, filename:string -> ; attr=key_index:int,min=-2; attr=value_index:int,min=-2; attr=vocab_size:int,default=-1,min=-1; attr=delimiter:string,default="\t"; attr=offset:int,default=0; is_stateful=true>
Op<name=InitializeTableV2; signature=table_handle:resource, keys:Tkey, values:Tval -> ; attr=Tkey:type; attr=Tval:type; is_stateful=true>
Op<name=InplaceAdd; signature=x:T, i:int32, v:T -> y:T; attr=T:type>
Op<name=InplaceSub; signature=x:T, i:int32, v:T -> y:T; attr=T:type>
Op<name=InplaceUpdate; signature=x:T, i:int32, v:T -> y:T; attr=T:type>
Op<name=InterleaveDataset; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=Inv; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=InvGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Invert; signature=x:T -> y:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]>
Op<name=InvertPermutation; signature=x:T -> y:T; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=IsBoostedTreesEnsembleInitialized; signature=tree_ensemble_handle:resource -> is_initialized:bool; is_stateful=true>
Op<name=IsBoostedTreesQuantileStreamResourceInitialized; signature=quantile_stream_resource_handle:resource -> is_initialized:bool; is_stateful=true>
Op<name=IsFinite; signature=x:T -> y:bool; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=IsInf; signature=x:T -> y:bool; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=IsNan; signature=x:T -> y:bool; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=IsTPUEmbeddingInitialized; signature= -> is_tpu_embedding_initialized:bool; attr=config:string,default=""; is_stateful=true>
Op<name=IsVariableInitialized; signature=ref:Ref(dtype) -> is_initialized:bool; attr=dtype:type; allows_uninitialized_input=true>
Op<name=IsotonicRegression; signature=input:T -> output:output_dtype, segments:int32; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=output_dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=Iterator; signature= -> handle:resource; attr=shared_name:string; attr=container:string; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=IteratorFromStringHandle; signature=string_handle:string -> resource_handle:resource; attr=output_types:list(type),default=[],min=0; attr=output_shapes:list(shape),default=[],min=0; is_stateful=true>
Op<name=IteratorFromStringHandleV2; signature=string_handle:string -> resource_handle:resource; attr=output_types:list(type),default=[],min=0; attr=output_shapes:list(shape),default=[],min=0; is_stateful=true>
Op<name=IteratorGetDevice; signature=resource:resource -> device:string; is_stateful=true>
Op<name=IteratorGetNext; signature=iterator:resource -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=IteratorGetNextAsOptional; signature=iterator:resource -> optional:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=IteratorGetNextSync; signature=iterator:resource -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=IteratorToStringHandle; signature=resource_handle:resource -> string_handle:string; is_stateful=true>
Op<name=IteratorV2; signature= -> handle:resource; attr=shared_name:string; attr=container:string; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=KMC2ChainInitialization; signature=distances:float, seed:int64 -> index:int64>
Op<name=KmeansPlusPlusInitialization; signature=points:float, num_to_sample:int64, seed:int64, num_retries_per_sample:int64 -> samples:float>
Op<name=KthOrderStatistic; signature=input:float -> output:float; attr=k:int>
Op<name=L2Loss; signature=t:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=LMDBDataset; signature=filenames:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=LMDBReader; signature= -> reader_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=LRN; signature=input:T -> output:T; attr=depth_radius:int,default=5; attr=bias:float,default=1; attr=alpha:float,default=1; attr=beta:float,default=0.5; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]>
Op<name=LRNGrad; signature=input_grads:T, input_image:T, output_image:T -> output:T; attr=depth_radius:int,default=5; attr=bias:float,default=1; attr=alpha:float,default=1; attr=beta:float,default=0.5; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]>
Op<name=LSTMBlockCell; signature=x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T -> i:T, cs:T, f:T, o:T, ci:T, co:T, h:T; attr=forget_bias:float,default=1; attr=cell_clip:float,default=3; attr=use_peephole:bool,default=false; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=LSTMBlockCellGrad; signature=x:T, cs_prev:T, h_prev:T, w:T, wci:T, wcf:T, wco:T, b:T, i:T, cs:T, f:T, o:T, ci:T, co:T, cs_grad:T, h_grad:T -> cs_prev_grad:T, dicfo:T, wci_grad:T, wcf_grad:T, wco_grad:T; attr=use_peephole:bool; attr=T:type,allowed=[DT_HALF, DT_FLOAT]>
Op<name=LatencyStatsDataset; signature=input_dataset:variant, tag:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=LeakyRelu; signature=features:T -> activations:T; attr=alpha:float,default=0.2; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=LeakyReluGrad; signature=gradients:T, features:T -> backprops:T; attr=alpha:float,default=0.2; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=LearnedUnigramCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=range_max:int,min=1; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=LeftShift; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]>
Op<name=LegacyParallelInterleaveDatasetV2; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, buffer_output_elements:int64, prefetch_input_elements:int64 -> handle:variant; attr=f:func; attr=deterministic:string,default="default"; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=Less; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=LessEqual; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=Lgamma; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=LinSpace; signature=start:T, stop:T, num:Tidx -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ListDataset; signature=tensors: -> handle:variant; attr=Tinput_types:list(type),min=1; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=ListDiff; signature=x:T, y:T -> out:T, idx:out_idx; attr=T:type; attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ListSnapshotChunksDataset; signature=snapshot_path:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=LoadAllTPUEmbeddingParameters; signature=parameters:NumTables*float, auxiliary1:NumTables*float, auxiliary2:NumTables*float, auxiliary3:NumTables*float, auxiliary4:NumTables*float, auxiliary5:NumTables*float, auxiliary6:NumTables*float, auxiliary7:NumTables*float -> ; attr=NumTables:int,min=1; attr=config:string; attr=num_shards:int; attr=shard_id:int; is_stateful=true>
Op<name=LoadAndRemapMatrix; signature=ckpt_path:string, old_tensor_name:string, row_remapping:int64, col_remapping:int64, initializing_values:float -> output_matrix:float; attr=num_rows:int,min=0; attr=num_cols:int,min=1; attr=max_rows_in_memory:int,default=-1; is_stateful=true>
Op<name=LoadDataset; signature=path:string, reader_func_other_args: -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=compression:string,default=""; attr=reader_func:func; attr=Treader_func_args:list(type),min=0; is_stateful=true>
Op<name=LoadTPUEmbeddingADAMParameters; signature=parameters:float, momenta:float, velocities:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingAdadeltaParameters; signature=parameters:float, accumulators:float, updates:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingAdagradMomentumParameters; signature=parameters:float, accumulators:float, momenta:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingAdagradParameters; signature=parameters:float, accumulators:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingCenteredRMSPropParameters; signature=parameters:float, ms:float, mom:float, mg:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingFTRLParameters; signature=parameters:float, accumulators:float, linears:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingFrequencyEstimatorParameters; signature=parameters:float, last_hit_step:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingMDLAdagradLightParameters; signature=parameters:float, accumulators:float, weights:float, benefits:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingMomentumParameters; signature=parameters:float, momenta:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingProximalAdagradParameters; signature=parameters:float, accumulators:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingProximalYogiParameters; signature=parameters:float, v:float, m:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingRMSPropParameters; signature=parameters:float, ms:float, mom:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=LoadTPUEmbeddingStochasticGradientDescentParameters; signature=parameters:float -> ; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=Log; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Log1p; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=LogMatrixDeterminant; signature=input:T -> sign:T, log_abs_determinant:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=LogSoftmax; signature=logits:T -> logsoftmax:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=LogUniformCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=range_max:int,min=1; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=LogicalAnd; signature=x:bool, y:bool -> z:bool; is_commutative=true>
Op<name=LogicalNot; signature=x:bool -> y:bool>
Op<name=LogicalOr; signature=x:bool, y:bool -> z:bool; is_commutative=true>
Op<name=LookupTableExport; signature=table_handle:Ref(string) -> keys:Tkeys, values:Tvalues; attr=Tkeys:type; attr=Tvalues:type>
Op<name=LookupTableExportV2; signature=table_handle:resource -> keys:Tkeys, values:Tvalues; attr=Tkeys:type; attr=Tvalues:type; is_stateful=true>
Op<name=LookupTableFind; signature=table_handle:Ref(string), keys:Tin, default_value:Tout -> values:Tout; attr=Tin:type; attr=Tout:type>
Op<name=LookupTableFindV2; signature=table_handle:resource, keys:Tin, default_value:Tout -> values:Tout; attr=Tin:type; attr=Tout:type; is_stateful=true>
Op<name=LookupTableImport; signature=table_handle:Ref(string), keys:Tin, values:Tout -> ; attr=Tin:type; attr=Tout:type>
Op<name=LookupTableImportV2; signature=table_handle:resource, keys:Tin, values:Tout -> ; attr=Tin:type; attr=Tout:type; is_stateful=true>
Op<name=LookupTableInsert; signature=table_handle:Ref(string), keys:Tin, values:Tout -> ; attr=Tin:type; attr=Tout:type>
Op<name=LookupTableInsertV2; signature=table_handle:resource, keys:Tin, values:Tout -> ; attr=Tin:type; attr=Tout:type; is_stateful=true>
Op<name=LookupTableRemoveV2; signature=table_handle:resource, keys:Tin -> ; attr=Tin:type; is_stateful=true>
Op<name=LookupTableSize; signature=table_handle:Ref(string) -> size:int64>
Op<name=LookupTableSizeV2; signature=table_handle:resource -> size:int64; is_stateful=true>
Op<name=LoopCond; signature=input:bool -> output:bool>
Op<name=LowerBound; signature=sorted_inputs:T, values:T -> output:out_type; attr=T:type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Lu; signature=input:T -> lu:T, p:output_idx_type; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]; attr=output_idx_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=MakeIterator; signature=dataset:variant, iterator:resource -> ; is_stateful=true>
Op<name=MakeUnique; signature=input:float -> output:float>
Op<name=MapAndBatchDataset; signature=input_dataset:variant, other_arguments:, batch_size:int64, num_parallel_calls:int64, drop_remainder:bool -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default="">
Op<name=MapClear; signature= -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default="">
Op<name=MapDefun; signature=arguments:, captured_inputs: -> output:; attr=Targuments:list(type),min=1; attr=Tcaptured:list(type),default=[],min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=f:func; attr=max_intra_op_parallelism:int,default=1>
Op<name=MapIncompleteSize; signature= -> size:int32; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapPeek; signature=key:int64, indices:int32 -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapSize; signature= -> size:int32; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapStage; signature=key:int64, indices:int32, values: -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=fake_dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapUnstage; signature=key:int64, indices:int32 -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MapUnstageNoKey; signature=indices:int32 -> key:int64, values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=MatMul; signature=a:T, b:T -> product:T; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128]; attr=grad_a:bool,default=false; attr=grad_b:bool,default=false>
Op<name=MatchingFiles; signature=pattern:string -> filenames:string>
Op<name=MatchingFilesDataset; signature=patterns:string -> handle:variant; is_stateful=true>
Op<name=MatrixBandPart; signature=input:T, num_lower:Tindex, num_upper:Tindex -> band:T; attr=T:type; attr=Tindex:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=MatrixDeterminant; signature=input:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixDiag; signature=diagonal:T -> output:T; attr=T:type>
Op<name=MatrixDiagPart; signature=input:T -> diagonal:T; attr=T:type>
Op<name=MatrixDiagPartV2; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type>
Op<name=MatrixDiagPartV3; signature=input:T, k:int32, padding_value:T -> diagonal:T; attr=T:type; attr=align:string,default="RIGHT_LEFT",allowed=["LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"]>
Op<name=MatrixDiagV2; signature=diagonal:T, k:int32, num_rows:int32, num_cols:int32, padding_value:T -> output:T; attr=T:type>
Op<name=MatrixDiagV3; signature=diagonal:T, k:int32, num_rows:int32, num_cols:int32, padding_value:T -> output:T; attr=T:type; attr=align:string,default="RIGHT_LEFT",allowed=["LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"]>
Op<name=MatrixExponential; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixInverse; signature=input:T -> output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixLogarithm; signature=input:T -> output:T; attr=T:type,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixSetDiag; signature=input:T, diagonal:T -> output:T; attr=T:type>
Op<name=MatrixSetDiagV2; signature=input:T, diagonal:T, k:int32 -> output:T; attr=T:type>
Op<name=MatrixSetDiagV3; signature=input:T, diagonal:T, k:int32 -> output:T; attr=T:type; attr=align:string,default="RIGHT_LEFT",allowed=["LEFT_RIGHT", "RIGHT_LEFT", "LEFT_LEFT", "RIGHT_RIGHT"]>
Op<name=MatrixSolve; signature=matrix:T, rhs:T -> output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixSolveLs; signature=matrix:T, rhs:T, l2_regularizer:double -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]; attr=fast:bool,default=true>
Op<name=MatrixSquareRoot; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MatrixTriangularSolve; signature=matrix:T, rhs:T -> output:T; attr=lower:bool,default=true; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Max; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=MaxIntraOpParallelismDataset; signature=input_dataset:variant, max_intra_op_parallelism:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=MaxPool; signature=input:T -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16, DT_QINT8]; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NCHW_VECT_C"]>
Op<name=MaxPool3D; signature=input:T -> output:T; attr=ksize:list(int),min=5; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]>
Op<name=MaxPool3DGrad; signature=orig_input:TInput, orig_output:TInput, grad:T -> output:T; attr=ksize:list(int),min=5; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=TInput:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]>
Op<name=MaxPool3DGradGrad; signature=orig_input:T, orig_output:T, grad:T -> output:T; attr=ksize:list(int),min=5; attr=strides:list(int),min=5; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NDHWC",allowed=["NDHWC", "NCDHW"]; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGrad; signature=orig_input:T, orig_output:T, grad:T -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID", "EXPLICIT"]; attr=explicit_paddings:list(int),default=[]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGradGrad; signature=orig_input:T, orig_output:T, grad:T -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGradGradV2; signature=orig_input:T, orig_output:T, grad:T, ksize:int32, strides:int32 -> output:T; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGradGradWithArgmax; signature=input:T, grad:T, argmax:Targmax -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]; attr=include_batch_in_index:bool,default=false; attr=Targmax:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGradV2; signature=orig_input:T, orig_output:T, grad:T, ksize:int32, strides:int32 -> output:T; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolGradWithArgmax; signature=input:T, grad:T, argmax:Targmax -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=padding:string,allowed=["SAME", "VALID"]; attr=include_batch_in_index:bool,default=false; attr=Targmax:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=MaxPoolV2; signature=input:T, ksize:int32, strides:int32 -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16, DT_QINT8]; attr=padding:string,allowed=["SAME", "VALID"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NCHW_VECT_C"]>
Op<name=MaxPoolWithArgmax; signature=input:T -> output:T, argmax:Targmax; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=Targmax:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=padding:string,allowed=["SAME", "VALID"]; attr=include_batch_in_index:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=Maximum; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64]>
Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Merge; signature=inputs:N*T -> output:T, value_index:int32; attr=T:type; attr=N:int,min=1>
Op<name=MergeDedupData; signature=integer_tensor:integer_type, float_tensor:float_type -> output:variant; attr=tuple_mask:string; attr=integer_type:type,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]; attr=float_type:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=config:string,default="">
Op<name=MergeSummary; signature=inputs:N*string -> summary:string; attr=N:int,min=1>
Op<name=MergeV2Checkpoints; signature=checkpoint_prefixes:string, destination_prefix:string -> ; attr=delete_old_dirs:bool,default=true; attr=allow_missing_files:bool,default=false; is_stateful=true>
Op<name=Mfcc; signature=spectrogram:float, sample_rate:int32 -> output:float; attr=upper_frequency_limit:float,default=4000; attr=lower_frequency_limit:float,default=20; attr=filterbank_channel_count:int,default=40; attr=dct_coefficient_count:int,default=13>
Op<name=Min; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Minimum; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64]>
Op<name=MirrorPad; signature=input:T, paddings:Tpaddings -> output:T; attr=T:type; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=mode:string,allowed=["REFLECT", "SYMMETRIC"]>
Op<name=MirrorPadGrad; signature=input:T, paddings:Tpaddings -> output:T; attr=T:type; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=mode:string,allowed=["REFLECT", "SYMMETRIC"]>
Op<name=MlirPassthroughOp; signature=inputs: -> outputs:; attr=mlir_module:string; attr=Tinputs:list(type),min=0; attr=Toutputs:list(type),min=0>
Op<name=Mod; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_HALF, DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=ModelDataset; signature=input_dataset:variant -> handle:variant; attr=algorithm:int,default=0; attr=cpu_budget:int,default=0; attr=ram_budget:int,default=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true>
Op<name=MulNoNan; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=MultiDeviceIterator; signature= -> handle:resource; attr=devices:list(string),min=1; attr=shared_name:string; attr=container:string; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=MultiDeviceIteratorFromStringHandle; signature=string_handle:string -> multi_device_iterator:resource; attr=output_types:list(type),default=[],min=0; attr=output_shapes:list(shape),default=[],min=0; is_stateful=true>
Op<name=MultiDeviceIteratorGetNextFromShard; signature=multi_device_iterator:resource, shard_num:int32, incarnation_id:int64 -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=MultiDeviceIteratorInit; signature=dataset:variant, multi_device_iterator:resource, max_buffer_size:int64 -> incarnation_id:int64; is_stateful=true>
Op<name=MultiDeviceIteratorToStringHandle; signature=multi_device_iterator:resource -> string_handle:string; is_stateful=true>
Op<name=Multinomial; signature=logits:T, num_samples:int32 -> output:output_dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=output_dtype:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=MutableDenseHashTable; signature=empty_key:key_dtype -> table_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; attr=initial_num_buckets:int,default=131072; attr=max_load_factor:float,default=0.8; is_stateful=true>
Op<name=MutableDenseHashTableV2; signature=empty_key:key_dtype, deleted_key:key_dtype -> table_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; attr=initial_num_buckets:int,default=131072; attr=max_load_factor:float,default=0.8; is_stateful=true>
Op<name=MutableHashTable; signature= -> table_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=MutableHashTableOfTensors; signature= -> table_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; is_stateful=true>
Op<name=MutableHashTableOfTensorsV2; signature= -> table_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; attr=value_shape:shape,default=[]; is_stateful=true>
Op<name=MutableHashTableV2; signature= -> table_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=use_node_name_sharing:bool,default=false; attr=key_dtype:type; attr=value_dtype:type; is_stateful=true>
Op<name=MutexLock; signature=mutex:resource -> mutex_lock:variant; is_stateful=true>
Op<name=MutexV2; signature= -> resource:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=NcclAllReduce; signature=input:T -> data:T; attr=reduction:string,allowed=["min", "max", "prod", "sum"]; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=num_devices:int; attr=shared_name:string; is_stateful=true>
Op<name=NcclBroadcast; signature=input:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=shape:shape; is_stateful=true>
Op<name=NcclReduce; signature=input:num_devices*T -> data:T; attr=reduction:string,allowed=["min", "max", "prod", "sum"]; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=num_devices:int,min=1; is_stateful=true>
Op<name=Ndtri; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=NearestNeighbors; signature=points:float, centers:float, k:int64 -> nearest_center_indices:int64, nearest_center_distances:float>
Op<name=Neg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=NegTrain; signature=w_in:Ref(float), w_out:Ref(float), examples:int32, labels:int32, lr:float -> ; attr=vocab_count:list(int); attr=num_negative_samples:int; is_stateful=true>
Op<name=NextAfter; signature=x1:T, x2:T -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_DOUBLE, DT_FLOAT]>
Op<name=NextIteration; signature=data:T -> output:T; attr=T:type>
Op<name=NoOp; signature= -> >
Op<name=NonDeterministicInts; signature=shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_INT64; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=NonMaxSuppression; signature=boxes:float, scores:float, max_output_size:int32 -> selected_indices:int32; attr=iou_threshold:float,default=0.5>
Op<name=NonMaxSuppressionV2; signature=boxes:T, scores:T, max_output_size:int32, iou_threshold:T_threshold -> selected_indices:int32; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]; attr=T_threshold:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]>
Op<name=NonMaxSuppressionV3; signature=boxes:T, scores:T, max_output_size:int32, iou_threshold:T_threshold, score_threshold:T_threshold -> selected_indices:int32; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]; attr=T_threshold:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]>
Op<name=NonMaxSuppressionV4; signature=boxes:T, scores:T, max_output_size:int32, iou_threshold:T_threshold, score_threshold:T_threshold -> selected_indices:int32, valid_outputs:int32; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]; attr=T_threshold:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]; attr=pad_to_max_output_size:bool,default=false>
Op<name=NonMaxSuppressionV5; signature=boxes:T, scores:T, max_output_size:int32, iou_threshold:T, score_threshold:T, soft_nms_sigma:T -> selected_indices:int32, selected_scores:T, valid_outputs:int32; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_FLOAT]; attr=pad_to_max_output_size:bool,default=false>
Op<name=NonMaxSuppressionWithOverlaps; signature=overlaps:float, scores:float, max_output_size:int32, overlap_threshold:float, score_threshold:float -> selected_indices:int32>
Op<name=NonSerializableDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=NotEqual; signature=x:T, y:T -> z:bool; attr=T:type; attr=incompatible_shape_error:bool,default=true; is_commutative=true>
Op<name=NthElement; signature=input:T, n:int32 -> values:T; attr=reverse:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=OneHot; signature=indices:TI, depth:int32, on_value:T, off_value:T -> output:T; attr=axis:int,default=-1; attr=T:type; attr=TI:type,default=DT_INT64,allowed=[DT_UINT8, DT_INT8, DT_INT32, DT_INT64]>
Op<name=OneShotIterator; signature= -> handle:resource; attr=dataset_factory:func; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OnesLike; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL]>
Op<name=OptimizeDataset; signature=input_dataset:variant, optimizations:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=optimization_configs:list(string),default=[]>
Op<name=OptimizeDatasetV2; signature=input_dataset:variant, optimizations_enabled:string, optimizations_disabled:string, optimizations_default:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=optimization_configs:list(string),default=[]>
Op<name=OptionalFromValue; signature=components: -> optional:variant; attr=Toutput_types:list(type),min=1>
Op<name=OptionalGetValue; signature=optional:variant -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=OptionalHasValue; signature=optional:variant -> has_value:bool>
Op<name=OptionalNone; signature= -> optional:variant>
Op<name=OptionsDataset; signature=input_dataset:variant -> handle:variant; attr=serialized_options:string; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=OrderedMapClear; signature= -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapIncompleteSize; signature= -> size:int32; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapPeek; signature=key:int64, indices:int32 -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapSize; signature= -> size:int32; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapStage; signature=key:int64, indices:int32, values: -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=fake_dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapUnstage; signature=key:int64, indices:int32 -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OrderedMapUnstageNoKey; signature=indices:int32 -> key:int64, values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=OutfeedDequeue; signature= -> output:dtype; attr=dtype:type; attr=shape:shape; attr=device_ordinal:int,default=-1; is_stateful=true>
Op<name=OutfeedDequeueTuple; signature= -> outputs:; attr=dtypes:list(type),min=1; attr=shapes:list(shape); attr=device_ordinal:int,default=-1; is_stateful=true>
Op<name=OutfeedDequeueTupleV2; signature=device_ordinal:int32 -> outputs:; attr=dtypes:list(type),min=1; attr=shapes:list(shape); is_stateful=true>
Op<name=OutfeedDequeueV2; signature=device_ordinal:int32 -> output:dtype; attr=dtype:type; attr=shape:shape; is_stateful=true>
Op<name=OutfeedEnqueue; signature=input:dtype -> ; attr=dtype:type; is_stateful=true>
Op<name=OutfeedEnqueueTuple; signature=inputs: -> ; attr=dtypes:list(type),min=1; is_stateful=true>
Op<name=Pack; signature=values:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=axis:int,default=0>
Op<name=Pad; signature=input:T, paddings:Tpaddings -> output:T; attr=T:type; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=PadV2; signature=input:T, paddings:Tpaddings, constant_values:T -> output:T; attr=T:type; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=PaddedBatchDataset; signature=input_dataset:variant, batch_size:int64, padded_shapes:N*int64, padding_values: -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1; attr=metadata:string,default="">
Op<name=PaddedBatchDatasetV2; signature=input_dataset:variant, batch_size:int64, padded_shapes:N*int64, padding_values:, drop_remainder:bool -> handle:variant; attr=parallel_copy:bool,default=false; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1; attr=metadata:string,default="">
Op<name=PaddingFIFOQueue; signature= -> handle:Ref(string); attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=PaddingFIFOQueueV2; signature= -> handle:resource; attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=ParallelBatchDataset; signature=input_dataset:variant, batch_size:int64, num_parallel_calls:int64, drop_remainder:bool -> handle:variant; attr=parallel_copy:bool,default=false; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=deterministic:string,default="default"; attr=metadata:string,default="">
Op<name=ParallelConcat; signature=values:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=shape:shape>
Op<name=ParallelDynamicStitch; signature=indices:N*int32, data:N*T -> merged:T; attr=N:int,min=1; attr=T:type>
Op<name=ParallelFilterDataset; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=predicate:func; attr=deterministic:string,default="default"; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ParallelInterleaveDataset; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, sloppy:bool, buffer_output_elements:int64, prefetch_input_elements:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ParallelInterleaveDatasetV2; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=sloppy:bool,default=false; attr=metadata:string,default="">
Op<name=ParallelInterleaveDatasetV3; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=deterministic:string,default="default"; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ParallelInterleaveDatasetV4; signature=input_dataset:variant, other_arguments:, cycle_length:int64, block_length:int64, buffer_output_elements:int64, prefetch_input_elements:int64, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=deterministic:string,default="default"; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ParallelMapDataset; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int32 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=sloppy:bool,default=false; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default="">
Op<name=ParallelMapDatasetV2; signature=input_dataset:variant, other_arguments:, num_parallel_calls:int64 -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=deterministic:string,default="default"; attr=preserve_cardinality:bool,default=false; attr=metadata:string,default="">
Op<name=ParameterizedTruncatedNormal; signature=shape:T, means:dtype, stdevs:dtype, minvals:dtype, maxvals:dtype -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=dtype:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ParseExample; signature=serialized:string, names:string, sparse_keys:Nsparse*string, dense_keys:Ndense*string, dense_defaults: -> sparse_indices:Nsparse*int64, sparse_values:, sparse_shapes:Nsparse*int64, dense_values:; attr=Nsparse:int,min=0; attr=Ndense:int,min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=dense_shapes:list(shape),min=0>
Op<name=ParseExampleDataset; signature=input_dataset:variant, num_parallel_calls:int64, dense_defaults: -> handle:variant; attr=sparse_keys:list(string),min=0; attr=dense_keys:list(string),min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=dense_shapes:list(shape),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=sloppy:bool,default=false; attr=ragged_keys:list(string),default=[],min=0; attr=ragged_value_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=ragged_split_types:list(type),default=[],min=0,allowed=[DT_INT32, DT_INT64]>
Op<name=ParseExampleDatasetV2; signature=input_dataset:variant, num_parallel_calls:int64, dense_defaults: -> handle:variant; attr=sparse_keys:list(string),min=0; attr=dense_keys:list(string),min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=dense_shapes:list(shape),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=deterministic:string,default="default"; attr=ragged_keys:list(string),default=[],min=0; attr=ragged_value_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=ragged_split_types:list(type),default=[],min=0,allowed=[DT_INT32, DT_INT64]>
Op<name=ParseExampleV2; signature=serialized:string, names:string, sparse_keys:string, dense_keys:string, ragged_keys:string, dense_defaults: -> sparse_indices:num_sparse*int64, sparse_values:, sparse_shapes:num_sparse*int64, dense_values:, ragged_values:, ragged_row_splits:; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=num_sparse:int,min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=ragged_value_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=ragged_split_types:list(type),min=0,allowed=[DT_INT32, DT_INT64]; attr=dense_shapes:list(shape),min=0>
Op<name=ParseSequenceExample; signature=serialized:string, debug_name:string, context_dense_defaults: -> context_sparse_indices:Ncontext_sparse*int64, context_sparse_values:, context_sparse_shapes:Ncontext_sparse*int64, context_dense_values:, feature_list_sparse_indices:Nfeature_list_sparse*int64, feature_list_sparse_values:, feature_list_sparse_shapes:Nfeature_list_sparse*int64, feature_list_dense_values:, feature_list_dense_lengths:Nfeature_list_dense*int64; attr=feature_list_dense_missing_assumed_empty:list(string),min=0; attr=context_sparse_keys:list(string),min=0; attr=context_dense_keys:list(string),min=0; attr=feature_list_sparse_keys:list(string),min=0; attr=feature_list_dense_keys:list(string),min=0; attr=Ncontext_sparse:int,default=0,min=0; attr=Ncontext_dense:int,default=0,min=0; attr=Nfeature_list_sparse:int,default=0,min=0; attr=Nfeature_list_dense:int,default=0,min=0; attr=context_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tcontext_dense:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_dense_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=context_dense_shapes:list(shape),default=[],min=0; attr=feature_list_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_dense_shapes:list(shape),default=[],min=0>
Op<name=ParseSequenceExampleV2; signature=serialized:string, debug_name:string, context_sparse_keys:string, context_dense_keys:string, context_ragged_keys:string, feature_list_sparse_keys:string, feature_list_dense_keys:string, feature_list_ragged_keys:string, feature_list_dense_missing_assumed_empty:bool, context_dense_defaults: -> context_sparse_indices:Ncontext_sparse*int64, context_sparse_values:, context_sparse_shapes:Ncontext_sparse*int64, context_dense_values:, context_ragged_values:, context_ragged_row_splits:, feature_list_sparse_indices:Nfeature_list_sparse*int64, feature_list_sparse_values:, feature_list_sparse_shapes:Nfeature_list_sparse*int64, feature_list_dense_values:, feature_list_dense_lengths:Nfeature_list_dense*int64, feature_list_ragged_values:, feature_list_ragged_outer_splits:, feature_list_ragged_inner_splits:; attr=Ncontext_sparse:int,default=0,min=0; attr=Tcontext_dense:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=context_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=context_ragged_value_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=context_ragged_split_types:list(type),default=[],min=0,allowed=[DT_INT32, DT_INT64]; attr=context_dense_shapes:list(shape),default=[],min=0; attr=Nfeature_list_sparse:int,default=0,min=0; attr=Nfeature_list_dense:int,default=0,min=0; attr=feature_list_dense_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_ragged_value_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_ragged_split_types:list(type),default=[],min=0,allowed=[DT_INT32, DT_INT64]; attr=feature_list_dense_shapes:list(shape),default=[],min=0>
Op<name=ParseSingleExample; signature=serialized:string, dense_defaults: -> sparse_indices:num_sparse*int64, sparse_values:, sparse_shapes:num_sparse*int64, dense_values:; attr=num_sparse:int,min=0; attr=sparse_keys:list(string),min=0; attr=dense_keys:list(string),min=0; attr=sparse_types:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tdense:list(type),min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=dense_shapes:list(shape),min=0>
Op<name=ParseSingleSequenceExample; signature=serialized:string, feature_list_dense_missing_assumed_empty:string, context_sparse_keys:Ncontext_sparse*string, context_dense_keys:Ncontext_dense*string, feature_list_sparse_keys:Nfeature_list_sparse*string, feature_list_dense_keys:Nfeature_list_dense*string, context_dense_defaults:, debug_name:string -> context_sparse_indices:Ncontext_sparse*int64, context_sparse_values:, context_sparse_shapes:Ncontext_sparse*int64, context_dense_values:, feature_list_sparse_indices:Nfeature_list_sparse*int64, feature_list_sparse_values:, feature_list_sparse_shapes:Nfeature_list_sparse*int64, feature_list_dense_values:; attr=Ncontext_sparse:int,default=0,min=0; attr=Ncontext_dense:int,default=0,min=0; attr=Nfeature_list_sparse:int,default=0,min=0; attr=Nfeature_list_dense:int,default=0,min=0; attr=context_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=Tcontext_dense:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_dense_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=context_dense_shapes:list(shape),default=[],min=0; attr=feature_list_sparse_types:list(type),default=[],min=0,allowed=[DT_FLOAT, DT_INT64, DT_STRING]; attr=feature_list_dense_shapes:list(shape),default=[],min=0>
Op<name=ParseTensor; signature=serialized:string -> output:out_type; attr=out_type:type>
Op<name=PartitionedCall; signature=args: -> output:; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=f:func; attr=config:string,default=""; attr=config_proto:string,default=""; attr=executor_type:string,default="">
Op<name=Placeholder; signature= -> output:dtype; attr=dtype:type; attr=shape:shape,default=<unknown>>
Op<name=PlaceholderV2; signature= -> output:dtype; attr=dtype:type; attr=shape:shape>
Op<name=PlaceholderWithDefault; signature=input:dtype -> output:dtype; attr=dtype:type; attr=shape:shape>
Op<name=Polygamma; signature=a:T, x:T -> z:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=PopulationCount; signature=x:T -> y:uint8; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]>
Op<name=Pow; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_FLOAT, DT_HALF, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=PrefetchDataset; signature=input_dataset:variant, buffer_size:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=slack_period:int,default=0; attr=legacy_autotune:bool,default=true; attr=buffer_size_min:int,default=0; attr=metadata:string,default="">
Op<name=Prelinearize; signature=input:dtype -> output:variant; attr=dtype:type; attr=shape:shape,default=[]; attr=layout:list(int),default=[]>
Op<name=PrelinearizeTuple; signature=inputs: -> output:variant; attr=dtypes:list(type),min=1; attr=shapes:list(shape); attr=layouts:list(int),default=[]>
Op<name=PreventGradient; signature=input:T -> output:T; attr=T:type; attr=message:string,default="">
Op<name=Print; signature=input:T, data: -> output:T; attr=T:type; attr=U:list(type),min=0; attr=message:string,default=""; attr=first_n:int,default=-1; attr=summarize:int,default=3; is_stateful=true>
Op<name=PrintV2; signature=input:string -> ; attr=output_stream:string,default="stderr"; attr=end:string,default="\n"; is_stateful=true>
Op<name=PriorityQueue; signature= -> handle:Ref(string); attr=component_types:list(type),default=[],min=0; attr=shapes:list(shape),min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=PriorityQueueV2; signature= -> handle:resource; attr=component_types:list(type),default=[],min=0; attr=shapes:list(shape),min=0; attr=capacity:int,default=-1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=PrivateThreadPoolDataset; signature=input_dataset:variant, num_threads:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Prod; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=PyFunc; signature=input: -> output:; attr=token:string; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; is_stateful=true>
Op<name=PyFuncStateless; signature=input: -> output:; attr=token:string; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0>
Op<name=Qr; signature=input:T -> q:T, r:T; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=QuantizeAndDequantize; signature=input:T -> output:T; attr=signed_input:bool,default=true; attr=num_bits:int,default=8; attr=range_given:bool,default=false; attr=input_min:float,default=0; attr=input_max:float,default=0; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=QuantizeAndDequantizeV2; signature=input:T, input_min:T, input_max:T -> output:T; attr=signed_input:bool,default=true; attr=num_bits:int,default=8; attr=range_given:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=round_mode:string,default="HALF_TO_EVEN",allowed=["HALF_TO_EVEN", "HALF_UP"]; attr=narrow_range:bool,default=false; attr=axis:int,default=-1>
Op<name=QuantizeAndDequantizeV3; signature=input:T, input_min:T, input_max:T, num_bits:int32 -> output:T; attr=signed_input:bool,default=true; attr=range_given:bool,default=true; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=narrow_range:bool,default=false; attr=axis:int,default=-1>
Op<name=QuantizeAndDequantizeV4; signature=input:T, input_min:T, input_max:T -> output:T; attr=signed_input:bool,default=true; attr=num_bits:int,default=8; attr=range_given:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=round_mode:string,default="HALF_TO_EVEN",allowed=["HALF_TO_EVEN", "HALF_UP"]; attr=narrow_range:bool,default=false; attr=axis:int,default=-1>
Op<name=QuantizeAndDequantizeV4Grad; signature=gradients:T, input:T, input_min:T, input_max:T -> input_backprop:T, input_min_backprop:T, input_max_backprop:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=axis:int,default=-1>
Op<name=QuantizeDownAndShrinkRange; signature=input:Tinput, input_min:float, input_max:float -> output:out_type, output_min:float, output_max:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizeV2; signature=input:float, min_range:float, max_range:float -> output:T, output_min:float, output_max:float; attr=T:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=mode:string,default="MIN_COMBINED",allowed=["MIN_COMBINED", "MIN_FIRST", "SCALED"]; attr=round_mode:string,default="HALF_AWAY_FROM_ZERO",allowed=["HALF_AWAY_FROM_ZERO", "HALF_TO_EVEN"]; attr=narrow_range:bool,default=false; attr=axis:int,default=-1; attr=ensure_minimum_range:float,default=0.01>
Op<name=QuantizedAdd; signature=x:T1, y:T2, min_x:float, max_x:float, min_y:float, max_y:float -> z:Toutput, min_z:float, max_z:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Toutput:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedAvgPool; signature=input:T, min_input:float, max_input:float -> output:T, min_output:float, max_output:float; attr=T:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=ksize:list(int); attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=QuantizedBatchNormWithGlobalNormalization; signature=t:Tinput, t_min:float, t_max:float, m:Tinput, m_min:float, m_max:float, v:Tinput, v_min:float, v_max:float, beta:Tinput, beta_min:float, beta_max:float, gamma:Tinput, gamma_min:float, gamma_max:float -> result:out_type, result_min:float, result_max:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=variance_epsilon:float; attr=scale_after_normalization:bool>
Op<name=QuantizedBiasAdd; signature=input:T1, bias:T2, min_input:float, max_input:float, min_bias:float, max_bias:float -> output:out_type, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedConcat; signature=concat_dim:int32, values:N*T, input_mins:N*float, input_maxes:N*float -> output:T, output_min:float, output_max:float; attr=N:int,min=2; attr=T:type>
Op<name=QuantizedConcatV2; signature=values:N*T, axis:Tidx, input_mins:N*float, input_maxes:N*float -> output:T, output_min:float, output_max:float; attr=N:int,min=2; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=QuantizedConv2D; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=QuantizedConv2DAndRelu; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DAndReluAndRequantize; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DAndRequantize; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DPerChannel; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=QuantizedConv2DWithBias; signature=input:Tinput, filter:Tfilter, bias:float, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasAndRelu; signature=input:Tinput, filter:Tfilter, bias:float, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasAndReluAndRequantize; signature=input:Tinput, filter:Tfilter, bias:Tbias, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasAndRequantize; signature=input:Tinput, filter:Tfilter, bias:Tbias, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=out_type:type,default=DT_QINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasSignedSumAndReluAndRequantize; signature=input:Tinput, filter:Tfilter, bias:Tbias, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float, summand:Tsummand, min_summand:float, max_summand:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Tsummand:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasSumAndRelu; signature=input:Tinput, filter:Tfilter, bias:float, min_input:float, max_input:float, min_filter:float, max_filter:float, summand:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedConv2DWithBiasSumAndReluAndRequantize; signature=input:Tinput, filter:Tfilter, bias:Tbias, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float, summand:Tsummand, min_summand:float, max_summand:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Tsummand:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedDepthwiseConv2D; signature=input:Tinput, filter:Tfilter, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=QuantizedDepthwiseConv2DWithBias; signature=input:Tinput, filter:Tfilter, bias:float, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=QuantizedDepthwiseConv2DWithBiasAndRelu; signature=input:Tinput, filter:Tfilter, bias:float, min_input:float, max_input:float, min_filter:float, max_filter:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize; signature=input:Tinput, filter:Tfilter, bias:Tbias, min_input:float, max_input:float, min_filter:float, max_filter:float, min_freezed_output:float, max_freezed_output:float -> output:out_type, min_output:float, max_output:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tfilter:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]; attr=dilations:list(int),default=[1, 1, 1, 1]; attr=padding_list:list(int),default=[]>
Op<name=QuantizedInstanceNorm; signature=x:T, x_min:float, x_max:float -> y:T, y_min:float, y_max:float; attr=T:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=output_range_given:bool,default=false; attr=given_y_min:float,default=0; attr=given_y_max:float,default=0; attr=variance_epsilon:float,default=1e-05; attr=min_separation:float,default=0.001>
Op<name=QuantizedMatMul; signature=a:T1, b:T2, min_a:float, max_a:float, min_b:float, max_b:float -> out:Toutput, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Toutput:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=Tactivation:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedMatMulWithBias; signature=a:T1, b:T2, bias:Tbias, min_a:float, max_a:float, min_b:float, max_b:float -> out:Toutput, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Toutput:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=input_quant_mode:string,default="MIN_FIRST",allowed=["MIN_FIRST", "SCALED"]>
Op<name=QuantizedMatMulWithBiasAndDequantize; signature=a:T1, b:T2, bias:Tbias, min_a:float, max_a:float, min_b:float, max_b:float, min_freezed_output:float, max_freezed_output:float -> out:Toutput; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Toutput:type,allowed=[DT_FLOAT]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=input_quant_mode:string,default="MIN_FIRST",allowed=["MIN_FIRST", "SCALED"]>
Op<name=QuantizedMatMulWithBiasAndRelu; signature=a:T1, b:T2, bias:float, min_a:float, max_a:float, min_b:float, max_b:float -> out:Toutput, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Toutput:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=input_quant_mode:string,default="MIN_FIRST",allowed=["MIN_FIRST", "SCALED"]>
Op<name=QuantizedMatMulWithBiasAndReluAndRequantize; signature=a:T1, b:T2, bias:Tbias, min_a:float, max_a:float, min_b:float, max_b:float, min_freezed_output:float, max_freezed_output:float -> out:Toutput, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Toutput:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=input_quant_mode:string,default="MIN_FIRST",allowed=["MIN_FIRST", "SCALED"]>
Op<name=QuantizedMatMulWithBiasAndRequantize; signature=a:T1, b:T2, bias:Tbias, min_a:float, max_a:float, min_b:float, max_b:float, min_freezed_output:float, max_freezed_output:float -> out:Toutput, min_out:float, max_out:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Tbias:type,allowed=[DT_FLOAT, DT_QINT32]; attr=Toutput:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=input_quant_mode:string,default="MIN_FIRST",allowed=["MIN_FIRST", "SCALED"]>
Op<name=QuantizedMaxPool; signature=input:T, min_input:float, max_input:float -> output:T, min_output:float, max_output:float; attr=T:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=ksize:list(int); attr=strides:list(int); attr=padding:string,allowed=["SAME", "VALID"]>
Op<name=QuantizedMul; signature=x:T1, y:T2, min_x:float, max_x:float, min_y:float, max_y:float -> z:Toutput, min_z:float, max_z:float; attr=T1:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=T2:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=Toutput:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedRelu; signature=features:Tinput, min_features:float, max_features:float -> activations:out_type, min_activations:float, max_activations:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedRelu6; signature=features:Tinput, min_features:float, max_features:float -> activations:out_type, min_activations:float, max_activations:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedReluX; signature=features:Tinput, max_value:float, min_features:float, max_features:float -> activations:out_type, min_activations:float, max_activations:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=QuantizedReshape; signature=tensor:T, shape:Tshape, input_min:float, input_max:float -> output:T, output_min:float, output_max:float; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=QuantizedResizeBilinear; signature=images:T, size:int32, min:float, max:float -> resized_images:T, out_min:float, out_max:float; attr=T:type,allowed=[DT_QUINT8, DT_QINT32, DT_FLOAT]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=QueueClose; signature=handle:Ref(string) -> ; attr=cancel_pending_enqueues:bool,default=false>
Op<name=QueueCloseV2; signature=handle:resource -> ; attr=cancel_pending_enqueues:bool,default=false; is_stateful=true>
Op<name=QueueDequeue; signature=handle:Ref(string) -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1>
Op<name=QueueDequeueMany; signature=handle:Ref(string), n:int32 -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1>
Op<name=QueueDequeueManyV2; signature=handle:resource, n:int32 -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1; is_stateful=true>
Op<name=QueueDequeueUpTo; signature=handle:Ref(string), n:int32 -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1>
Op<name=QueueDequeueUpToV2; signature=handle:resource, n:int32 -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1; is_stateful=true>
Op<name=QueueDequeueV2; signature=handle:resource -> components:; attr=component_types:list(type),min=1; attr=timeout_ms:int,default=-1; is_stateful=true>
Op<name=QueueEnqueue; signature=handle:Ref(string), components: -> ; attr=Tcomponents:list(type),min=1; attr=timeout_ms:int,default=-1>
Op<name=QueueEnqueueMany; signature=handle:Ref(string), components: -> ; attr=Tcomponents:list(type),min=1; attr=timeout_ms:int,default=-1>
Op<name=QueueEnqueueManyV2; signature=handle:resource, components: -> ; attr=Tcomponents:list(type),min=1; attr=timeout_ms:int,default=-1; is_stateful=true>
Op<name=QueueEnqueueV2; signature=handle:resource, components: -> ; attr=Tcomponents:list(type),min=1; attr=timeout_ms:int,default=-1; is_stateful=true>
Op<name=QueueIsClosed; signature=handle:Ref(string) -> is_closed:bool>
Op<name=QueueIsClosedV2; signature=handle:resource -> is_closed:bool; is_stateful=true>
Op<name=QueueSize; signature=handle:Ref(string) -> size:int32>
Op<name=QueueSizeV2; signature=handle:resource -> size:int32; is_stateful=true>
Op<name=RFFT; signature=input:Treal, fft_length:int32 -> output:Tcomplex; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RFFT2D; signature=input:Treal, fft_length:int32 -> output:Tcomplex; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RFFT3D; signature=input:Treal, fft_length:int32 -> output:Tcomplex; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RFFTND; signature=input:Treal, fft_length:int32, axes:int32 -> output:Tcomplex; attr=Treal:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RGBToHSV; signature=images:T -> output:T; attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=RaggedBincount; signature=splits:int64, values:Tidx, size:Tidx, weights:T -> output:T; attr=Tidx:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=binary_output:bool,default=false>
Op<name=RaggedCountSparseOutput; signature=splits:int64, values:T, weights:output_type -> output_indices:int64, output_values:output_type, output_dense_shape:int64; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=minlength:int,default=-1,min=-1; attr=maxlength:int,default=-1,min=-1; attr=binary_output:bool; attr=output_type:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]>
Op<name=RaggedCross; signature=ragged_values:, ragged_row_splits:, sparse_indices:Nsparse*int64, sparse_values:, sparse_shape:Nsparse*int64, dense_inputs: -> output_values:out_values_type, output_row_splits:out_row_splits_type; attr=Nsparse:int,min=0; attr=input_order:string; attr=hashed_output:bool; attr=num_buckets:int,min=0; attr=hash_key:int; attr=ragged_values_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=ragged_splits_types:list(type),min=0,allowed=[DT_INT32, DT_INT64]; attr=sparse_values_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=dense_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=out_values_type:type,allowed=[DT_INT64, DT_STRING]; attr=out_row_splits_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=RaggedFillEmptyRows; signature=value_rowids:int64, values:T, nrows:int64, default_value:T -> output_value_rowids:int64, output_values:T, empty_row_indicator:bool, reverse_index_map:int64; attr=T:type>
Op<name=RaggedFillEmptyRowsGrad; signature=reverse_index_map:int64, grad_values:T -> d_values:T, d_default_value:T; attr=T:type>
Op<name=RaggedGather; signature=params_nested_splits:PARAMS_RAGGED_RANK*Tsplits, params_dense_values:Tvalues, indices:Tindices -> output_nested_splits:OUTPUT_RAGGED_RANK*Tsplits, output_dense_values:Tvalues; attr=Tvalues:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=PARAMS_RAGGED_RANK:int,min=1; attr=OUTPUT_RAGGED_RANK:int,min=0>
Op<name=RaggedRange; signature=starts:T, limits:T, deltas:T -> rt_nested_splits:Tsplits, rt_dense_values:T; attr=T:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=RaggedTensorFromVariant; signature=encoded_ragged:variant -> output_nested_splits:output_ragged_rank*Tsplits, output_dense_values:Tvalues; attr=input_ragged_rank:int,min=-1; attr=output_ragged_rank:int,min=0; attr=Tvalues:type; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=RaggedTensorToSparse; signature=rt_nested_splits:RAGGED_RANK*Tsplits, rt_dense_values:T -> sparse_indices:int64, sparse_values:T, sparse_dense_shape:int64; attr=RAGGED_RANK:int,min=1; attr=T:type; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=RaggedTensorToTensor; signature=shape:Tshape, values:T, default_value:T, row_partition_tensors:num_row_partition_tensors*Tindex -> result:T; attr=T:type; attr=Tindex:type,allowed=[DT_INT64, DT_INT32]; attr=Tshape:type,allowed=[DT_INT64, DT_INT32]; attr=num_row_partition_tensors:int,min=1; attr=row_partition_types:list(string)>
Op<name=RaggedTensorToVariant; signature=rt_nested_splits:RAGGED_RANK*Tsplits, rt_dense_values:Tvalues -> encoded_ragged:variant; attr=RAGGED_RANK:int,min=0; attr=Tvalues:type; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=batched_input:bool>
Op<name=RaggedTensorToVariantGradient; signature=encoded_ragged_grad:variant, row_splits:Tsplits, dense_values_shape:int32 -> dense_values_grad:Tvalues; attr=Tvalues:type; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=RandomCrop; signature=image:T, size:int64 -> output:T; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=RandomDataset; signature=seed:int64, seed2:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=RandomDatasetV2; signature=seed:int64, seed2:int64, seed_generator:resource -> handle:variant; attr=rerandomize_each_iteration:bool,default=false; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=RandomGamma; signature=shape:S, alpha:T -> output:T; attr=seed:int,default=0; attr=seed2:int,default=0; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; is_stateful=true>
Op<name=RandomGammaGrad; signature=alpha:T, sample:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=RandomIndexShuffle; signature=index:dtype, seed:Tseed, max_index:dtype -> output:dtype; attr=rounds:int,default=4; attr=dtype:type,allowed=[DT_INT32, DT_UINT32, DT_INT64, DT_UINT64]; attr=Tseed:type,allowed=[DT_INT32, DT_UINT32, DT_INT64, DT_UINT64]>
Op<name=RandomPoisson; signature=shape:S, rate:dtype -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=dtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; is_stateful=true>
Op<name=RandomPoissonV2; signature=shape:S, rate:R -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=R:type,default=DT_DOUBLE,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=dtype:type,default=DT_INT64,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; is_stateful=true>
Op<name=RandomShuffle; signature=value:T -> output:T; attr=seed:int,default=0; attr=seed2:int,default=0; attr=T:type; is_stateful=true>
Op<name=RandomShuffleQueue; signature= -> handle:Ref(string); attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=min_after_dequeue:int,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=RandomShuffleQueueV2; signature= -> handle:resource; attr=component_types:list(type),min=1; attr=shapes:list(shape),default=[],min=0; attr=capacity:int,default=-1; attr=min_after_dequeue:int,default=0; attr=seed:int,default=0; attr=seed2:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=RandomStandardNormal; signature=shape:T -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=dtype:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=RandomUniform; signature=shape:T -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=dtype:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=RandomUniformInt; signature=shape:T, minval:Tout, maxval:Tout -> output:Tout; attr=seed:int,default=0; attr=seed2:int,default=0; attr=Tout:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]>
Op<name=RangeDataset; signature=start:int64, stop:int64, step:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; attr=replicate_on_split:bool,default=false; is_stateful=true>
Op<name=Rank; signature=input:T -> output:int32; attr=T:type>
Op<name=ReadFile; signature=filename:string -> contents:string>
Op<name=ReadVariableOp; signature=resource:resource -> value:dtype; attr=dtype:type; is_stateful=true>
Op<name=ReadVariableXlaSplitND; signature=resource:resource -> outputs:N*T; attr=T:type; attr=N:int,min=1; attr=num_splits:list(int); attr=paddings:list(int),default=[]; is_stateful=true>
Op<name=ReaderNumRecordsProduced; signature=reader_handle:Ref(string) -> records_produced:int64>
Op<name=ReaderNumRecordsProducedV2; signature=reader_handle:resource -> records_produced:int64; is_stateful=true>
Op<name=ReaderNumWorkUnitsCompleted; signature=reader_handle:Ref(string) -> units_completed:int64>
Op<name=ReaderNumWorkUnitsCompletedV2; signature=reader_handle:resource -> units_completed:int64; is_stateful=true>
Op<name=ReaderRead; signature=reader_handle:Ref(string), queue_handle:Ref(string) -> key:string, value:string>
Op<name=ReaderReadUpTo; signature=reader_handle:Ref(string), queue_handle:Ref(string), num_records:int64 -> keys:string, values:string>
Op<name=ReaderReadUpToV2; signature=reader_handle:resource, queue_handle:resource, num_records:int64 -> keys:string, values:string; is_stateful=true>
Op<name=ReaderReadV2; signature=reader_handle:resource, queue_handle:resource -> key:string, value:string; is_stateful=true>
Op<name=ReaderReset; signature=reader_handle:Ref(string) -> >
Op<name=ReaderResetV2; signature=reader_handle:resource -> ; is_stateful=true>
Op<name=ReaderRestoreState; signature=reader_handle:Ref(string), state:string -> >
Op<name=ReaderRestoreStateV2; signature=reader_handle:resource, state:string -> ; is_stateful=true>
Op<name=ReaderSerializeState; signature=reader_handle:Ref(string) -> state:string>
Op<name=ReaderSerializeStateV2; signature=reader_handle:resource -> state:string; is_stateful=true>
Op<name=Real; signature=input:T -> output:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=RealDiv; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RebatchDataset; signature=input_dataset:variant, num_replicas:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_fallback:bool,default=true>
Op<name=RebatchDatasetV2; signature=input_dataset:variant, batch_sizes:int64, drop_remainder:bool -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Reciprocal; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=ReciprocalGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RecordInput; signature= -> records:string; attr=file_pattern:string; attr=file_random_seed:int,default=301; attr=file_shuffle_shift_ratio:float,default=0; attr=file_buffer_size:int,default=10000; attr=file_parallelism:int,default=16; attr=batch_size:int,default=32; attr=compression_type:string,default=""; is_stateful=true>
Op<name=Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true; is_distributed_communication=true>
Op<name=RecvTPUEmbeddingActivations; signature= -> outputs:num_outputs*float; attr=num_outputs:int,min=1; attr=config:string; is_stateful=true>
Op<name=ReduceDataset; signature=input_dataset:variant, initial_state:, other_arguments: -> components:; attr=f:func; attr=Tstate:list(type),min=1; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=metadata:string,default=""; is_stateful=true>
Op<name=ReduceJoin; signature=inputs:string, reduction_indices:int32 -> output:string; attr=keep_dims:bool,default=false; attr=separator:string,default="">
Op<name=RefEnter; signature=data:Ref(T) -> output:Ref(T); attr=T:type; attr=frame_name:string; attr=is_constant:bool,default=false; attr=parallel_iterations:int,default=10>
Op<name=RefExit; signature=data:Ref(T) -> output:Ref(T); attr=T:type>
Op<name=RefIdentity; signature=input:Ref(T) -> output:Ref(T); attr=T:type; allows_uninitialized_input=true>
Op<name=RefMerge; signature=inputs:Ref(N*T) -> output:Ref(T), value_index:int32; attr=T:type; attr=N:int,min=1>
Op<name=RefNextIteration; signature=data:Ref(T) -> output:Ref(T); attr=T:type>
Op<name=RefSelect; signature=index:int32, inputs:Ref(N*T) -> output:Ref(T); attr=T:type; attr=N:int,min=1>
Op<name=RefSwitch; signature=data:Ref(T), pred:bool -> output_false:Ref(T), output_true:Ref(T); attr=T:type; allows_uninitialized_input=true>
Op<name=RegexFullMatch; signature=input:string, pattern:string -> output:bool>
Op<name=RegexReplace; signature=input:string, pattern:string, rewrite:string -> output:string; attr=replace_global:bool,default=true>
Op<name=RegisterDataset; signature=dataset:variant, address:string, protocol:string -> dataset_id:int64; attr=external_state_policy:int; attr=element_spec:string,default=""; attr=metadata:string,default="">
Op<name=RegisterDatasetV2; signature=dataset:variant, address:string, protocol:string -> dataset_id:string; attr=external_state_policy:int; attr=element_spec:string,default=""; attr=requested_dataset_id:string,default=""; attr=metadata:string,default="">
Op<name=Relayout; signature=input:T -> output:T; attr=layout:string; attr=T:type>
Op<name=RelayoutLike; signature=input:T, layout_input:U -> output:T; attr=T:type; attr=U:type>
Op<name=Relu; signature=features:T -> activations:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_QINT8]>
Op<name=Relu6; signature=features:T -> activations:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=Relu6Grad; signature=gradients:T, features:T -> backprops:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=ReluGrad; signature=gradients:T, features:T -> backprops:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=RemoteCall; signature=target:string, args: -> output:; attr=Tin:list(type),min=1; attr=Tout:list(type),min=1; attr=f:func; is_stateful=true>
Op<name=RepeatDataset; signature=input_dataset:variant, count:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=RequantizationRange; signature=input:Tinput, input_min:float, input_max:float -> output_min:float, output_max:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=RequantizationRangePerChannel; signature=input:T, input_min:float, input_max:float -> output_min:float, output_max:float; attr=T:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=clip_value_max:float>
Op<name=Requantize; signature=input:Tinput, input_min:float, input_max:float, requested_output_min:float, requested_output_max:float -> output:out_type, output_min:float, output_max:float; attr=Tinput:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=RequantizePerChannel; signature=input:T, input_min:float, input_max:float, requested_output_min:float, requested_output_max:float -> output:out_type, output_min:float, output_max:float; attr=T:type,default=DT_QINT32,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]; attr=out_type:type,default=DT_QUINT8,allowed=[DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16]>
Op<name=Reshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ResizeArea; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16]; attr=align_corners:bool,default=false>
Op<name=ResizeBicubic; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResizeBicubicGrad; signature=grads:float, original_image:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResizeBilinear; signature=images:T, size:int32 -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResizeBilinearGrad; signature=grads:float, original_image:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_BFLOAT16, DT_HALF, DT_DOUBLE]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResizeNearestNeighbor; signature=images:T, size:int32 -> resized_images:T; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResizeNearestNeighborGrad; signature=grads:T, size:int32 -> output:T; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT32, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16]; attr=align_corners:bool,default=false; attr=half_pixel_centers:bool,default=false>
Op<name=ResourceAccumulatorApplyGradient; signature=handle:resource, local_step:int64, gradient:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; is_stateful=true>
Op<name=ResourceAccumulatorNumAccumulated; signature=handle:resource -> num_accumulated:int32; is_stateful=true>
Op<name=ResourceAccumulatorSetGlobalStep; signature=handle:resource, new_global_step:int64 -> ; is_stateful=true>
Op<name=ResourceAccumulatorTakeGradient; signature=handle:resource, num_required:int32 -> average:dtype; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; is_stateful=true>
Op<name=ResourceApplyAdaMax; signature=var:resource, m:resource, v:resource, beta1_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyAdadelta; signature=var:resource, accum:resource, accum_update:resource, lr:T, rho:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyAdagrad; signature=var:resource, accum:resource, lr:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true; is_stateful=true>
Op<name=ResourceApplyAdagradDA; signature=var:resource, gradient_accumulator:resource, gradient_squared_accumulator:resource, grad:T, lr:T, l1:T, l2:T, global_step:int64 -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyAdagradV2; signature=var:resource, accum:resource, lr:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true; is_stateful=true>
Op<name=ResourceApplyAdam; signature=var:resource, m:resource, v:resource, beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false; is_stateful=true>
Op<name=ResourceApplyAdamWithAmsgrad; signature=var:resource, m:resource, v:resource, vhat:resource, beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyAddSign; signature=var:resource, m:resource, lr:T, alpha:T, sign_decay:T, beta:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyCenteredRMSProp; signature=var:resource, mg:resource, ms:resource, mom:resource, lr:T, rho:T, momentum:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyFtrl; signature=var:resource, accum:resource, linear:resource, grad:T, lr:T, l1:T, l2:T, lr_power:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false; is_stateful=true>
Op<name=ResourceApplyFtrlV2; signature=var:resource, accum:resource, linear:resource, grad:T, lr:T, l1:T, l2:T, l2_shrinkage:T, lr_power:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false; is_stateful=true>
Op<name=ResourceApplyGradientDescent; signature=var:resource, alpha:T, delta:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyKerasMomentum; signature=var:resource, accum:resource, lr:T, grad:T, momentum:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false; is_stateful=true>
Op<name=ResourceApplyMomentum; signature=var:resource, accum:resource, lr:T, grad:T, momentum:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false; is_stateful=true>
Op<name=ResourceApplyPowerSign; signature=var:resource, m:resource, lr:T, logbase:T, sign_decay:T, beta:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyProximalAdagrad; signature=var:resource, accum:resource, lr:T, l1:T, l2:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyProximalGradientDescent; signature=var:resource, alpha:T, l1:T, l2:T, delta:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceApplyRMSProp; signature=var:resource, ms:resource, mom:resource, lr:T, rho:T, momentum:T, epsilon:T, grad:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceConditionalAccumulator; signature= -> handle:resource; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=""; attr=shared_name:string,default=""; attr=reduction_type:string,default="MEAN",allowed=["MEAN", "SUM"]; is_stateful=true>
Op<name=ResourceCountUpTo; signature=resource:resource -> output:T; attr=limit:int; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=batch_dims:int,default=0; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceGatherNd; signature=resource:resource, indices:Tindices -> output:dtype; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterAdd; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterDiv; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterMax; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterMin; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterMul; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterNdAdd; signature=ref:resource, indices:Tindices, updates:T -> ; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true; is_stateful=true>
Op<name=ResourceScatterNdMax; signature=ref:resource, indices:Tindices, updates:T -> ; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true; is_stateful=true>
Op<name=ResourceScatterNdMin; signature=ref:resource, indices:Tindices, updates:T -> ; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true; is_stateful=true>
Op<name=ResourceScatterNdSub; signature=ref:resource, indices:Tindices, updates:T -> ; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true; is_stateful=true>
Op<name=ResourceScatterNdUpdate; signature=ref:resource, indices:Tindices, updates:T -> ; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true; is_stateful=true>
Op<name=ResourceScatterSub; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceScatterUpdate; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=ResourceSparseApplyAdadelta; signature=var:resource, accum:resource, accum_update:resource, lr:T, rho:T, epsilon:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyAdagrad; signature=var:resource, accum:resource, lr:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true; is_stateful=true>
Op<name=ResourceSparseApplyAdagradDA; signature=var:resource, gradient_accumulator:resource, gradient_squared_accumulator:resource, grad:T, indices:Tindices, lr:T, l1:T, l2:T, global_step:int64 -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyAdagradV2; signature=var:resource, accum:resource, lr:T, epsilon:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true; is_stateful=true>
Op<name=ResourceSparseApplyCenteredRMSProp; signature=var:resource, mg:resource, ms:resource, mom:resource, lr:T, rho:T, momentum:T, epsilon:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyFtrl; signature=var:resource, accum:resource, linear:resource, grad:T, indices:Tindices, lr:T, l1:T, l2:T, lr_power:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyFtrlV2; signature=var:resource, accum:resource, linear:resource, grad:T, indices:Tindices, lr:T, l1:T, l2:T, l2_shrinkage:T, lr_power:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyKerasMomentum; signature=var:resource, accum:resource, lr:T, grad:T, indices:Tindices, momentum:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyMomentum; signature=var:resource, accum:resource, lr:T, grad:T, indices:Tindices, momentum:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyProximalAdagrad; signature=var:resource, accum:resource, lr:T, l1:T, l2:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyProximalGradientDescent; signature=var:resource, alpha:T, l1:T, l2:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceSparseApplyRMSProp; signature=var:resource, ms:resource, mom:resource, lr:T, rho:T, momentum:T, epsilon:T, grad:T, indices:Tindices -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; is_stateful=true>
Op<name=ResourceStridedSliceAssign; signature=ref:resource, begin:Index, end:Index, strides:Index, value:T -> ; attr=T:type; attr=Index:type,allowed=[DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0; is_stateful=true>
Op<name=Restore; signature=file_pattern:string, tensor_name:string -> tensor:dt; attr=dt:type; attr=preferred_shard:int,default=-1; is_stateful=true>
Op<name=RestoreSlice; signature=file_pattern:string, tensor_name:string, shape_and_slice:string -> tensor:dt; attr=dt:type; attr=preferred_shard:int,default=-1; is_stateful=true>
Op<name=RestoreV2; signature=prefix:string, tensor_names:string, shape_and_slices:string -> tensors:; attr=dtypes:list(type),min=1; is_stateful=true>
Op<name=RetrieveAllTPUEmbeddingParameters; signature= -> parameters:NumTables*float, auxiliary1:NumTables*float, auxiliary2:NumTables*float, auxiliary3:NumTables*float, auxiliary4:NumTables*float, auxiliary5:NumTables*float, auxiliary6:NumTables*float, auxiliary7:NumTables*float; attr=NumTables:int,min=1; attr=config:string; attr=num_shards:int; attr=shard_id:int; is_stateful=true>
Op<name=RetrieveTPUEmbeddingADAMParameters; signature= -> parameters:float, momenta:float, velocities:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingAdadeltaParameters; signature= -> parameters:float, accumulators:float, updates:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingAdagradMomentumParameters; signature= -> parameters:float, accumulators:float, momenta:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingAdagradParameters; signature= -> parameters:float, accumulators:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingCenteredRMSPropParameters; signature= -> parameters:float, ms:float, mom:float, mg:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingFTRLParameters; signature= -> parameters:float, accumulators:float, linears:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingFrequencyEstimatorParameters; signature= -> parameters:float, last_hit_step:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingMDLAdagradLightParameters; signature= -> parameters:float, accumulators:float, weights:float, benefits:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingMomentumParameters; signature= -> parameters:float, momenta:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingProximalAdagradParameters; signature= -> parameters:float, accumulators:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingProximalYogiParameters; signature= -> parameters:float, v:float, m:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingRMSPropParameters; signature= -> parameters:float, ms:float, mom:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=RetrieveTPUEmbeddingStochasticGradientDescentParameters; signature= -> parameters:float; attr=table_id:int,default=-1; attr=table_name:string,default=""; attr=num_shards:int; attr=shard_id:int; attr=config:string,default=""; is_stateful=true>
Op<name=Reverse; signature=tensor:T, dims:bool -> output:T; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_UINT32, DT_INT32, DT_UINT64, DT_INT64, DT_BOOL, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING]>
Op<name=ReverseSequence; signature=input:T, seq_lengths:Tlen -> output:T; attr=seq_dim:int; attr=batch_dim:int,default=0; attr=T:type; attr=Tlen:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=ReverseV2; signature=tensor:T, axis:Tidx -> output:T; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING]>
Op<name=RewriteDataset; signature=input_dataset:variant, rewrite_name:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=RightShift; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64]>
Op<name=Rint; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscAbs; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscAdd; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; is_commutative=true; is_aggregate=true>
Op<name=RiscBinaryArithmetic; signature=x:T, y:T -> z:T; attr=op_type:string,allowed=["ADD", "SUB", "MUL", "DIV", "REM", "MIN", "POW"]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscBinaryComparison; signature=x:T, y:T -> z:bool; attr=op_type:string,allowed=["EQ", "NE", "GE", "GT", "LE", "LT"]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscBitcast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>
Op<name=RiscBroadcast; signature=input:T, shape:Tidx -> output:T; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscCast; signature=x:SrcT -> y:DstT; attr=SrcT:type; attr=DstT:type>
Op<name=RiscCeil; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscCholesky; signature=input:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscConcat; signature=values:N*T, axis:Tidx -> output:T; attr=N:int,min=2; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscCondition; signature=pred:bool, input_true:SrcT, input_false:SrcT -> output:DstT; attr=func_true:func; attr=func_false:func; attr=SrcT:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=DstT:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscConv; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=strides:list(int); attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=dilations:list(int),default=[1, 1, 1, 1]>
Op<name=RiscCos; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscDiv; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscDot; signature=a:T, b:T -> product:T; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscExp; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscFft; signature=input:Tcomplex -> output:Tcomplex; attr=Tcomplex:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RiscFloor; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscGather; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=batch_dims:int,default=0; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscImag; signature=input:T -> output:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=RiscIsFinite; signature=x:T -> y:bool; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscLog; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscLogicalAnd; signature=x:bool, y:bool -> z:bool>
Op<name=RiscLogicalNot; signature=x:bool -> z:bool>
Op<name=RiscLogicalOr; signature=x:bool, y:bool -> z:bool>
Op<name=RiscMax; signature=x:T, y:T -> max:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscMin; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscMul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscNeg; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscPad; signature=input:T, paddings:Tpaddings, constant_values:T -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscPool; signature=value:T -> output:T; attr=ksize:list(int),min=4; attr=strides:list(int),min=4; attr=pooling_type:string,allowed=["AVG", "MAX"]; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW"]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscPow; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscRandomUniform; signature=shape:T -> output:float; attr=seed:int,default=0; attr=T:type,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscReal; signature=input:T -> output:Tout; attr=T:type,default=DT_COMPLEX64,allowed=[DT_COMPLEX64, DT_COMPLEX128]; attr=Tout:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=RiscReduce; signature=tensor:T, axis:Index -> output:T; attr=reduce_type:string,allowed=["MEAN", "SUM"]; attr=Index:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscRem; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscReshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscReverse; signature=tensor:T, axis:Tidx -> output:T; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscScatter; signature=indices:Tindices, updates:T, shape:Tindices -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscShape; signature=input:T -> output:out_type; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscSign; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscSlice; signature=input:T, begin:Index, size:Index -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Index:type,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscSort; signature=input:T, axis:Index -> output:T; attr=Index:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=direction:string,allowed=["ASCENDING", "DESCENDING"]>
Op<name=RiscSqueeze; signature=input:T -> output:T; attr=T:type; attr=squeeze_dims:list(int),default=[],min=0>
Op<name=RiscSub; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscTranspose; signature=x:T, perm:Tperm -> y:T; attr=T:type; attr=Tperm:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=RiscTriangularSolve; signature=matrix:T, rhs:T -> output:T; attr=lower:bool,default=true; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscUnary; signature=x:T -> y:T; attr=op_type:string,allowed=["ABL", "CEIL", "COS", "EXP", "FLOOR", "IMAG", "LOG", "NEG", "REAL", "SIGN"]; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=RiscWhile; signature=input: -> output:; attr=T:list(type),min=0; attr=cond:func; attr=body:func; attr=output_shapes:list(shape),default=[]; attr=parallel_iterations:int,default=10; is_stateful=true>
Op<name=RngReadAndSkip; signature=resource:resource, alg:int32, delta:uint64 -> value:int64; is_stateful=true>
Op<name=RngSkip; signature=resource:resource, algorithm:int64, delta:int64 -> ; is_stateful=true>
Op<name=Roll; signature=input:T, shift:Tshift, axis:Taxis -> output:T; attr=T:type; attr=Tshift:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>
Op<name=Round; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=RsqrtGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SampleDistortedBoundingBox; signature=image_size:T, bounding_boxes:float -> begin:T, size:T, bboxes:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64]; attr=seed:int,default=0; attr=seed2:int,default=0; attr=min_object_covered:float,default=0.1; attr=aspect_ratio_range:list(float),default=[0.75, 1.33]; attr=area_range:list(float),default=[0.05, 1]; attr=max_attempts:int,default=100; attr=use_image_if_no_bounding_boxes:bool,default=false; is_stateful=true>
Op<name=SampleDistortedBoundingBoxV2; signature=image_size:T, bounding_boxes:float, min_object_covered:float -> begin:T, size:T, bboxes:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64]; attr=seed:int,default=0; attr=seed2:int,default=0; attr=aspect_ratio_range:list(float),default=[0.75, 1.33]; attr=area_range:list(float),default=[0.05, 1]; attr=max_attempts:int,default=100; attr=use_image_if_no_bounding_boxes:bool,default=false; is_stateful=true>
Op<name=SamplingDataset; signature=input_dataset:variant, rate:float, seed:int64, seed2:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Save; signature=filename:string, tensor_names:string, data: -> ; attr=T:list(type),min=1; is_stateful=true>
Op<name=SaveDataset; signature=input_dataset:variant, path:string, shard_func_other_args: -> ; attr=compression:string,default=""; attr=shard_func:func; attr=use_shard_func:bool,default=true; attr=Tshard_func_args:list(type),min=0; is_stateful=true>
Op<name=SaveDatasetV2; signature=input_dataset:variant, path:string, shard_func_other_args: -> handle:variant; attr=compression:string,default=""; attr=shard_func:func; attr=use_shard_func:bool,default=true; attr=Tshard_func_args:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=SaveSlices; signature=filename:string, tensor_names:string, shapes_and_slices:string, data: -> ; attr=T:list(type),min=1; is_stateful=true>
Op<name=SaveV2; signature=prefix:string, tensor_names:string, shape_and_slices:string, tensors: -> ; attr=dtypes:list(type),min=1; is_stateful=true>
Op<name=ScalarSummary; signature=tags:string, values:T -> summary:string; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=ScaleAndTranslate; signature=images:T, size:int32, scale:float, translation:float -> resized_images:float; attr=T:type,allowed=[DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=kernel_type:string,default="lanczos3"; attr=antialias:bool,default=true>
Op<name=ScaleAndTranslateGrad; signature=grads:T, original_image:T, scale:float, translation:float -> output:T; attr=T:type,allowed=[DT_FLOAT]; attr=kernel_type:string,default="lanczos3"; attr=antialias:bool,default=true>
Op<name=ScanDataset; signature=input_dataset:variant, initial_state:, other_arguments: -> handle:variant; attr=f:func; attr=Tstate:list(type),min=1; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=preserve_cardinality:bool,default=false; attr=use_default_device:bool,default=true; attr=metadata:string,default="">
Op<name=ScatterAdd; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterDiv; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterMax; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterMin; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterMul; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterNd; signature=indices:Tindices, updates:T, shape:Tindices -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64]>
Op<name=ScatterNdAdd; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterNdMax; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterNdMin; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterNdNonAliasingAdd; signature=input:T, indices:Tindices, updates:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_BOOL]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=ScatterNdSub; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterNdUpdate; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true>
Op<name=ScatterSub; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=ScatterUpdate; signature=ref:Ref(T), indices:Tindices, updates:T -> output_ref:Ref(T); attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=true>
Op<name=SdcaFprint; signature=input:string -> output:int64>
Op<name=SdcaOptimizer; signature=sparse_example_indices:num_sparse_features*int64, sparse_feature_indices:num_sparse_features*int64, sparse_feature_values:num_sparse_features_with_values*float, dense_features:num_dense_features*float, example_weights:float, example_labels:float, sparse_indices:num_sparse_features*int64, sparse_weights:num_sparse_features*float, dense_weights:num_dense_features*float, example_state_data:float -> out_example_state_data:float, out_delta_sparse_weights:num_sparse_features*float, out_delta_dense_weights:num_dense_features*float; attr=loss_type:string,allowed=["logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"]; attr=adaptative:bool,default=false; attr=num_sparse_features:int,min=0; attr=num_sparse_features_with_values:int,min=0; attr=num_dense_features:int,min=0; attr=l1:float; attr=l2:float; attr=num_loss_partitions:int,min=1; attr=num_inner_iterations:int,min=1>
Op<name=SdcaOptimizerV2; signature=sparse_example_indices:num_sparse_features*int64, sparse_feature_indices:num_sparse_features*int64, sparse_feature_values:num_sparse_features_with_values*float, dense_features:num_dense_features*float, example_weights:float, example_labels:float, sparse_indices:num_sparse_features*int64, sparse_weights:num_sparse_features*float, dense_weights:num_dense_features*float, example_state_data:float -> out_example_state_data:float, out_delta_sparse_weights:num_sparse_features*float, out_delta_dense_weights:num_dense_features*float; attr=loss_type:string,allowed=["logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"]; attr=adaptive:bool,default=false; attr=num_sparse_features:int,min=0; attr=num_sparse_features_with_values:int,min=0; attr=num_dense_features:int,min=0; attr=l1:float; attr=l2:float; attr=num_loss_partitions:int,min=1; attr=num_inner_iterations:int,min=1>
Op<name=SdcaShrinkL1; signature=weights:Ref(num_features*float) -> ; attr=num_features:int,min=0; attr=l1:float; attr=l2:float>
Op<name=SegmentMax; signature=data:T, segment_ids:Tindices -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentMaxV2; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentMean; signature=data:T, segment_ids:Tindices -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentMin; signature=data:T, segment_ids:Tindices -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentMinV2; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentProd; signature=data:T, segment_ids:Tindices -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentProdV2; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentSum; signature=data:T, segment_ids:Tindices -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SegmentSumV2; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Select; signature=condition:bool, t:T, e:T -> output:T; attr=T:type>
Op<name=SelectV2; signature=condition:bool, t:T, e:T -> output:T; attr=T:type>
Op<name=SelfAdjointEig; signature=input:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF]>
Op<name=SelfAdjointEigV2; signature=input:T -> e:T, v:T; attr=compute_v:bool,default=true; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Selu; signature=features:T -> activations:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=SeluGrad; signature=gradients:T, outputs:T -> backprops:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=Send; signature=tensor:T -> ; attr=T:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
Op<name=SendTPUEmbeddingGradients; signature=inputs:N*float, learning_rates:NN*float -> ; attr=N:int,min=1; attr=NN:int,default=0,min=0; attr=config:string; is_stateful=true>
Op<name=SerializeIterator; signature=resource_handle:resource -> serialized:variant; attr=external_state_policy:int,default=0; is_stateful=true>
Op<name=SerializeManySparse; signature=sparse_indices:int64, sparse_values:T, sparse_shape:int64 -> serialized_sparse:out_type; attr=T:type; attr=out_type:type,default=DT_STRING,allowed=[DT_STRING, DT_VARIANT]>
Op<name=SerializeSparse; signature=sparse_indices:int64, sparse_values:T, sparse_shape:int64 -> serialized_sparse:out_type; attr=T:type; attr=out_type:type,default=DT_STRING,allowed=[DT_STRING, DT_VARIANT]>
Op<name=SerializeTensor; signature=tensor:T -> serialized:string; attr=T:type>
Op<name=SetSize; signature=set_indices:int64, set_values:T, set_shape:int64 -> size:int32; attr=validate_indices:bool,default=true; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_STRING]>
Op<name=SetStatsAggregatorDataset; signature=input_dataset:variant, stats_aggregator:resource, tag:string, counter_prefix:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=Shape; signature=input:T -> output:out_type; attr=T:type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ShapeN; signature=input:N*T -> output:N*out_type; attr=N:int,min=1; attr=T:type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=ShardDataset; signature=input_dataset:variant, num_shards:int64, index:int64 -> handle:variant; attr=require_non_empty:bool,default=false; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ShardedFilename; signature=basename:string, shard:int32, num_shards:int32 -> filename:string>
Op<name=ShardedFilespec; signature=basename:string, num_shards:int32 -> filename:string>
Op<name=ShuffleAndRepeatDataset; signature=input_dataset:variant, buffer_size:int64, seed:int64, seed2:int64, count:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=reshuffle_each_iteration:bool,default=true; attr=metadata:string,default="">
Op<name=ShuffleAndRepeatDatasetV2; signature=input_dataset:variant, buffer_size:int64, seed:int64, seed2:int64, count:int64, seed_generator:resource -> handle:variant; attr=reshuffle_each_iteration:bool,default=true; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=ShuffleDataset; signature=input_dataset:variant, buffer_size:int64, seed:int64, seed2:int64 -> handle:variant; attr=reshuffle_each_iteration:bool,default=true; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=ShuffleDatasetV2; signature=input_dataset:variant, buffer_size:int64, seed_generator:resource -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=ShuffleDatasetV3; signature=input_dataset:variant, buffer_size:int64, seed:int64, seed2:int64, seed_generator:resource -> handle:variant; attr=reshuffle_each_iteration:bool,default=true; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=ShutdownDistributedTPU; signature= -> ; is_stateful=true>
Op<name=ShutdownTPUSystem; signature= -> success:bool; is_stateful=true>
Op<name=Sigmoid; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SigmoidGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Sign; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Sin; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Sinh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Size; signature=input:T -> output:out_type; attr=T:type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SkipDataset; signature=input_dataset:variant, count:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=Skipgram; signature= -> vocab_word:string, vocab_freq:int32, words_per_epoch:int64, current_epoch:int32, total_words_processed:int64, examples:int32, labels:int32; attr=filename:string; attr=batch_size:int; attr=window_size:int,default=5; attr=min_count:int,default=5; attr=subsample:float,default=0.001; is_stateful=true>
Op<name=SleepDataset; signature=input_dataset:variant, sleep_microseconds:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Slice; signature=input:T, begin:Index, size:Index -> output:T; attr=T:type; attr=Index:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SlidingWindowDataset; signature=input_dataset:variant, window_size:int64, window_shift:int64, window_stride:int64 -> handle:variant; attr=drop_remainder:bool,default=true; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=Snapshot; signature=input:T -> output:T; attr=T:type>
Op<name=SnapshotChunkDataset; signature=chunk_file:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=compression:string,default="">
Op<name=SnapshotDataset; signature=input_dataset:variant, path:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=compression:string,default=""; attr=reader_path_prefix:string,default=""; attr=writer_path_prefix:string,default=""; attr=shard_size_bytes:int,default=10737418240; attr=pending_snapshot_expiry_seconds:int,default=86400; attr=num_reader_threads:int,default=1; attr=reader_buffer_size:int,default=1; attr=num_writer_threads:int,default=1; attr=writer_buffer_size:int,default=1; attr=shuffle_on_read:bool,default=false; attr=seed:int,default=0; attr=seed2:int,default=0; attr=mode:string,default="auto"; attr=snapshot_name:string,default="">
Op<name=SnapshotDatasetReader; signature=shard_dir:string, start_index:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=compression:string,default=""; attr=version:int>
Op<name=SnapshotDatasetV2; signature=input_dataset:variant, path:string, reader_func_other_args:, shard_func_other_args: -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=compression:string,default=""; attr=reader_prefix:string,default=""; attr=writer_prefix:string,default=""; attr=hash_valid:bool,default=false; attr=hash:int,default=0; attr=reader_func:func; attr=shard_func:func; attr=Treader_func_args:list(type),min=0; attr=Tshard_func_args:list(type),min=0; attr=metadata:string,default="">
Op<name=SnapshotNestedDatasetReader; signature=inputs:N*variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1>
Op<name=SobolSample; signature=dim:int32, num_results:int32, skip:int32 -> samples:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=Softmax; signature=logits:T -> softmax:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=SoftmaxCrossEntropyWithLogits; signature=features:T, labels:T -> loss:T, backprop:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=Softplus; signature=features:T -> activations:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=SoftplusGrad; signature=gradients:T, features:T -> backprops:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=Softsign; signature=features:T -> activations:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=SoftsignGrad; signature=gradients:T, features:T -> backprops:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>
Op<name=SpaceToBatch; signature=input:T, paddings:Tpaddings -> output:T; attr=T:type; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=block_size:int,min=2>
Op<name=SpaceToBatchND; signature=input:T, block_shape:Tblock_shape, paddings:Tpaddings -> output:T; attr=T:type; attr=Tblock_shape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tpaddings:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SpaceToDepth; signature=input:T -> output:T; attr=T:type; attr=block_size:int,min=2; attr=data_format:string,default="NHWC",allowed=["NHWC", "NCHW", "NCHW_VECT_C"]>
Op<name=SparseAccumulatorApplyGradient; signature=handle:Ref(string), local_step:int64, gradient_indices:int64, gradient_values:dtype, gradient_shape:int64 -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=has_known_shape:bool>
Op<name=SparseAccumulatorTakeGradient; signature=handle:Ref(string), num_required:int32 -> indices:int64, values:dtype, shape:int64; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseAdd; signature=a_indices:int64, a_values:T, a_shape:int64, b_indices:int64, b_values:T, b_shape:int64, thresh:Treal -> sum_indices:int64, sum_values:T, sum_shape:int64; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Treal:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseAddGrad; signature=backprop_val_grad:T, a_indices:int64, b_indices:int64, sum_indices:int64 -> a_val_grad:T, b_val_grad:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseApplyAdadelta; signature=var:Ref(T), accum:Ref(T), accum_update:Ref(T), lr:T, rho:T, epsilon:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseApplyAdagrad; signature=var:Ref(T), accum:Ref(T), lr:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true>
Op<name=SparseApplyAdagradDA; signature=var:Ref(T), gradient_accumulator:Ref(T), gradient_squared_accumulator:Ref(T), grad:T, indices:Tindices, lr:T, l1:T, l2:T, global_step:int64 -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseApplyAdagradV2; signature=var:Ref(T), accum:Ref(T), lr:T, epsilon:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=update_slots:bool,default=true>
Op<name=SparseApplyCenteredRMSProp; signature=var:Ref(T), mg:Ref(T), ms:Ref(T), mom:Ref(T), lr:T, rho:T, momentum:T, epsilon:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseApplyFtrl; signature=var:Ref(T), accum:Ref(T), linear:Ref(T), grad:T, indices:Tindices, lr:T, l1:T, l2:T, lr_power:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false>
Op<name=SparseApplyFtrlV2; signature=var:Ref(T), accum:Ref(T), linear:Ref(T), grad:T, indices:Tindices, lr:T, l1:T, l2:T, l2_shrinkage:T, lr_power:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=multiply_linear_by_lr:bool,default=false>
Op<name=SparseApplyMomentum; signature=var:Ref(T), accum:Ref(T), lr:T, grad:T, indices:Tindices, momentum:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false; attr=use_nesterov:bool,default=false>
Op<name=SparseApplyProximalAdagrad; signature=var:Ref(T), accum:Ref(T), lr:T, l1:T, l2:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseApplyProximalGradientDescent; signature=var:Ref(T), alpha:T, l1:T, l2:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseApplyRMSProp; signature=var:Ref(T), ms:Ref(T), mom:Ref(T), lr:T, rho:T, momentum:T, epsilon:T, grad:T, indices:Tindices -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=use_locking:bool,default=false>
Op<name=SparseBincount; signature=indices:int64, values:Tidx, dense_shape:int64, size:Tidx, weights:T -> output:T; attr=Tidx:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]; attr=binary_output:bool,default=false>
Op<name=SparseConcat; signature=indices:N*int64, values:N*T, shapes:N*int64 -> output_indices:int64, output_values:T, output_shape:int64; attr=concat_dim:int; attr=N:int,min=2; attr=T:type>
Op<name=SparseConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=""; attr=shared_name:string,default=""; attr=reduction_type:string,default="MEAN",allowed=["MEAN", "SUM"]; is_stateful=true>
Op<name=SparseCountSparseOutput; signature=indices:int64, values:T, dense_shape:int64, weights:output_type -> output_indices:int64, output_values:output_type, output_dense_shape:int64; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=minlength:int,default=-1,min=-1; attr=maxlength:int,default=-1,min=-1; attr=binary_output:bool; attr=output_type:type,allowed=[DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE]>
Op<name=SparseCross; signature=indices:N*int64, values:, shapes:N*int64, dense_inputs: -> output_indices:int64, output_values:out_type, output_shape:int64; attr=N:int,min=0; attr=hashed_output:bool; attr=num_buckets:int,min=0; attr=hash_key:int; attr=sparse_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=dense_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=out_type:type,allowed=[DT_INT64, DT_STRING]; attr=internal_type:type,allowed=[DT_INT64, DT_STRING]>
Op<name=SparseCrossHashed; signature=indices:N*int64, values:, shapes:N*int64, dense_inputs:, num_buckets:int64, strong_hash:bool, salt:int64 -> output_indices:int64, output_values:int64, output_shape:int64; attr=N:int,min=0; attr=sparse_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=dense_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]>
Op<name=SparseCrossV2; signature=indices:N*int64, values:, shapes:N*int64, dense_inputs:, sep:string -> output_indices:int64, output_values:string, output_shape:int64; attr=N:int,min=0; attr=sparse_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]; attr=dense_types:list(type),min=0,allowed=[DT_INT64, DT_STRING]>
Op<name=SparseDenseCwiseAdd; signature=sp_indices:int64, sp_values:T, sp_shape:int64, dense:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseDenseCwiseDiv; signature=sp_indices:int64, sp_values:T, sp_shape:int64, dense:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseDenseCwiseMul; signature=sp_indices:int64, sp_values:T, sp_shape:int64, dense:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseFillEmptyRows; signature=indices:int64, values:T, dense_shape:int64, default_value:T -> output_indices:int64, output_values:T, empty_row_indicator:bool, reverse_index_map:int64; attr=T:type>
Op<name=SparseFillEmptyRowsGrad; signature=reverse_index_map:int64, grad_values:T -> d_values:T, d_default_value:T; attr=T:type>
Op<name=SparseMatMul; signature=a:Ta, b:Tb -> product:float; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=a_is_sparse:bool,default=false; attr=b_is_sparse:bool,default=false; attr=Ta:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_BFLOAT16]; attr=Tb:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_BFLOAT16]>
Op<name=SparseMatrixAdd; signature=a:variant, b:variant, alpha:T, beta:T -> c:variant; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SparseMatrixMatMul; signature=a:variant, b:T -> output:T; attr=T:type; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=adjoint_a:bool,default=false; attr=adjoint_b:bool,default=false; attr=transpose_output:bool,default=false; attr=conjugate_output:bool,default=false>
Op<name=SparseMatrixMul; signature=a:variant, b:T -> output:variant; attr=T:type>
Op<name=SparseMatrixNNZ; signature=sparse_matrix:variant -> nnz:int32>
Op<name=SparseMatrixOrderingAMD; signature=input:variant -> output:int32>
Op<name=SparseMatrixSoftmax; signature=logits:variant -> softmax:variant; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=SparseMatrixSoftmaxGrad; signature=softmax:variant, grad_softmax:variant -> gradient:variant; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=SparseMatrixSparseCholesky; signature=input:variant, permutation:int32 -> output:variant; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SparseMatrixSparseMatMul; signature=a:variant, b:variant -> c:variant; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]; attr=transpose_a:bool,default=false; attr=transpose_b:bool,default=false; attr=adjoint_a:bool,default=false; attr=adjoint_b:bool,default=false>
Op<name=SparseMatrixTranspose; signature=input:variant -> output:variant; attr=conjugate:bool,default=false; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SparseMatrixZeros; signature=dense_shape:int64 -> sparse_matrix:variant; attr=type:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SparseReduceMax; signature=input_indices:int64, input_values:T, input_shape:int64, reduction_axes:int32 -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseReduceMaxSparse; signature=input_indices:int64, input_values:T, input_shape:int64, reduction_axes:int32 -> output_indices:int64, output_values:T, output_shape:int64; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseReduceSum; signature=input_indices:int64, input_values:T, input_shape:int64, reduction_axes:int32 -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseReduceSumSparse; signature=input_indices:int64, input_values:T, input_shape:int64, reduction_axes:int32 -> output_indices:int64, output_values:T, output_shape:int64; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseReorder; signature=input_indices:int64, input_values:T, input_shape:int64 -> output_indices:int64, output_values:T; attr=T:type>
Op<name=SparseReshape; signature=input_indices:int64, input_shape:int64, new_shape:int64 -> output_indices:int64, output_shape:int64>
Op<name=SparseSegmentMean; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSegmentMeanGrad; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, output_dim0:int32 -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentMeanGradV2; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, dense_output_dim0:int32 -> output:T, sorted_unique_indices:Tidx; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentMeanWithNumSegments; signature=data:T, indices:Tidx, segment_ids:Tsegmentids, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSegmentSqrtN; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSegmentSqrtNGrad; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, output_dim0:int32 -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentSqrtNGradV2; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, dense_output_dim0:int32 -> output:T, sorted_unique_indices:Tidx; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentSqrtNWithNumSegments; signature=data:T, indices:Tidx, segment_ids:Tsegmentids, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSegmentSum; signature=data:T, indices:Tidx, segment_ids:Tsegmentids -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSegmentSumGrad; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, output_dim0:int32 -> output:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentSumGradV2; signature=grad:T, indices:Tidx, segment_ids:Tsegmentids, dense_output_dim0:int32 -> output:T, sorted_unique_indices:Tidx; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSegmentSumWithNumSegments; signature=data:T, indices:Tidx, segment_ids:Tsegmentids, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tsegmentids:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=sparse_gradient:bool,default=false>
Op<name=SparseSlice; signature=indices:int64, values:T, shape:int64, start:int64, size:int64 -> output_indices:int64, output_values:T, output_shape:int64; attr=T:type>
Op<name=SparseSliceGrad; signature=backprop_val_grad:T, input_indices:int64, input_start:int64, output_indices:int64 -> val_grad:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseSoftmax; signature=sp_indices:int64, sp_values:T, sp_shape:int64 -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=SparseSoftmaxCrossEntropyWithLogits; signature=features:T, labels:Tlabels -> loss:T, backprop:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=Tlabels:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseSparseMaximum; signature=a_indices:int64, a_values:T, a_shape:int64, b_indices:int64, b_values:T, b_shape:int64 -> output_indices:int64, output_values:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseSparseMinimum; signature=a_indices:int64, a_values:T, a_shape:int64, b_indices:int64, b_values:T, b_shape:int64 -> output_indices:int64, output_values:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=SparseSplit; signature=split_dim:int64, indices:int64, values:T, shape:int64 -> output_indices:num_split*int64, output_values:num_split*T, output_shape:num_split*int64; attr=num_split:int,min=1; attr=T:type>
Op<name=SparseTensorDenseAdd; signature=a_indices:Tindices, a_values:T, a_shape:Tindices, b:T -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseTensorDenseMatMul; signature=a_indices:Tindices, a_values:T, a_shape:int64, b:T -> product:T; attr=T:type; attr=Tindices:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=adjoint_a:bool,default=false; attr=adjoint_b:bool,default=false>
Op<name=SparseTensorSliceDataset; signature=indices:int64, values:Tvalues, dense_shape:int64 -> handle:variant; attr=Tvalues:type; is_stateful=true>
Op<name=SparseTensorToCSRSparseMatrix; signature=indices:int64, values:T, dense_shape:int64 -> sparse_matrix:variant; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SparseToDense; signature=sparse_indices:Tindices, output_shape:Tindices, sparse_values:T, default_value:T -> dense:T; attr=validate_indices:bool,default=true; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=SparseToSparseSetOperation; signature=set1_indices:int64, set1_values:T, set1_shape:int64, set2_indices:int64, set2_values:T, set2_shape:int64 -> result_indices:int64, result_values:T, result_shape:int64; attr=set_operation:string; attr=validate_indices:bool,default=true; attr=T:type,allowed=[DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_STRING]>
Op<name=Spence; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=Split; signature=split_dim:int32, value:T -> output:num_split*T; attr=num_split:int,min=1; attr=T:type>
Op<name=SplitDedupData; signature=input:variant -> integer_tensor:integer_type, float_tensor:float_type; attr=integer_type:type,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]; attr=float_type:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT]; attr=tuple_mask:string; attr=config:string,default="">
Op<name=SplitV; signature=value:T, size_splits:Tlen, split_dim:int32 -> output:num_split*T; attr=num_split:int,min=1; attr=T:type; attr=Tlen:type,default=DT_INT64,allowed=[DT_INT8, DT_INT32, DT_INT64]>
Op<name=SqlDataset; signature=driver_name:string, data_source_name:string, query:string -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=Sqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SqrtGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Square; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=SquaredDifference; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true>
Op<name=Squeeze; signature=input:T -> output:T; attr=T:type; attr=squeeze_dims:list(int),default=[],min=0>
Op<name=Stack; signature= -> handle:Ref(string); attr=elem_type:type; attr=stack_name:string,default=""; is_stateful=true>
Op<name=StackClose; signature=handle:Ref(string) -> >
Op<name=StackCloseV2; signature=handle:resource -> ; is_stateful=true>
Op<name=StackPop; signature=handle:Ref(string) -> elem:elem_type; attr=elem_type:type>
Op<name=StackPopV2; signature=handle:resource -> elem:elem_type; attr=elem_type:type; is_stateful=true>
Op<name=StackPush; signature=handle:Ref(string), elem:T -> output:T; attr=T:type; attr=swap_memory:bool,default=false>
Op<name=StackPushV2; signature=handle:resource, elem:T -> output:T; attr=T:type; attr=swap_memory:bool,default=false; is_stateful=true>
Op<name=StackV2; signature=max_size:int32 -> handle:resource; attr=elem_type:type; attr=stack_name:string,default=""; is_stateful=true>
Op<name=Stage; signature=values: -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StageClear; signature= -> ; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StagePeek; signature=index:int32 -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StageSize; signature= -> size:int32; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StatefulPartitionedCall; signature=args: -> output:; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=f:func; attr=config:string,default=""; attr=config_proto:string,default=""; attr=executor_type:string,default=""; is_stateful=true; is_distributed_communication=true>
Op<name=StatefulRandomBinomial; signature=resource:resource, algorithm:int64, shape:S, counts:T, probs:T -> output:dtype; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,default=DT_DOUBLE,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=dtype:type,default=DT_INT64,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; is_stateful=true>
Op<name=StatefulStandardNormal; signature=resource:resource, shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_FLOAT; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatefulStandardNormalV2; signature=resource:resource, algorithm:int64, shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_FLOAT; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatefulTruncatedNormal; signature=resource:resource, algorithm:int64, shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_FLOAT; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatefulUniform; signature=resource:resource, algorithm:int64, shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_FLOAT; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatefulUniformFullInt; signature=resource:resource, algorithm:int64, shape:shape_dtype -> output:dtype; attr=dtype:type,default=DT_UINT64; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatefulUniformInt; signature=resource:resource, algorithm:int64, shape:shape_dtype, minval:dtype, maxval:dtype -> output:dtype; attr=dtype:type,default=DT_INT64; attr=shape_dtype:type,default=DT_INT64; is_stateful=true>
Op<name=StatelessCase; signature=branch_index:int32, input: -> output:; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=branches:list(func),min=1; attr=output_shapes:list(shape),default=[]>
Op<name=StatelessIf; signature=cond:Tcond, input: -> output:; attr=Tcond:type; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=then_branch:func; attr=else_branch:func; attr=output_shapes:list(shape),default=[]>
Op<name=StatelessMultinomial; signature=logits:T, num_samples:int32, seed:Tseed -> output:output_dtype; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=output_dtype:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessParameterizedTruncatedNormal; signature=shape:S, seed:Tseed, means:dtype, stddevs:dtype, minvals:dtype, maxvals:dtype -> output:dtype; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=dtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=StatelessRandomBinomial; signature=shape:S, seed:Tseed, counts:T, probs:T -> output:dtype; attr=S:type,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=T:type,default=DT_DOUBLE,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=dtype:type,default=DT_INT64,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=StatelessRandomGammaV2; signature=shape:T, seed:Tseed, alpha:dtype -> output:dtype; attr=dtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomGammaV3; signature=shape:shape_dtype, key:uint64, counter:uint64, alg:int32, alpha:dtype -> output:dtype; attr=dtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE]; attr=shape_dtype:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomGetAlg; signature= -> alg:int32; is_stateful=true>
Op<name=StatelessRandomGetKeyCounter; signature=seed:Tseed -> key:uint64, counter:uint64; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomGetKeyCounterAlg; signature=seed:Tseed -> key:uint64, counter:uint64, alg:int32; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomNormal; signature=shape:T, seed:Tseed -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomNormalV2; signature=shape:Tshape, key:uint64, counter:uint64, alg:int32 -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomPoisson; signature=shape:T, seed:Tseed, lam:Rtype -> output:dtype; attr=Rtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=dtype:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomUniform; signature=shape:T, seed:Tseed -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomUniformFullInt; signature=shape:T, seed:Tseed -> output:dtype; attr=dtype:type,default=DT_UINT64,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]>
Op<name=StatelessRandomUniformFullIntV2; signature=shape:Tshape, key:uint64, counter:uint64, alg:int32 -> output:dtype; attr=dtype:type,default=DT_UINT64,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomUniformInt; signature=shape:T, seed:Tseed, minval:dtype, maxval:dtype -> output:dtype; attr=dtype:type,allowed=[DT_INT32, DT_INT64]; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomUniformIntV2; signature=shape:Tshape, key:uint64, counter:uint64, alg:int32, minval:dtype, maxval:dtype -> output:dtype; attr=dtype:type,allowed=[DT_INT32, DT_INT64, DT_UINT32, DT_UINT64]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessRandomUniformV2; signature=shape:Tshape, key:uint64, counter:uint64, alg:int32 -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessSampleDistortedBoundingBox; signature=image_size:T, bounding_boxes:float, min_object_covered:float, seed:Tseed -> begin:T, size:T, bboxes:float; attr=T:type,allowed=[DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64]; attr=Tseed:type,allowed=[DT_INT32, DT_INT64]; attr=aspect_ratio_range:list(float),default=[0.75, 1.33]; attr=area_range:list(float),default=[0.05, 1]; attr=max_attempts:int,default=100; attr=use_image_if_no_bounding_boxes:bool,default=false>
Op<name=StatelessShuffle; signature=value:T, key:uint64, counter:uint64, alg:int32 -> output:T; attr=T:type>
Op<name=StatelessTruncatedNormal; signature=shape:T, seed:Tseed -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; attr=Tseed:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessTruncatedNormalV2; signature=shape:Tshape, key:uint64, counter:uint64, alg:int32 -> output:dtype; attr=dtype:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=StatelessWhile; signature=input: -> output:; attr=T:list(type),min=0; attr=cond:func; attr=body:func; attr=output_shapes:list(shape),default=[]; attr=parallel_iterations:int,default=10>
Op<name=StaticRegexFullMatch; signature=input:string -> output:bool; attr=pattern:string>
Op<name=StaticRegexReplace; signature=input:string -> output:string; attr=pattern:string; attr=rewrite:string; attr=replace_global:bool,default=true>
Op<name=StatsAggregatorHandle; signature= -> handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StatsAggregatorHandleV2; signature= -> handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=StatsAggregatorSetSummaryWriter; signature=stats_aggregator:resource, summary:resource -> ; is_stateful=true>
Op<name=StatsAggregatorSummary; signature=iterator:resource -> summary:string; is_stateful=true>
Op<name=StochasticCastToInt; signature=input:Tin, key:uint64, counter:uint64, alg:int32 -> output:Tout; attr=Tin:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=Tout:type,allowed=[DT_INT8, DT_INT16, DT_INT32]>
Op<name=StopGradient; signature=input:T -> output:T; attr=T:type>
Op<name=StoreMinibatchStatisticsInFdo; signature=program_key:string, max_ids:int32, max_uniques:int32 -> ; attr=sample_count:int,min=1; attr=num_replica:int,min=1; attr=feature_width:int,min=1; attr=num_sc_per_chip:int,min=1; attr=table_name:string; attr=mini_batch_splits:string; is_stateful=true>
Op<name=StridedSlice; signature=input:T, begin:Index, end:Index, strides:Index -> output:T; attr=T:type; attr=Index:type,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0>
Op<name=StridedSliceAssign; signature=ref:Ref(T), begin:Index, end:Index, strides:Index, value:T -> output_ref:Ref(T); attr=T:type; attr=Index:type,allowed=[DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0>
Op<name=StridedSliceGrad; signature=shape:Index, begin:Index, end:Index, strides:Index, dy:T -> output:T; attr=T:type; attr=Index:type,allowed=[DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0>
Op<name=StringFormat; signature=inputs: -> output:string; attr=T:list(type),min=0; attr=template:string,default="%s"; attr=placeholder:string,default="%s"; attr=summarize:int,default=3>
Op<name=StringJoin; signature=inputs:N*string -> output:string; attr=N:int,min=0; attr=separator:string,default="">
Op<name=StringLength; signature=input:string -> output:int32; attr=unit:string,default="BYTE",allowed=["BYTE", "UTF8_CHAR"]>
Op<name=StringLower; signature=input:string -> output:string; attr=encoding:string,default="">
Op<name=StringNGrams; signature=data:string, data_splits:Tsplits -> ngrams:string, ngrams_splits:Tsplits; attr=separator:string; attr=ngram_widths:list(int),min=0; attr=left_pad:string; attr=right_pad:string; attr=pad_width:int; attr=preserve_short_sequences:bool; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=StringSplit; signature=input:string, delimiter:string -> indices:int64, values:string, shape:int64; attr=skip_empty:bool,default=true>
Op<name=StringSplitV2; signature=input:string, sep:string -> indices:int64, values:string, shape:int64; attr=maxsplit:int,default=-1>
Op<name=StringStrip; signature=input:string -> output:string>
Op<name=StringToHashBucket; signature=string_tensor:string -> output:int64; attr=num_buckets:int,min=1>
Op<name=StringToHashBucketFast; signature=input:string -> output:int64; attr=num_buckets:int,min=1>
Op<name=StringToHashBucketStrong; signature=input:string -> output:int64; attr=num_buckets:int,min=1; attr=key:list(int)>
Op<name=StringToNumber; signature=string_tensor:string -> output:out_type; attr=out_type:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64]>
Op<name=StringUpper; signature=input:string -> output:string; attr=encoding:string,default="">
Op<name=Sub; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_UINT32, DT_UINT64]>
Op<name=Substr; signature=input:string, pos:T, len:T -> output:string; attr=T:type,allowed=[DT_INT32, DT_INT64]; attr=unit:string,default="BYTE",allowed=["BYTE", "UTF8_CHAR"]>
Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=SummaryWriter; signature= -> writer:resource; attr=shared_name:string,default=""; attr=container:string,default=""; is_stateful=true>
Op<name=Svd; signature=input:T -> s:T, u:T, v:T; attr=compute_uv:bool,default=true; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Switch; signature=data:T, pred:bool -> output_false:T, output_true:T; attr=T:type>
Op<name=SymbolicGradient; signature=input: -> output:; attr=Tin:list(type),min=1; attr=Tout:list(type),min=1; attr=f:func>
Op<name=SyncDevice; signature= -> ; is_stateful=true>
Op<name=TFRecordDataset; signature=filenames:string, compression_type:string, buffer_size:int64 -> handle:variant; attr=metadata:string,default=""; is_stateful=true>
Op<name=TFRecordDatasetV2; signature=filenames:string, compression_type:string, buffer_size:int64, byte_offsets:int64 -> handle:variant; attr=metadata:string,default=""; is_stateful=true>
Op<name=TFRecordReader; signature= -> reader_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; attr=compression_type:string,default=""; is_stateful=true>
Op<name=TFRecordReaderV2; signature= -> reader_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=compression_type:string,default=""; is_stateful=true>
Op<name=TPUAnnotateTensorsWithDynamicShape; signature=tensors: -> tpu_tensors:; attr=T:list(type),min=1; is_stateful=true>
Op<name=TPUCompilationResult; signature= -> output:string>
Op<name=TPUCompile; signature=dynamic_shapes:NumDynamicShapes*int64, guaranteed_constants: -> compilation_status:string, program:num_computations*string, may_modify_variables:num_computations*bool; attr=num_computations:int,min=0; attr=function:func; attr=metadata:string; attr=NumDynamicShapes:int,min=0; attr=Tguaranteed_constants:list(type),min=0; is_stateful=true>
Op<name=TPUCompileSucceededAssert; signature=compilation_status:string -> ; is_stateful=true>
Op<name=TPUCopyWithDynamicShape; signature=tensors:, unpadded_sizes:N*int32 -> tpu_tensors:; attr=N:int,min=0; attr=T:list(type),min=1; is_stateful=true>
Op<name=TPUEmbeddingActivations; signature=embedding_variable:float, sliced_activations:float -> output:float; attr=table_id:int,min=0; attr=lookup_id:int,min=0>
Op<name=TPUExecute; signature=args:, key:string -> results:; attr=Targs:list(type),min=0; attr=Tresults:list(type),min=0; is_stateful=true; is_distributed_communication=true>
Op<name=TPUExecuteAndUpdateVariables; signature=args:, key:string -> results:; attr=Targs:list(type),min=0; attr=Tresults:list(type),min=0; attr=device_var_reads_indices:list(int),min=0; attr=device_var_updates_indices:list(int),min=0; is_stateful=true; is_distributed_communication=true>
Op<name=TPUOrdinalSelector; signature= -> device_ordinals:int32; is_stateful=true>
Op<name=TPUPartitionedCall; signature=args:, device_ordinal:int32 -> output:; attr=Tin:list(type),min=0; attr=Tout:list(type),min=0; attr=f:func; attr=autotuner_thresh:int,default=0>
Op<name=TPUPartitionedInput; signature=inputs:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=partition_dim:int,default=0>
Op<name=TPUPartitionedInputV2; signature=inputs:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=partition_dims:list(int); attr=is_packed:bool,default=false>
Op<name=TPUPartitionedOutput; signature=inputs:T -> output:num_splits*T; attr=T:type; attr=num_splits:int,min=1; attr=partition_dim:int,default=0>
Op<name=TPUPartitionedOutputV2; signature=inputs:T -> output:num_splits*T; attr=T:type; attr=num_splits:int,min=1; attr=partition_dims:list(int)>
Op<name=TPUReplicateMetadata; signature= -> ; attr=num_replicas:int,min=0; attr=num_cores_per_replica:int,default=1; attr=topology:string,default=""; attr=use_tpu:bool,default=true; attr=device_assignment:list(int),default=[]; attr=computation_shape:list(int),default=[]; attr=host_compute_core:list(string),default=[]; attr=padding_map:list(string),default=[]; attr=step_marker_location:string,default="STEP_MARK_AT_ENTRY"; attr=allow_soft_placement:bool,default=false; attr=use_spmd_for_xla_partitioning:bool,default=false; attr=tpu_compile_options_proto:string,default="">
Op<name=TPUReplicatedInput; signature=inputs:N*T -> output:T; attr=N:int,min=1; attr=T:type; attr=is_mirrored_variable:bool,default=false; attr=index:int,default=-1; attr=is_packed:bool,default=false>
Op<name=TPUReplicatedOutput; signature=input:T -> outputs:num_replicas*T; attr=num_replicas:int,min=1; attr=T:type>
Op<name=TPUReshardVariables; signature=vars:N*resource, new_format_key:string, format_state_var:resource -> ; attr=N:int,min=0; is_stateful=true>
Op<name=TPURoundRobin; signature= -> device_ordinal:int32; is_stateful=true>
Op<name=TakeDataset; signature=input_dataset:variant, count:int64 -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=TakeManySparseFromTensorsMap; signature=sparse_handles:int64 -> sparse_indices:int64, sparse_values:dtype, sparse_shape:int64; attr=dtype:type; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=TakeWhileDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=predicate:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=Tan; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Tanh; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=TanhGrad; signature=y:T, dy:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=TemporaryVariable; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=var_name:string,default=""; is_stateful=true>
Op<name=TensorArray; signature=size:int32 -> handle:Ref(string); attr=dtype:type; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""; attr=element_shape:shape,default=<unknown>; is_stateful=true>
Op<name=TensorArrayClose; signature=handle:Ref(string) -> >
Op<name=TensorArrayCloseV2; signature=handle:string -> >
Op<name=TensorArrayCloseV3; signature=handle:resource -> ; is_stateful=true>
Op<name=TensorArrayConcat; signature=handle:Ref(string), flow_in:float -> value:dtype, lengths:int64; attr=dtype:type; attr=element_shape_except0:shape,default=<unknown>>
Op<name=TensorArrayConcatV2; signature=handle:string, flow_in:float -> value:dtype, lengths:int64; attr=dtype:type; attr=element_shape_except0:shape,default=<unknown>>
Op<name=TensorArrayConcatV3; signature=handle:resource, flow_in:float -> value:dtype, lengths:int64; attr=dtype:type; attr=element_shape_except0:shape,default=<unknown>; is_stateful=true>
Op<name=TensorArrayGather; signature=handle:Ref(string), indices:int32, flow_in:float -> value:dtype; attr=dtype:type; attr=element_shape:shape,default=<unknown>>
Op<name=TensorArrayGatherV2; signature=handle:string, indices:int32, flow_in:float -> value:dtype; attr=dtype:type; attr=element_shape:shape,default=<unknown>>
Op<name=TensorArrayGatherV3; signature=handle:resource, indices:int32, flow_in:float -> value:dtype; attr=dtype:type; attr=element_shape:shape,default=<unknown>; is_stateful=true>
Op<name=TensorArrayGrad; signature=handle:string, flow_in:float -> grad_handle:Ref(string); attr=source:string; is_stateful=true>
Op<name=TensorArrayGradV2; signature=handle:string, flow_in:float -> grad_handle:string; attr=source:string; is_stateful=true>
Op<name=TensorArrayGradV3; signature=handle:resource, flow_in:float -> grad_handle:resource, flow_out:float; attr=source:string; is_stateful=true>
Op<name=TensorArrayGradWithShape; signature=handle:resource, flow_in:float, shape_to_prepend:int32 -> grad_handle:resource, flow_out:float; attr=source:string; is_stateful=true>
Op<name=TensorArrayPack; signature=handle:Ref(string), flow_in:float -> value:dtype; attr=dtype:type; attr=element_shape:shape,default=<unknown>>
Op<name=TensorArrayRead; signature=handle:Ref(string), index:int32, flow_in:float -> value:dtype; attr=dtype:type>
Op<name=TensorArrayReadV2; signature=handle:string, index:int32, flow_in:float -> value:dtype; attr=dtype:type>
Op<name=TensorArrayReadV3; signature=handle:resource, index:int32, flow_in:float -> value:dtype; attr=dtype:type; is_stateful=true>
Op<name=TensorArrayScatter; signature=handle:Ref(string), indices:int32, value:T, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArrayScatterV2; signature=handle:string, indices:int32, value:T, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArrayScatterV3; signature=handle:resource, indices:int32, value:T, flow_in:float -> flow_out:float; attr=T:type; is_stateful=true>
Op<name=TensorArraySize; signature=handle:Ref(string), flow_in:float -> size:int32>
Op<name=TensorArraySizeV2; signature=handle:string, flow_in:float -> size:int32>
Op<name=TensorArraySizeV3; signature=handle:resource, flow_in:float -> size:int32; is_stateful=true>
Op<name=TensorArraySplit; signature=handle:Ref(string), value:T, lengths:int64, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArraySplitV2; signature=handle:string, value:T, lengths:int64, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArraySplitV3; signature=handle:resource, value:T, lengths:int64, flow_in:float -> flow_out:float; attr=T:type; is_stateful=true>
Op<name=TensorArrayUnpack; signature=handle:Ref(string), value:T, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArrayV2; signature=size:int32 -> handle:string; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=tensor_array_name:string,default=""; is_stateful=true>
Op<name=TensorArrayV3; signature=size:int32 -> handle:resource, flow:float; attr=dtype:type; attr=element_shape:shape,default=<unknown>; attr=dynamic_size:bool,default=false; attr=clear_after_read:bool,default=true; attr=identical_element_shapes:bool,default=false; attr=tensor_array_name:string,default=""; is_stateful=true>
Op<name=TensorArrayWrite; signature=handle:Ref(string), index:int32, value:T, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArrayWriteV2; signature=handle:string, index:int32, value:T, flow_in:float -> flow_out:float; attr=T:type>
Op<name=TensorArrayWriteV3; signature=handle:resource, index:int32, value:T, flow_in:float -> flow_out:float; attr=T:type; is_stateful=true>
Op<name=TensorDataset; signature=components: -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default=""; is_stateful=true>
Op<name=TensorListConcat; signature=input_handle:variant -> tensor:element_dtype, lengths:int64; attr=element_dtype:type; attr=element_shape:shape,default=<unknown>>
Op<name=TensorListConcatLists; signature=input_a:variant, input_b:variant -> output:variant; attr=element_dtype:type>
Op<name=TensorListConcatV2; signature=input_handle:variant, element_shape:shape_type, leading_dims:int64 -> tensor:element_dtype, lengths:int64; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListElementShape; signature=input_handle:variant -> element_shape:shape_type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListFromTensor; signature=tensor:element_dtype, element_shape:shape_type -> output_handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListGather; signature=input_handle:variant, indices:int32, element_shape:int32 -> values:element_dtype; attr=element_dtype:type>
Op<name=TensorListGetItem; signature=input_handle:variant, index:int32, element_shape:int32 -> item:element_dtype; attr=element_dtype:type>
Op<name=TensorListLength; signature=input_handle:variant -> length:int32>
Op<name=TensorListPopBack; signature=input_handle:variant, element_shape:int32 -> output_handle:variant, tensor:element_dtype; attr=element_dtype:type>
Op<name=TensorListPushBack; signature=input_handle:variant, tensor:element_dtype -> output_handle:variant; attr=element_dtype:type>
Op<name=TensorListPushBackBatch; signature=input_handles:variant, tensor:element_dtype -> output_handles:variant; attr=element_dtype:type>
Op<name=TensorListReserve; signature=element_shape:shape_type, num_elements:int32 -> handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListResize; signature=input_handle:variant, size:int32 -> output_handle:variant>
Op<name=TensorListScatter; signature=tensor:element_dtype, indices:int32, element_shape:shape_type -> output_handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListScatterIntoExistingList; signature=input_handle:variant, tensor:element_dtype, indices:int32 -> output_handle:variant; attr=element_dtype:type>
Op<name=TensorListScatterV2; signature=tensor:element_dtype, indices:int32, element_shape:shape_type, num_elements:int32 -> output_handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListSetItem; signature=input_handle:variant, index:int32, item:element_dtype -> output_handle:variant; attr=element_dtype:type; attr=resize_if_index_out_of_bounds:bool,default=false>
Op<name=TensorListSplit; signature=tensor:element_dtype, element_shape:shape_type, lengths:int64 -> output_handle:variant; attr=element_dtype:type; attr=shape_type:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorListStack; signature=input_handle:variant, element_shape:int32 -> tensor:element_dtype; attr=element_dtype:type; attr=num_elements:int,default=-1>
Op<name=TensorMapErase; signature=input_handle:variant, key:key_dtype -> output_handle:variant; attr=key_dtype:type; attr=value_dtype:type>
Op<name=TensorMapHasKey; signature=input_handle:variant, key:key_dtype -> has_key:bool; attr=key_dtype:type>
Op<name=TensorMapInsert; signature=input_handle:variant, key:key_dtype, value:value_dtype -> output_handle:variant; attr=key_dtype:type; attr=value_dtype:type>
Op<name=TensorMapLookup; signature=input_handle:variant, key:key_dtype -> value:value_dtype; attr=key_dtype:type; attr=value_dtype:type>
Op<name=TensorMapSize; signature=input_handle:variant -> size:int32>
Op<name=TensorMapStackKeys; signature=input_handle:variant -> keys:key_dtype; attr=key_dtype:type>
Op<name=TensorScatterAdd; signature=tensor:T, indices:Tindices, updates:T -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorScatterMax; signature=tensor:T, indices:Tindices, updates:T -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorScatterMin; signature=tensor:T, indices:Tindices, updates:T -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorScatterSub; signature=tensor:T, indices:Tindices, updates:T -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]>
Op<name=TensorScatterUpdate; signature=tensor:T, indices:Tindices, updates:T -> output:T; attr=T:type; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64, DT_UINT16]>
Op<name=TensorSliceDataset; signature=components: -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=is_files:bool,default=false; attr=metadata:string,default=""; attr=replicate_on_split:bool,default=false; is_stateful=true>
Op<name=TensorStridedSliceUpdate; signature=input:T, begin:Index, end:Index, strides:Index, value:T -> output:T; attr=T:type; attr=Index:type,allowed=[DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0>
Op<name=TensorSummary; signature=tensor:T -> summary:string; attr=T:type; attr=description:string,default=""; attr=labels:list(string),default=[]; attr=display_name:string,default="">
Op<name=TensorSummaryV2; signature=tag:string, tensor:T, serialized_summary_metadata:string -> summary:string; attr=T:type>
Op<name=TextLineDataset; signature=filenames:string, compression_type:string, buffer_size:int64 -> handle:variant; attr=metadata:string,default=""; is_stateful=true>
Op<name=TextLineReader; signature= -> reader_handle:Ref(string); attr=skip_header_lines:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=TextLineReaderV2; signature= -> reader_handle:resource; attr=skip_header_lines:int,default=0; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=ThreadPoolDataset; signature=input_dataset:variant, thread_pool:resource -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; is_stateful=true>
Op<name=ThreadPoolHandle; signature= -> handle:resource; attr=num_threads:int; attr=max_intra_op_parallelism:int,default=1; attr=display_name:string; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=ThreadUnsafeUnigramCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=range_max:int,min=1; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=Tile; signature=input:T, multiples:Tmultiples -> output:T; attr=T:type; attr=Tmultiples:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=TileGrad; signature=input:T, multiples:int32 -> output:T; attr=T:type>
Op<name=Timestamp; signature= -> ts:double; is_stateful=true>
Op<name=ToBool; signature=input:T -> output:bool; attr=T:type>
Op<name=TopK; signature=input:T -> values:T, indices:int32; attr=k:int,min=0; attr=sorted:bool,default=true; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]>
Op<name=TopKUnique; signature=input:float -> topk:float, topk_indices:int32; attr=k:int>
Op<name=TopKV2; signature=input:T, k:Tk -> values:T, indices:index_type; attr=sorted:bool,default=true; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tk:type,default=DT_INT32,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=index_type:type,default=DT_INT32,allowed=[DT_INT16, DT_INT32, DT_INT64]>
Op<name=TopKWithUnique; signature=input:float -> topk:float, topk_indices:int32; attr=k:int>
Op<name=TpuHandleToProtoKey; signature=uid:int64 -> proto_keys:string>
Op<name=Transpose; signature=x:T, perm:Tperm -> y:T; attr=T:type; attr=Tperm:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=TridiagonalMatMul; signature=superdiag:T, maindiag:T, subdiag:T, rhs:T -> output:T; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=TridiagonalSolve; signature=diagonals:T, rhs:T -> output:T; attr=partial_pivoting:bool,default=true; attr=perturb_singular:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=TruncateDiv; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=TruncateMod; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>
Op<name=TruncatedNormal; signature=shape:T -> output:dtype; attr=seed:int,default=0; attr=seed2:int,default=0; attr=dtype:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]; attr=T:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=Unbatch; signature=batched_tensor:T, batch_index:int64, id:int64 -> unbatched_tensor:T; attr=timeout_micros:int; attr=container:string,default=""; attr=shared_name:string,default=""; attr=T:type>
Op<name=UnbatchDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=UnbatchGrad; signature=original_input:T, batch_index:int64, grad:T, id:int64 -> batched_grad:T; attr=container:string,default=""; attr=shared_name:string,default=""; attr=T:type>
Op<name=UncompressElement; signature=compressed:variant -> components:; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1>
Op<name=UnicodeDecode; signature=input:string -> row_splits:Tsplits, char_values:int32; attr=input_encoding:string; attr=errors:string,default="replace",allowed=["strict", "replace", "ignore"]; attr=replacement_char:int,default=65533; attr=replace_control_characters:bool,default=false; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=UnicodeDecodeWithOffsets; signature=input:string -> row_splits:Tsplits, char_values:int32, char_to_byte_starts:int64; attr=input_encoding:string; attr=errors:string,default="replace",allowed=["strict", "replace", "ignore"]; attr=replacement_char:int,default=65533; attr=replace_control_characters:bool,default=false; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=UnicodeEncode; signature=input_values:int32, input_splits:Tsplits -> output:string; attr=errors:string,default="replace",allowed=["ignore", "replace", "strict"]; attr=output_encoding:string,allowed=["UTF-8", "UTF-16-BE", "UTF-32-BE"]; attr=replacement_char:int,default=65533; attr=Tsplits:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]>
Op<name=UnicodeScript; signature=input:int32 -> output:int32>
Op<name=UnicodeTranscode; signature=input:string -> output:string; attr=input_encoding:string; attr=output_encoding:string,allowed=["UTF-8", "UTF-16-BE", "UTF-32-BE"]; attr=errors:string,default="replace",allowed=["strict", "replace", "ignore"]; attr=replacement_char:int,default=65533; attr=replace_control_characters:bool,default=false>
Op<name=UniformCandidateSampler; signature=true_classes:int64 -> sampled_candidates:int64, true_expected_count:float, sampled_expected_count:float; attr=num_true:int,min=1; attr=num_sampled:int,min=1; attr=unique:bool; attr=range_max:int,min=1; attr=seed:int,default=0; attr=seed2:int,default=0; is_stateful=true>
Op<name=UniformDequantize; signature=input:Tin, scales:float, zero_points:int32 -> output:Tout; attr=Tin:type,allowed=[DT_QINT8, DT_QINT32]; attr=Tout:type,allowed=[DT_FLOAT]; attr=quantization_axis:int,default=-1; attr=quantization_min_val:int; attr=quantization_max_val:int>
Op<name=UniformQuantize; signature=input:Tin, scales:float, zero_points:int32 -> output:Tout; attr=Tin:type,allowed=[DT_FLOAT]; attr=Tout:type,allowed=[DT_QINT8, DT_QINT32]; attr=quantization_axis:int,default=-1; attr=quantization_min_val:int; attr=quantization_max_val:int>
Op<name=UniformQuantizedAdd; signature=lhs:T, rhs:T, lhs_scales:float, lhs_zero_points:int32, rhs_scales:float, rhs_zero_points:int32, output_scales:float, output_zero_points:int32 -> output:T; attr=lhs_quantization_axis:int,default=-1; attr=lhs_quantization_min_val:int; attr=lhs_quantization_max_val:int; attr=rhs_quantization_axis:int,default=-1; attr=rhs_quantization_min_val:int; attr=rhs_quantization_max_val:int; attr=output_quantization_axis:int,default=-1; attr=output_quantization_min_val:int; attr=output_quantization_max_val:int; attr=T:type,allowed=[DT_QINT32]>
Op<name=UniformQuantizedClipByValue; signature=operand:T, min:T, max:T, scales:float, zero_points:int32 -> output:T; attr=T:type,allowed=[DT_QINT32]; attr=quantization_axis:int,default=-1; attr=quantization_min_val:int; attr=quantization_max_val:int>
Op<name=UniformQuantizedConvolution; signature=lhs:Tin, rhs:Tin, lhs_scales:float, lhs_zero_points:int32, rhs_scales:float, rhs_zero_points:int32, output_scales:float, output_zero_points:int32 -> output:Tout; attr=Tin:type,allowed=[DT_QINT8]; attr=Tout:type,allowed=[DT_QINT32]; attr=window_strides:list(int),default=[]; attr=padding:string; attr=explicit_padding:list(int),default=[]; attr=lhs_dilation:list(int),default=[]; attr=rhs_dilation:list(int),default=[]; attr=batch_group_count:int,default=1; attr=feature_group_count:int,default=1; attr=dimension_numbers:string,default=""; attr=lhs_quantization_axis:int,default=-1; attr=lhs_quantization_min_val:int; attr=lhs_quantization_max_val:int; attr=rhs_quantization_axis:int,default=-1; attr=rhs_quantization_min_val:int; attr=rhs_quantization_max_val:int; attr=output_quantization_axis:int,default=-1; attr=output_quantization_min_val:int; attr=output_quantization_max_val:int>
Op<name=UniformQuantizedConvolutionHybrid; signature=lhs:Tlhs, rhs:Trhs, rhs_scales:float, rhs_zero_points:int32 -> output:Tout; attr=Tlhs:type,allowed=[DT_FLOAT]; attr=Trhs:type,allowed=[DT_QINT8]; attr=Tout:type,allowed=[DT_FLOAT]; attr=window_strides:list(int),default=[]; attr=padding:string; attr=explicit_padding:list(int),default=[]; attr=lhs_dilation:list(int),default=[]; attr=rhs_dilation:list(int),default=[]; attr=batch_group_count:int,default=1; attr=feature_group_count:int,default=1; attr=dimension_numbers:string,default=""; attr=rhs_quantization_axis:int,default=-1; attr=rhs_quantization_min_val:int; attr=rhs_quantization_max_val:int>
Op<name=UniformQuantizedDot; signature=lhs:Tin, rhs:Tin, lhs_scales:float, lhs_zero_points:int32, rhs_scales:float, rhs_zero_points:int32, output_scales:float, output_zero_points:int32 -> output:Tout; attr=Tin:type,allowed=[DT_QINT8]; attr=Tout:type,allowed=[DT_QINT32]; attr=lhs_quantization_axis:int,default=-1; attr=lhs_quantization_min_val:int; attr=lhs_quantization_max_val:int; attr=rhs_quantization_axis:int,default=-1; attr=rhs_quantization_min_val:int; attr=rhs_quantization_max_val:int; attr=output_quantization_axis:int,default=-1; attr=output_quantization_min_val:int; attr=output_quantization_max_val:int>
Op<name=UniformQuantizedDotHybrid; signature=lhs:Tlhs, rhs:Trhs, rhs_scales:float, rhs_zero_points:int32 -> output:Tout; attr=Tlhs:type,allowed=[DT_FLOAT]; attr=Trhs:type,allowed=[DT_QINT8]; attr=Tout:type,allowed=[DT_FLOAT]; attr=rhs_quantization_axis:int,default=-1; attr=rhs_quantization_min_val:int; attr=rhs_quantization_max_val:int>
Op<name=UniformRequantize; signature=input:Tin, input_scales:float, input_zero_points:int32, output_scales:float, output_zero_points:int32 -> output:Tout; attr=Tin:type,allowed=[DT_QINT8, DT_QINT32]; attr=Tout:type,allowed=[DT_QINT8, DT_QINT32]; attr=input_quantization_axis:int,default=-1; attr=input_quantization_min_val:int; attr=input_quantization_max_val:int; attr=output_quantization_axis:int,default=-1; attr=output_quantization_min_val:int; attr=output_quantization_max_val:int>
Op<name=Unique; signature=x:T -> y:T, idx:out_idx; attr=T:type; attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UniqueDataset; signature=input_dataset:variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=UniqueV2; signature=x:T, axis:Taxis -> y:T, idx:out_idx; attr=T:type; attr=Taxis:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UniqueWithCounts; signature=x:T -> y:T, idx:out_idx, count:out_idx; attr=T:type; attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UniqueWithCountsV2; signature=x:T, axis:Taxis -> y:T, idx:out_idx, count:out_idx; attr=T:type; attr=Taxis:type,default=DT_INT64,allowed=[DT_INT32, DT_INT64]; attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Unpack; signature=value:T -> output:num*T; attr=num:int,min=0; attr=T:type; attr=axis:int,default=0>
Op<name=UnravelIndex; signature=indices:Tidx, dims:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UnsortedSegmentJoin; signature=inputs:string, segment_ids:Tindices, num_segments:Tnumsegments -> output:string; attr=separator:string,default=""; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UnsortedSegmentMax; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UnsortedSegmentMin; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UnsortedSegmentProd; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=UnsortedSegmentSum; signature=data:T, segment_ids:Tindices, num_segments:Tnumsegments -> output:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=Tnumsegments:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=Unstage; signature= -> values:; attr=capacity:int,default=0,min=0; attr=memory_limit:int,default=0,min=0; attr=dtypes:list(type),min=1; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=UnwrapDatasetVariant; signature=input_handle:variant -> output_handle:variant>
Op<name=UpperBound; signature=sorted_inputs:T, values:T -> output:out_type; attr=T:type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>
Op<name=VarHandleOp; signature= -> resource:resource; attr=container:string,default=""; attr=shared_name:string,default=""; attr=debug_name:string,default=""; attr=dtype:type; attr=shape:shape; attr=allowed_devices:list(string),default=[]; is_stateful=true>
Op<name=VarIsInitializedOp; signature=resource:resource -> is_initialized:bool; is_stateful=true>
Op<name=Variable; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=VariableShape; signature=input:resource -> output:out_type; attr=out_type:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]; is_stateful=true>
Op<name=VariableV2; signature= -> ref:Ref(dtype); attr=shape:shape; attr=dtype:type; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=Where; signature=input:T -> index:int64; attr=T:type,default=DT_BOOL,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_BOOL]>
Op<name=While; signature=input: -> output:; attr=T:list(type),min=0; attr=cond:func; attr=body:func; attr=output_shapes:list(shape),default=[]; attr=parallel_iterations:int,default=10; is_stateful=true>
Op<name=WholeFileReader; signature= -> reader_handle:Ref(string); attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=WholeFileReaderV2; signature= -> reader_handle:resource; attr=container:string,default=""; attr=shared_name:string,default=""; is_stateful=true>
Op<name=WindowDataset; signature=input_dataset:variant, size:int64, shift:int64, stride:int64, drop_remainder:bool -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=metadata:string,default="">
Op<name=WindowOp; signature=inputs: -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=Tinputs:list(type),min=1>
Op<name=WorkerHeartbeat; signature=request:string -> response:string; is_stateful=true>
Op<name=WrapDatasetVariant; signature=input_handle:variant -> output_handle:variant>
Op<name=WriteAudioSummary; signature=writer:resource, step:int64, tag:string, tensor:float, sample_rate:float -> ; attr=max_outputs:int,default=3,min=1; is_stateful=true>
Op<name=WriteFile; signature=filename:string, contents:string -> ; is_stateful=true>
Op<name=WriteGraphSummary; signature=writer:resource, step:int64, tensor:string -> ; is_stateful=true>
Op<name=WriteHistogramSummary; signature=writer:resource, step:int64, tag:string, values:T -> ; attr=T:type,default=DT_FLOAT,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64, DT_BOOL]; is_stateful=true>
Op<name=WriteImageSummary; signature=writer:resource, step:int64, tag:string, tensor:T, bad_color:uint8 -> ; attr=max_images:int,default=3,min=1; attr=T:type,default=DT_FLOAT,allowed=[DT_UINT8, DT_DOUBLE, DT_FLOAT, DT_HALF]; is_stateful=true>
Op<name=WriteRawProtoSummary; signature=writer:resource, step:int64, tensor:string -> ; is_stateful=true>
Op<name=WriteScalarSummary; signature=writer:resource, step:int64, tag:string, value:T -> ; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UINT16, DT_HALF, DT_UINT32, DT_UINT64]; is_stateful=true>
Op<name=WriteSummary; signature=writer:resource, step:int64, tensor:T, tag:string, summary_metadata:string -> ; attr=T:type; is_stateful=true>
Op<name=Xdivy; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=XlaConcatND; signature=inputs:N*T -> output:T; attr=T:type; attr=N:int,min=1; attr=num_concats:list(int); attr=paddings:list(int),default=[]>
Op<name=XlaHostCompute; signature=inputs: -> outputs:; attr=Tinputs:list(type),min=0; attr=Toutputs:list(type),min=0; attr=ancestors:list(string),min=0; attr=shapes:list(shape),min=0; attr=shape_inference_graph:func; attr=key:string; attr=send_key:string,default=""; attr=recv_key:string,default=""; attr=cost_estimate_ns:int,default=1000000; attr=tpu_core:int,default=0; is_stateful=true>
Op<name=XlaRecvFromHost; signature= -> output:Toutput; attr=Toutput:type; attr=shape:shape; attr=key:string; is_stateful=true>
Op<name=XlaRecvTPUEmbeddingActivations; signature=deduplication_data:variant -> outputs:num_tables*float; attr=num_tables:int,min=1; attr=config:string; is_stateful=true>
Op<name=XlaRecvTPUEmbeddingDeduplicationData; signature= -> output:variant; attr=config:string; is_stateful=true>
Op<name=XlaSendTPUEmbeddingGradients; signature=gradients:NumTables*float, learning_rates:NumLearningRateTags*float, deduplication_data:variant -> ; attr=NumTables:int,min=1; attr=NumLearningRateTags:int,default=0,min=0; attr=config:string; is_stateful=true>
Op<name=XlaSendToHost; signature=input:Tinput -> ; attr=Tinput:type; attr=key:string; is_stateful=true>
Op<name=XlaSparseCoreAdagrad; signature=indices:int32, gradient:float, learning_rate:float, accumulator:float, embedding_table:float -> updated_embedding_table:float, updated_accumulator:float; attr=feature_width:int; is_stateful=true>
Op<name=XlaSparseCoreAdagradMomentum; signature=indices:int32, gradient:float, learning_rate:float, beta_1:float, epsilon:float, accumulator:float, momentum:float, embedding_table:float -> updated_embedding_table:float, updated_accumulator:float, updated_momentum:float; attr=feature_width:int; attr=use_nesterov:bool; attr=beta_2:float; attr=exponent:float; is_stateful=true>
Op<name=XlaSparseCoreAdam; signature=embedding_table:float, indices:int32, gradient:float, learning_rate:float, momentum:float, velocity:float, beta_1:float, beta_2:float, epsilon:float -> updated_embedding_table:float, updated_velocity:float, updated_momentum:float; attr=feature_width:int; attr=use_sum_inside_sqrt:bool; is_stateful=true>
Op<name=XlaSparseCoreFtrl; signature=embedding_table:float, accumulator:float, linear:float, learning_rate:float, indices:int32, gradient:float, beta:float, learning_rate_power:float, l2_regularization_strength:float -> updated_embedding_table:float, updated_accumulator:float, updated_linear:float; attr=feature_width:int; attr=multiply_linear_by_learning_rate:bool; attr=l1_regularization_strength:float; is_stateful=true>
Op<name=XlaSparseCoreSgd; signature=indices:int32, gradient:float, learning_rate:float, embedding_table:float -> updated_embedding_table:float; attr=feature_width:int; is_stateful=true>
Op<name=XlaSparseDenseMatmul; signature=row_ids:int32, col_ids:uint32, values:float, offsets:uint32, embedding_table:float -> activations:float, row_pointers:int32, sorted_embedding_ids:int32, sorted_sample_ids:int32, sorted_gains:float; attr=max_ids_per_partition:int,min=0; attr=max_unique_ids_per_partition:int,min=0; attr=input_size:int,min=0>
Op<name=XlaSparseDenseMatmulGradWithAdagradAndCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, activation_gradients:float, learning_rate:float, embedding_table:float, accumulator:float, num_minibatches_per_physical_sparse_core:int32 -> updated_embedding_table:float, updated_accumulator:float; attr=clip_weight_min:float,default=-inf; attr=clip_weight_max:float,default=inf; attr=table_name:string>
Op<name=XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, activation_gradients:float, learning_rate:float, embedding_table:float, accumulator:float, momenta:float, num_minibatches_per_physical_sparse_core:int32 -> updated_embedding_table:float, updated_accumulator:float, updated_momenta:float; attr=use_nesterov:bool; attr=exponent:float; attr=beta1:float; attr=beta2:float; attr=epsilon:float; attr=clip_weight_min:float,default=-inf; attr=clip_weight_max:float,default=inf; attr=table_name:string>
Op<name=XlaSparseDenseMatmulGradWithAdamAndCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, activation_gradients:float, learning_rate:float, embedding_table:float, momenta:float, velocity:float, num_minibatches_per_physical_sparse_core:int32 -> updated_embedding_table:float, updated_momenta:float, updated_velocity:float; attr=use_sum_inside_sqrt:bool; attr=beta1:float; attr=beta2:float; attr=epsilon:float; attr=clip_weight_min:float,default=-inf; attr=clip_weight_max:float,default=inf; attr=table_name:string>
Op<name=XlaSparseDenseMatmulGradWithFtrlAndCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, activation_gradients:float, learning_rate:float, embedding_table:float, accumulator:float, linear:float, num_minibatches_per_physical_sparse_core:int32 -> updated_embedding_table:float, updated_accumulator:float, updated_linear:float; attr=multiply_linear_by_learning_rate:bool; attr=beta:float; attr=learning_rate_power:float; attr=l1_regularization_strength:float; attr=l2_regularization_strength:float; attr=clip_weight_min:float,default=-inf; attr=clip_weight_max:float,default=inf; attr=table_name:string>
Op<name=XlaSparseDenseMatmulGradWithSgdAndCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, activation_gradients:float, learning_rate:float, embedding_table:float, num_minibatches_per_physical_sparse_core:int32 -> updated_embedding_table:float; attr=clip_weight_min:float,default=-inf; attr=clip_weight_max:float,default=inf; attr=table_name:string>
Op<name=XlaSparseDenseMatmulWithCsrInput; signature=row_pointers:int32, sorted_sample_ids:int32, sorted_token_ids:int32, sorted_gains:float, embedding_table:float, num_minibatches_per_physical_sparse_core:int32 -> activations:float; attr=input_size:int,min=0; attr=quantization_config_low:float; attr=quantization_config_high:float; attr=quantization_config_num_buckets:int,min=0; attr=table_name:string>
Op<name=XlaSplitND; signature=input:T -> outputs:N*T; attr=T:type; attr=N:int,min=1; attr=num_splits:list(int); attr=paddings:list(int),default=[]>
Op<name=Xlog1py; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=Xlogy; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]>
Op<name=ZerosLike; signature=x:T -> y:T; attr=T:type>
Op<name=Zeta; signature=x:T, q:T -> z:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]>
Op<name=ZipDataset; signature=input_datasets:N*variant -> handle:variant; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1; attr=metadata:string,default="">

